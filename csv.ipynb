{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18485a42-30e9-4708-a8f2-ddc0a854c15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59a3b76a-99bb-42f5-88c3-75e6c026a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc0c818b-dac9-4cb7-9ced-ae533275a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a7eca55-029e-4d2a-be7e-68f80cfae5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialiser SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\n",
    "\n",
    "# Définir le schéma\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\", StringType(), True),\n",
    "    StructField(\"middlename\", StringType(), True),\n",
    "    StructField(\"lastname\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Données\n",
    "data = [\n",
    "    (\"James\", \"\", \"Smith\", \"36636\", \"M\", 3000),\n",
    "    (\"Michael\", \"Rose\", \"\", \"40288\", \"M\", 4000),\n",
    "    (\"Robert\", \"\", \"Williams\", \"42114\", \"M\", 4000),\n",
    "    (\"Maria\", \"Anne\", \"Jones\", \"39192\", \"F\", 4000),\n",
    "    (\"Jen\", \"Mary\", \"Brown\", \"\", \"F\", -1)\n",
    "]\n",
    "\n",
    "# Créer DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Afficher schéma et données\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d627a7-d668-48f7-9866-4a67d0b2b772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- identifiant: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_renamed = df.withColumnRenamed(\"id\", \"identifiant\")\n",
    "df_renamed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5e59104-8ac0-488d-9579-b259f716838d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_femmes = df.filter(df.gender == \"F\")\n",
    "df_femmes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac056915-db1c-486f-ba56-ec74ea16b05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy([\"lastname\", \"firstname\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6abcba78-645f-4f8e-82f8-ea47898f63db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"lastname\", \"firstname\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c010683b-a258-49cc-965a-a67e4cfed45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(emp_id='A101', emp_name='John', salary=3250),\n",
       " Row(emp_id='A102', emp_name='Peter', salary=6735),\n",
       " Row(emp_id='A103', emp_name='Charlie', salary=8650)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialiser la session Spark\n",
    "spark = SparkSession.builder.appName(\"JoinExample\").getOrCreate()\n",
    "\n",
    "# Définir le premier DataFrame\n",
    "data1 = [(\"A101\", \"John\"), (\"A102\", \"Peter\"), (\"A103\", \"Charlie\")]\n",
    "columns1 = [\"emp_id\", \"emp_name\"]\n",
    "dataframe_1 = spark.createDataFrame(data1, columns1)\n",
    "\n",
    "# Définir le deuxième DataFrame\n",
    "data2 = [(\"A101\", 3250), (\"A102\", 6735), (\"A103\", 8650)]\n",
    "columns2 = [\"emp_id\", \"salary\"]\n",
    "dataframe_2 = spark.createDataFrame(data2, columns2)\n",
    "\n",
    "# Jointure interne (INNER JOIN)\n",
    "combined_df = dataframe_1.join(dataframe_2, on=\"emp_id\", how=\"inner\")\n",
    "\n",
    "# Afficher les résultats sous forme de liste de Row\n",
    "combined_df.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a9647a1-3ce5-442a-9c8b-9e3b404721da",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/people.json.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPeopleExample\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Lire le fichier JSON (remplace 'people.json' par le chemin correct)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpeople.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Afficher le contenu du DataFrame\u001b[39;00m\n\u001b[1;32m     10\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:425\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator: Iterable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/people.json."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25ebe02c-5e1a-4107-878c-dbfda4093548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Créer une session Spark\n",
    "spark = SparkSession.builder.appName(\"PeopleExample\").getOrCreate()\n",
    "\n",
    "# Lire le fichier JSON (remplace 'people.json' par le chemin correct)\n",
    "df = spark.read.json(\"people.json\")\n",
    "\n",
    "# Afficher le contenu du DataFrame\n",
    "df.show()\n",
    "\n",
    "# Afficher le schéma du DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Enregistrer comme vue temporaire pour requêtes SQL\n",
    "df.createOrReplaceTempView(\"people\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51fda6e3-9e29-4de8-a4f2-9ce37d3a96ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas findspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41229f47-fe5e-4ade-b49b-b1212f2d71f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# Créer une session Spark\n",
    "spark = SparkSession.builder.appName(\"CovidAnalysis\").getOrCreate()\n",
    "\n",
    "# Vérification de la session\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38196a79-b797-41bb-a8ea-286923f8d4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  iso_code continent        location last_updated_date  \\\n",
      "0           0       AFG      Asia     Afghanistan        2024-08-04   \n",
      "1           1  OWID_AFR       NaN          Africa        2024-08-04   \n",
      "2           2       ALB    Europe         Albania        2024-08-04   \n",
      "3           3       DZA    Africa         Algeria        2024-08-04   \n",
      "4           4       ASM   Oceania  American Samoa        2024-08-04   \n",
      "\n",
      "   total_cases  new_cases  new_cases_smoothed  total_deaths  new_deaths  ...  \\\n",
      "0     235214.0        0.0               0.000        7998.0         0.0  ...   \n",
      "1   13145380.0       36.0               5.143      259117.0         0.0  ...   \n",
      "2     335047.0        0.0               0.000        3605.0         0.0  ...   \n",
      "3     272139.0       18.0               2.571        6881.0         0.0  ...   \n",
      "4       8359.0        0.0               0.000          34.0         0.0  ...   \n",
      "\n",
      "   male_smokers  handwashing_facilities  hospital_beds_per_thousand  \\\n",
      "0           NaN                  37.746                        0.50   \n",
      "1           NaN                     NaN                         NaN   \n",
      "2          51.2                     NaN                        2.89   \n",
      "3          30.4                  83.741                        1.90   \n",
      "4           NaN                     NaN                         NaN   \n",
      "\n",
      "   life_expectancy  human_development_index    population  \\\n",
      "0            64.83                    0.511  4.112877e+07   \n",
      "1              NaN                      NaN  1.426737e+09   \n",
      "2            78.57                    0.795  2.842318e+06   \n",
      "3            76.88                    0.748  4.490323e+07   \n",
      "4            73.74                      NaN  4.429500e+04   \n",
      "\n",
      "   excess_mortality_cumulative_absolute  excess_mortality_cumulative  \\\n",
      "0                                   NaN                          NaN   \n",
      "1                                   NaN                          NaN   \n",
      "2                                   NaN                          NaN   \n",
      "3                                   NaN                          NaN   \n",
      "4                                   NaN                          NaN   \n",
      "\n",
      "   excess_mortality  excess_mortality_cumulative_per_million  \n",
      "0               NaN                                      NaN  \n",
      "1               NaN                                      NaN  \n",
      "2               NaN                                      NaN  \n",
      "3               NaN                                      NaN  \n",
      "4               NaN                                      NaN  \n",
      "\n",
      "[5 rows x 68 columns]\n"
     ]
    }
   ],
   "source": [
    "# Lecture du fichier COVID-19\n",
    "vaccination_data = pd.read_csv(\"covid-latest.csv\")  # Remplace par le vrai chemin\n",
    "\n",
    "# Afficher les 5 premières lignes\n",
    "print(vaccination_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b4e078a-b767-4fe2-bb47-8b9d2935ed8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  continent  total_cases  total_deaths  total_vaccinations    population\n",
      "0      Asia     235214.0        7998.0                 NaN  4.112877e+07\n",
      "1       NaN   13145380.0      259117.0                 NaN  1.426737e+09\n",
      "2    Europe     335047.0        3605.0                 NaN  2.842318e+06\n",
      "3    Africa     272139.0        6881.0                 NaN  4.490323e+07\n",
      "4   Oceania       8359.0          34.0                 NaN  4.429500e+04\n"
     ]
    }
   ],
   "source": [
    "selected_data = vaccination_data[['continent', 'total_cases', 'total_deaths', 'total_vaccinations', 'population']]\n",
    "print(selected_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "153c4230-3b13-4979-8c9e-925d7b92bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "# Nettoyer les données pour Spark\n",
    "selected_data = selected_data.fillna(0)\n",
    "selected_data = selected_data.astype({\n",
    "    'continent': str,\n",
    "    'total_cases': float,\n",
    "    'total_deaths': float,\n",
    "    'total_vaccinations': float,\n",
    "    'population': float\n",
    "})\n",
    "\n",
    "# Définir le schéma pour Spark\n",
    "schema = StructType([\n",
    "    StructField(\"continent\", StringType(), True),\n",
    "    StructField(\"total_cases\", DoubleType(), True),\n",
    "    StructField(\"total_deaths\", DoubleType(), True),\n",
    "    StructField(\"total_vaccinations\", DoubleType(), True),\n",
    "    StructField(\"population\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Convertir en DataFrame Spark\n",
    "spark_df = spark.createDataFrame(selected_data, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f36ecb1c-fe91-447c-82b3-169b078f909b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = true)\n",
      " |-- total_cases: double (nullable = true)\n",
      " |-- total_deaths: double (nullable = true)\n",
      " |-- total_vaccinations: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af88ce0f-1a94-430d-97c3-9a9921bad5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+\n",
      "|continent|total_cases|total_deaths|\n",
      "+---------+-----------+------------+\n",
      "|     Asia|   235214.0|      7998.0|\n",
      "|        0| 1.314538E7|    259117.0|\n",
      "|   Europe|   335047.0|      3605.0|\n",
      "|   Africa|   272139.0|      6881.0|\n",
      "|  Oceania|     8359.0|        34.0|\n",
      "+---------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------+------------+\n",
      "|    continent| total_cases|\n",
      "+-------------+------------+\n",
      "|            0|  1.314538E7|\n",
      "|South America| 1.0101218E7|\n",
      "|            0|3.01499099E8|\n",
      "|      Oceania| 1.1861161E7|\n",
      "|       Europe|   6082444.0|\n",
      "|         Asia|   2051348.0|\n",
      "|       Europe|   4872829.0|\n",
      "|South America|   1212147.0|\n",
      "|South America| 3.7511921E7|\n",
      "|       Europe|   1329988.0|\n",
      "|North America|   4819055.0|\n",
      "|South America|   5401126.0|\n",
      "|         Asia| 9.9373219E7|\n",
      "|South America|   6391876.0|\n",
      "|North America|   1234701.0|\n",
      "|       Europe|   1317144.0|\n",
      "|North America|   1113662.0|\n",
      "|       Europe|   4761919.0|\n",
      "|       Europe|   3435679.0|\n",
      "|South America|   1077445.0|\n",
      "+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.select(\"continent\", \"total_cases\", \"total_deaths\").show(5)\n",
    "\n",
    "# Filtrer les données où les cas dépassent 1 million\n",
    "spark_df.filter(spark_df.total_cases > 1_000_000).select(\"continent\", \"total_cases\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06e81bff-3803-48e3-8d7f-c2e5d585e06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-------------+--------------+\n",
      "|continent|total_deaths|   population|mortality_rate|\n",
      "+---------+------------+-------------+--------------+\n",
      "|     Asia|      7998.0|  4.1128772E7|         0.02%|\n",
      "|        0|    259117.0|1.426736614E9|         0.02%|\n",
      "|   Europe|      3605.0|    2842318.0|         0.13%|\n",
      "|   Africa|      6881.0|  4.4903228E7|         0.02%|\n",
      "|  Oceania|        34.0|      44295.0|         0.08%|\n",
      "+---------+------------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, format_number, concat, lit\n",
    "\n",
    "# Calculer le taux de mortalité\n",
    "spark_df = spark_df.withColumn(\n",
    "    \"mortality_rate\",\n",
    "    format_number((col(\"total_deaths\") / col(\"population\")) * 100, 2)\n",
    ")\n",
    "\n",
    "# Ajouter le symbole %\n",
    "spark_df = spark_df.withColumn(\n",
    "    \"mortality_rate\",\n",
    "    concat(col(\"mortality_rate\"), lit(\"%\"))\n",
    ")\n",
    "\n",
    "# Afficher quelques lignes\n",
    "spark_df.select(\"continent\", \"total_deaths\", \"population\", \"mortality_rate\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c788f9fa-3ebf-4574-b2d2-c90b4edef9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: findspark in /opt/conda/lib/python3.11/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install pyspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211d6c55-7e67-4668-bd27-4d870f55585d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
