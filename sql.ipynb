{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc09d60c-2303-4919-97ea-ce5b0696661c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2eef137-7c70-4828-ac81-84b28093b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f68319a-18e0-479f-80a8-65c4404f9b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"tp-spark\").config(\"spark.ui.port\",\n",
    "\"4050\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c298a6ab-cb6d-4b7d-bbbd-f9913567b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5615b754-9cc9-4576-a1c0-905ed14a7ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialiser SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\n",
    "\n",
    "# Définir le schéma\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\", StringType(), True),\n",
    "    StructField(\"middlename\", StringType(), True),\n",
    "    StructField(\"lastname\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Données\n",
    "data = [\n",
    "    (\"James\", \"\", \"Smith\", \"36636\", \"M\", 3000),\n",
    "    (\"Michael\", \"Rose\", \"\", \"40288\", \"M\", 4000),\n",
    "    (\"Robert\", \"\", \"Williams\", \"42114\", \"M\", 4000),\n",
    "    (\"Maria\", \"Anne\", \"Jones\", \"39192\", \"F\", 4000),\n",
    "    (\"Jen\", \"Mary\", \"Brown\", \"\", \"F\", -1)\n",
    "]\n",
    "\n",
    "# Créer DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Afficher schéma et données\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "985e16fa-b47c-4279-af2f-b7b0e39ce076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- identifiant: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_renamed = df.withColumnRenamed(\"id\", \"identifiant\")\n",
    "df_renamed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c890a245-33ab-47cb-a1c0-c982c7fb0aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_femmes = df.filter(df.gender == \"F\")\n",
    "df_femmes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "367d5196-99b9-42dd-a328-3d09a22668b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_salaire_diff = df.filter(df.salary != 4000)\n",
    "df_salaire_diff.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd80cd5c-2499-47ed-896f-bd8f0a81935a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy([\"lastname\", \"firstname\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf844ab0-f326-40e2-9946-9b8473692b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"lastname\", \"firstname\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ee010bc-18dd-4edf-a3cf-55f0272182e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package pyspark:\n",
      "\n",
      "NAME\n",
      "    pyspark - PySpark is the Python API for Spark.\n",
      "\n",
      "DESCRIPTION\n",
      "    Public classes:\n",
      "    \n",
      "      - :class:`SparkContext`:\n",
      "          Main entry point for Spark functionality.\n",
      "      - :class:`RDD`:\n",
      "          A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
      "      - :class:`Broadcast`:\n",
      "          A broadcast variable that gets reused across tasks.\n",
      "      - :class:`Accumulator`:\n",
      "          An \"add-only\" shared variable that tasks can only add values to.\n",
      "      - :class:`SparkConf`:\n",
      "          For configuring Spark.\n",
      "      - :class:`SparkFiles`:\n",
      "          Access files shipped with jobs.\n",
      "      - :class:`StorageLevel`:\n",
      "          Finer-grained cache persistence levels.\n",
      "      - :class:`TaskContext`:\n",
      "          Information about the current running task, available on the workers and experimental.\n",
      "      - :class:`RDDBarrier`:\n",
      "          Wraps an RDD under a barrier stage for barrier execution.\n",
      "      - :class:`BarrierTaskContext`:\n",
      "          A :class:`TaskContext` that provides extra info and tooling for barrier execution.\n",
      "      - :class:`BarrierTaskInfo`:\n",
      "          Information about a barrier task.\n",
      "      - :class:`InheritableThread`:\n",
      "          A inheritable thread to use in Spark when the pinned thread mode is on.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _globals\n",
      "    accumulators\n",
      "    broadcast\n",
      "    cloudpickle (package)\n",
      "    conf\n",
      "    context\n",
      "    daemon\n",
      "    errors (package)\n",
      "    files\n",
      "    find_spark_home\n",
      "    install\n",
      "    instrumentation_utils\n",
      "    java_gateway\n",
      "    join\n",
      "    ml (package)\n",
      "    mllib (package)\n",
      "    pandas (package)\n",
      "    profiler\n",
      "    rdd\n",
      "    rddsampler\n",
      "    resource (package)\n",
      "    resultiterable\n",
      "    serializers\n",
      "    shell\n",
      "    shuffle\n",
      "    sql (package)\n",
      "    statcounter\n",
      "    status\n",
      "    storagelevel\n",
      "    streaming (package)\n",
      "    taskcontext\n",
      "    testing (package)\n",
      "    tests (package)\n",
      "    traceback_utils\n",
      "    util\n",
      "    version\n",
      "    worker\n",
      "    worker_util\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        pyspark.conf.SparkConf\n",
      "        pyspark.context.SparkContext\n",
      "        pyspark.files.SparkFiles\n",
      "        pyspark.profiler.Profiler\n",
      "            pyspark.profiler.BasicProfiler\n",
      "        pyspark.status.StatusTracker\n",
      "        pyspark.storagelevel.StorageLevel\n",
      "        pyspark.taskcontext.BarrierTaskInfo\n",
      "        pyspark.taskcontext.TaskContext\n",
      "            pyspark.taskcontext.BarrierTaskContext\n",
      "    builtins.tuple(builtins.object)\n",
      "        pyspark.status.SparkJobInfo\n",
      "        pyspark.status.SparkStageInfo\n",
      "    pyspark.serializers.FramedSerializer(pyspark.serializers.Serializer)\n",
      "        pyspark.serializers.CloudPickleSerializer\n",
      "        pyspark.serializers.MarshalSerializer\n",
      "    threading.Thread(builtins.object)\n",
      "        pyspark.util.InheritableThread\n",
      "    typing.Generic(builtins.object)\n",
      "        pyspark.accumulators.Accumulator\n",
      "        pyspark.accumulators.AccumulatorParam\n",
      "        pyspark.broadcast.Broadcast\n",
      "        pyspark.rdd.RDD\n",
      "        pyspark.rdd.RDDBarrier\n",
      "    \n",
      "    class Accumulator(typing.Generic)\n",
      "     |  Accumulator(aid: int, value: ~T, accum_param: 'AccumulatorParam[T]')\n",
      "     |  \n",
      "     |  A shared variable that can be accumulated, i.e., has a commutative and associative \"add\"\n",
      "     |  operation. Worker tasks on a Spark cluster can add values to an Accumulator with the `+=`\n",
      "     |  operator, but only the driver program is allowed to access its value, using `value`.\n",
      "     |  Updates from the workers get propagated automatically to the driver program.\n",
      "     |  \n",
      "     |  While :class:`SparkContext` supports accumulators for primitive data types like :class:`int` and\n",
      "     |  :class:`float`, users can also define accumulators for custom types by providing a custom\n",
      "     |  :py:class:`AccumulatorParam` object. Refer to its doctest for an example.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> a = sc.accumulator(1)\n",
      "     |  >>> a.value\n",
      "     |  1\n",
      "     |  >>> a.value = 2\n",
      "     |  >>> a.value\n",
      "     |  2\n",
      "     |  >>> a += 5\n",
      "     |  >>> a.value\n",
      "     |  7\n",
      "     |  >>> sc.accumulator(1.0).value\n",
      "     |  1.0\n",
      "     |  >>> sc.accumulator(1j).value\n",
      "     |  1j\n",
      "     |  >>> rdd = sc.parallelize([1,2,3])\n",
      "     |  >>> def f(x):\n",
      "     |  ...     global a\n",
      "     |  ...     a += x\n",
      "     |  ...\n",
      "     |  >>> rdd.foreach(f)\n",
      "     |  >>> a.value\n",
      "     |  13\n",
      "     |  >>> b = sc.accumulator(0)\n",
      "     |  >>> def g(x):\n",
      "     |  ...     b.add(x)\n",
      "     |  ...\n",
      "     |  >>> rdd.foreach(g)\n",
      "     |  >>> b.value\n",
      "     |  6\n",
      "     |  \n",
      "     |  >>> rdd.map(lambda x: a.value).collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "     |  Traceback (most recent call last):\n",
      "     |      ...\n",
      "     |  Py4JJavaError: ...\n",
      "     |  \n",
      "     |  >>> def h(x):\n",
      "     |  ...     global a\n",
      "     |  ...     a.value = 7\n",
      "     |  ...\n",
      "     |  >>> rdd.foreach(h) # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "     |  Traceback (most recent call last):\n",
      "     |      ...\n",
      "     |  Py4JJavaError: ...\n",
      "     |  \n",
      "     |  >>> sc.accumulator([1.0, 2.0, 3.0]) # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "     |  Traceback (most recent call last):\n",
      "     |      ...\n",
      "     |  TypeError: ...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Accumulator\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __iadd__(self, term: ~T) -> 'Accumulator[T]'\n",
      "     |      The += operator; adds a term to this accumulator's value\n",
      "     |  \n",
      "     |  __init__(self, aid: int, value: ~T, accum_param: 'AccumulatorParam[T]')\n",
      "     |      Create a new Accumulator with a given initial value and AccumulatorParam object\n",
      "     |  \n",
      "     |  __reduce__(self) -> Tuple[Callable[[int, ~T, ForwardRef('AccumulatorParam[T]')], ForwardRef('Accumulator[T]')], Tuple[int, ~T, ForwardRef('AccumulatorParam[T]')]]\n",
      "     |      Custom serialization; saves the zero value from our AccumulatorParam\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __str__(self) -> str\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  add(self, term: ~T) -> None\n",
      "     |      Adds a term to this accumulator's value\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  value\n",
      "     |      Get the accumulator's value; only usable in driver program\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[~T],)\n",
      "     |  \n",
      "     |  __parameters__ = (~T,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |      Parameterizes a generic class.\n",
      "     |      \n",
      "     |      At least, parameterizing a generic class is the *main* thing this method\n",
      "     |      does. For example, for some generic class `Foo`, this is called when we\n",
      "     |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      "     |      \n",
      "     |      However, note that this method is also called when defining generic\n",
      "     |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class AccumulatorParam(typing.Generic)\n",
      "     |  Helper object that defines how to accumulate values of a given type.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from pyspark.accumulators import AccumulatorParam\n",
      "     |  >>> class VectorAccumulatorParam(AccumulatorParam):\n",
      "     |  ...     def zero(self, value):\n",
      "     |  ...         return [0.0] * len(value)\n",
      "     |  ...     def addInPlace(self, val1, val2):\n",
      "     |  ...         for i in range(len(val1)):\n",
      "     |  ...              val1[i] += val2[i]\n",
      "     |  ...         return val1\n",
      "     |  >>> va = sc.accumulator([1.0, 2.0, 3.0], VectorAccumulatorParam())\n",
      "     |  >>> va.value\n",
      "     |  [1.0, 2.0, 3.0]\n",
      "     |  >>> def g(x):\n",
      "     |  ...     global va\n",
      "     |  ...     va += [x] * 3\n",
      "     |  ...\n",
      "     |  >>> rdd = sc.parallelize([1,2,3])\n",
      "     |  >>> rdd.foreach(g)\n",
      "     |  >>> va.value\n",
      "     |  [7.0, 8.0, 9.0]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AccumulatorParam\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  addInPlace(self, value1: ~T, value2: ~T) -> ~T\n",
      "     |      Add two values of the accumulator's data type, returning a new value;\n",
      "     |      for efficiency, can also update `value1` in place and return it.\n",
      "     |  \n",
      "     |  zero(self, value: ~T) -> ~T\n",
      "     |      Provide a \"zero value\" for the type, compatible in dimensions with the\n",
      "     |      provided `value` (e.g., a zero vector)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[~T],)\n",
      "     |  \n",
      "     |  __parameters__ = (~T,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |      Parameterizes a generic class.\n",
      "     |      \n",
      "     |      At least, parameterizing a generic class is the *main* thing this method\n",
      "     |      does. For example, for some generic class `Foo`, this is called when we\n",
      "     |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      "     |      \n",
      "     |      However, note that this method is also called when defining generic\n",
      "     |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class BarrierTaskContext(TaskContext)\n",
      "     |  BarrierTaskContext() -> 'TaskContext'\n",
      "     |  \n",
      "     |  A :class:`TaskContext` with extra contextual info and tooling for tasks in a barrier stage.\n",
      "     |  Use :func:`BarrierTaskContext.get` to obtain the barrier context for a running barrier task.\n",
      "     |  \n",
      "     |  .. versionadded:: 2.4.0\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  This API is experimental\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Set a barrier, and execute it with RDD.\n",
      "     |  \n",
      "     |  >>> from pyspark import BarrierTaskContext\n",
      "     |  >>> def block_and_do_something(itr):\n",
      "     |  ...     taskcontext = BarrierTaskContext.get()\n",
      "     |  ...     # Do something.\n",
      "     |  ...\n",
      "     |  ...     # Wait until all tasks finished.\n",
      "     |  ...     taskcontext.barrier()\n",
      "     |  ...\n",
      "     |  ...     return itr\n",
      "     |  ...\n",
      "     |  >>> rdd = spark.sparkContext.parallelize([1])\n",
      "     |  >>> rdd.barrier().mapPartitions(block_and_do_something).collect()\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BarrierTaskContext\n",
      "     |      TaskContext\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  allGather(self, message: str = '') -> List[str]\n",
      "     |      This function blocks until all tasks in the same stage have reached this routine.\n",
      "     |      Each task passes in a message and returns with a list of all the messages passed in\n",
      "     |      by each of those tasks.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is experimental\n",
      "     |      \n",
      "     |      In a barrier stage, each task much have the same number of `barrier()`\n",
      "     |      calls, in all possible code branches. Otherwise, you may get the job hanging\n",
      "     |      or a `SparkException` after timeout.\n",
      "     |  \n",
      "     |  barrier(self) -> None\n",
      "     |      Sets a global barrier and waits until all tasks in this stage hit this barrier.\n",
      "     |      Similar to `MPI_Barrier` function in MPI, this function blocks until all tasks\n",
      "     |      in the same stage have reached this routine.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is experimental\n",
      "     |      \n",
      "     |      In a barrier stage, each task much have the same number of `barrier()`\n",
      "     |      calls, in all possible code branches. Otherwise, you may get the job hanging\n",
      "     |      or a `SparkException` after timeout.\n",
      "     |  \n",
      "     |  getTaskInfos(self) -> List[ForwardRef('BarrierTaskInfo')]\n",
      "     |      Returns :class:`BarrierTaskInfo` for all tasks in this barrier stage,\n",
      "     |      ordered by partition ID.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is experimental\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark import BarrierTaskContext\n",
      "     |      >>> rdd = spark.sparkContext.parallelize([1])\n",
      "     |      >>> barrier_info = rdd.barrier().mapPartitions(\n",
      "     |      ...     lambda _: [BarrierTaskContext.get().getTaskInfos()]).collect()[0][0]\n",
      "     |      >>> barrier_info.address\n",
      "     |      '...:...'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  get() -> 'BarrierTaskContext' from builtins.type\n",
      "     |      Return the currently active :class:`BarrierTaskContext`.\n",
      "     |      This can be called inside of user functions to access contextual information about\n",
      "     |      running tasks.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Must be called on the worker, not the driver. Returns ``None`` if not initialized.\n",
      "     |      An Exception will raise if it is not in a barrier stage.\n",
      "     |      \n",
      "     |      This API is experimental\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'_port': typing.ClassVar[typing.Union[int, str, Non...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from TaskContext:\n",
      "     |  \n",
      "     |  attemptNumber(self) -> int\n",
      "     |      How many times this task has been attempted.  The first task attempt will be assigned\n",
      "     |      attemptNumber = 0, and subsequent attempts will have increasing attempt numbers.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          current attempt number.\n",
      "     |  \n",
      "     |  cpus(self) -> int\n",
      "     |      CPUs allocated to the task.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          the number of CPUs.\n",
      "     |  \n",
      "     |  getLocalProperty(self, key: str) -> Optional[str]\n",
      "     |      Get a local property set upstream in the driver, or None if it is missing.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key : str\n",
      "     |          the key of the local property to get.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          the value of the local property.\n",
      "     |  \n",
      "     |  partitionId(self) -> int\n",
      "     |      The ID of the RDD partition that is computed by this task.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          current partition id.\n",
      "     |  \n",
      "     |  resources(self) -> Dict[str, pyspark.resource.information.ResourceInformation]\n",
      "     |      Resources allocated to the task. The key is the resource name and the value is information\n",
      "     |      about the resource.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dict\n",
      "     |          a dictionary of a string resource name, and :class:`ResourceInformation`.\n",
      "     |  \n",
      "     |  stageId(self) -> int\n",
      "     |      The ID of the stage that this task belong to.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          current stage id.\n",
      "     |  \n",
      "     |  taskAttemptId(self) -> int\n",
      "     |      An ID that is unique to this task attempt (within the same :class:`SparkContext`,\n",
      "     |      no two task attempts will share the same attempt ID).  This is roughly equivalent\n",
      "     |      to Hadoop's `TaskAttemptID`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          current task attempt id.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from TaskContext:\n",
      "     |  \n",
      "     |  __new__(cls: Type[ForwardRef('TaskContext')]) -> 'TaskContext'\n",
      "     |      Even if users construct :class:`TaskContext` instead of using get, give them the singleton.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from TaskContext:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class BarrierTaskInfo(builtins.object)\n",
      "     |  BarrierTaskInfo(address: str) -> None\n",
      "     |  \n",
      "     |  Carries all task infos of a barrier task.\n",
      "     |  \n",
      "     |  .. versionadded:: 2.4.0\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  address : str\n",
      "     |      The IPv4 address (host:port) of the executor that the barrier task is running on\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  This API is experimental\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, address: str) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class BasicProfiler(Profiler)\n",
      "     |  BasicProfiler(ctx: 'SparkContext') -> None\n",
      "     |  \n",
      "     |  BasicProfiler is the default profiler, which is implemented based on\n",
      "     |  cProfile and Accumulator\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BasicProfiler\n",
      "     |      Profiler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, ctx: 'SparkContext') -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  dump(self, id: int, path: str) -> None\n",
      "     |      Dump the profile into path, id is the RDD id\n",
      "     |  \n",
      "     |  profile(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -> Any\n",
      "     |      Runs and profiles the method to_profile passed in. A profile object is returned.\n",
      "     |  \n",
      "     |  show(self, id: int) -> None\n",
      "     |      Print the profile stats to stdout, id is the RDD id\n",
      "     |  \n",
      "     |  stats(self) -> pstats.Stats\n",
      "     |      Return the collected profiling stats\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Profiler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Broadcast(typing.Generic)\n",
      "     |  Broadcast(sc: Optional[ForwardRef('SparkContext')] = None, value: Optional[~T] = None, pickle_registry: Optional[ForwardRef('BroadcastPickleRegistry')] = None, path: Optional[str] = None, sock_file: Optional[BinaryIO] = None)\n",
      "     |  \n",
      "     |  A broadcast variable created with :meth:`SparkContext.broadcast`.\n",
      "     |  Access its value through :attr:`value`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> b = spark.sparkContext.broadcast([1, 2, 3, 4, 5])\n",
      "     |  >>> b.value\n",
      "     |  [1, 2, 3, 4, 5]\n",
      "     |  >>> spark.sparkContext.parallelize([0, 0]).flatMap(lambda x: b.value).collect()\n",
      "     |  [1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n",
      "     |  >>> b.unpersist()\n",
      "     |  \n",
      "     |  >>> large_broadcast = spark.sparkContext.broadcast(range(10000))\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Broadcast\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sc: Optional[ForwardRef('SparkContext')] = None, value: Optional[~T] = None, pickle_registry: Optional[ForwardRef('BroadcastPickleRegistry')] = None, path: Optional[str] = None, sock_file: Optional[BinaryIO] = None)\n",
      "     |      Should not be called directly by users -- use :meth:`SparkContext.broadcast`\n",
      "     |      instead.\n",
      "     |  \n",
      "     |  __reduce__(self) -> Tuple[Callable[[int], ForwardRef('Broadcast[T]')], Tuple[int]]\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  destroy(self, blocking: bool = False) -> None\n",
      "     |      Destroy all data and metadata related to this broadcast variable.\n",
      "     |      Use this with caution; once a broadcast variable has been destroyed,\n",
      "     |      it cannot be used again.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.0.0\n",
      "     |         Added optional argument `blocking` to specify whether to block until all\n",
      "     |         blocks are deleted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      blocking : bool, optional, default False\n",
      "     |          Whether to block until unpersisting has completed.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> b = spark.sparkContext.broadcast([1, 2, 3, 4, 5])\n",
      "     |      \n",
      "     |      Destroy all data and metadata related to this broadcast variable\n",
      "     |      \n",
      "     |      >>> b.destroy()\n",
      "     |  \n",
      "     |  dump(self, value: ~T, f: <class 'BinaryIO'>) -> None\n",
      "     |      Write a pickled representation of value to the open file or socket.\n",
      "     |      The protocol pickle is HIGHEST_PROTOCOL.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      value : T\n",
      "     |          Value to write.\n",
      "     |      \n",
      "     |      f : :class:`BinaryIO`\n",
      "     |          File or socket where the pickled value will be stored.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      \n",
      "     |      >>> b = spark.sparkContext.broadcast([1, 2, 3, 4, 5])\n",
      "     |      \n",
      "     |      Write a pickled representation of `b` to the open temp file.\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     path = os.path.join(d, \"test.txt\")\n",
      "     |      ...     with open(path, \"wb\") as f:\n",
      "     |      ...         b.dump(b.value, f)\n",
      "     |  \n",
      "     |  load(self, file: <class 'BinaryIO'>) -> ~T\n",
      "     |      Read a pickled representation of value from the open file or socket.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      file : :class:`BinaryIO`\n",
      "     |          File or socket where the pickled value will be read.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T\n",
      "     |          The object hierarchy specified therein reconstituted\n",
      "     |          from the pickled representation of an object.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      \n",
      "     |      >>> b = spark.sparkContext.broadcast([1, 2, 3, 4, 5])\n",
      "     |      >>> c = spark.sparkContext.broadcast(1)\n",
      "     |      \n",
      "     |      Read the pickled representation of value from the open temp file.\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     path = os.path.join(d, \"test.txt\")\n",
      "     |      ...     with open(path, \"wb\") as f:\n",
      "     |      ...         b.dump(b.value, f)\n",
      "     |      ...     with open(path, \"rb\") as f:\n",
      "     |      ...         c.load(f)\n",
      "     |      [1, 2, 3, 4, 5]\n",
      "     |  \n",
      "     |  load_from_path(self, path: str) -> ~T\n",
      "     |      Read the pickled representation of an object from the open file and\n",
      "     |      return the reconstituted object hierarchy specified therein.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          File path where reads the pickled value.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T\n",
      "     |          The object hierarchy specified therein reconstituted\n",
      "     |          from the pickled representation of an object.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      \n",
      "     |      >>> b = spark.sparkContext.broadcast([1, 2, 3, 4, 5])\n",
      "     |      >>> c = spark.sparkContext.broadcast(1)\n",
      "     |      \n",
      "     |      Read the pickled representation of value from temp file.\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     path = os.path.join(d, \"test.txt\")\n",
      "     |      ...     with open(path, \"wb\") as f:\n",
      "     |      ...         b.dump(b.value, f)\n",
      "     |      ...     c.load_from_path(path)\n",
      "     |      [1, 2, 3, 4, 5]\n",
      "     |  \n",
      "     |  unpersist(self, blocking: bool = False) -> None\n",
      "     |      Delete cached copies of this broadcast on the executors. If the\n",
      "     |      broadcast is used after this is called, it will need to be\n",
      "     |      re-sent to each executor.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      blocking : bool, optional, default False\n",
      "     |          Whether to block until unpersisting has completed.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> b = spark.sparkContext.broadcast([1, 2, 3, 4, 5])\n",
      "     |      \n",
      "     |      Delete cached copies of this broadcast on the executors\n",
      "     |      \n",
      "     |      >>> b.unpersist()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  value\n",
      "     |      Return the broadcasted value\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[~T],)\n",
      "     |  \n",
      "     |  __parameters__ = (~T,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |      Parameterizes a generic class.\n",
      "     |      \n",
      "     |      At least, parameterizing a generic class is the *main* thing this method\n",
      "     |      does. For example, for some generic class `Foo`, this is called when we\n",
      "     |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      "     |      \n",
      "     |      However, note that this method is also called when defining generic\n",
      "     |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    CPickleSerializer = class CloudPickleSerializer(FramedSerializer)\n",
      "     |  Method resolution order:\n",
      "     |      CloudPickleSerializer\n",
      "     |      FramedSerializer\n",
      "     |      Serializer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  dumps(self, obj)\n",
      "     |      Serialize an object into a byte array.\n",
      "     |      When batching is used, this will be called with an array of objects.\n",
      "     |  \n",
      "     |  loads(self, obj, encoding='bytes')\n",
      "     |      Deserialize an object from a byte array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __slotnames__ = []\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from FramedSerializer:\n",
      "     |  \n",
      "     |  dump_stream(self, iterator, stream)\n",
      "     |      Serialize an iterator of objects to the output stream.\n",
      "     |  \n",
      "     |  load_stream(self, stream)\n",
      "     |      Return an iterator of deserialized objects from the input stream.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Serializer:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Serializer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class InheritableThread(threading.Thread)\n",
      "     |  InheritableThread(target: Callable, *args: Any, **kwargs: Any)\n",
      "     |  \n",
      "     |  Thread that is recommended to be used in PySpark instead of :class:`threading.Thread`\n",
      "     |  when the pinned thread mode is enabled. The usage of this class is exactly same as\n",
      "     |  :class:`threading.Thread` but correctly inherits the inheritable properties specific\n",
      "     |  to JVM thread such as ``InheritableThreadLocal``.\n",
      "     |  \n",
      "     |  Also, note that pinned thread mode does not close the connection from Python\n",
      "     |  to JVM when the thread is finished in the Python side. With this class, Python\n",
      "     |  garbage-collects the Python thread instance and also closes the connection\n",
      "     |  which finishes JVM thread correctly.\n",
      "     |  \n",
      "     |  When the pinned thread mode is off, this works as :class:`threading.Thread`.\n",
      "     |  \n",
      "     |  .. versionadded:: 3.1.0\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  This API is experimental.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      InheritableThread\n",
      "     |      threading.Thread\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, target: Callable, *args: Any, **kwargs: Any)\n",
      "     |      This constructor should always be called with keyword arguments. Arguments are:\n",
      "     |      \n",
      "     |      *group* should be None; reserved for future extension when a ThreadGroup\n",
      "     |      class is implemented.\n",
      "     |      \n",
      "     |      *target* is the callable object to be invoked by the run()\n",
      "     |      method. Defaults to None, meaning nothing is called.\n",
      "     |      \n",
      "     |      *name* is the thread name. By default, a unique name is constructed of\n",
      "     |      the form \"Thread-N\" where N is a small decimal number.\n",
      "     |      \n",
      "     |      *args* is a list or tuple of arguments for the target invocation. Defaults to ().\n",
      "     |      \n",
      "     |      *kwargs* is a dictionary of keyword arguments for the target\n",
      "     |      invocation. Defaults to {}.\n",
      "     |      \n",
      "     |      If a subclass overrides the constructor, it must make sure to invoke\n",
      "     |      the base class constructor (Thread.__init__()) before doing anything\n",
      "     |      else to the thread.\n",
      "     |  \n",
      "     |  start(self) -> None\n",
      "     |      Start the thread's activity.\n",
      "     |      \n",
      "     |      It must be called at most once per thread object. It arranges for the\n",
      "     |      object's run() method to be invoked in a separate thread of control.\n",
      "     |      \n",
      "     |      This method will raise a RuntimeError if called more than once on the\n",
      "     |      same thread object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'_props': <class 'py4j.java_gateway.JavaObject'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from threading.Thread:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getName(self)\n",
      "     |      Return a string used for identification purposes only.\n",
      "     |      \n",
      "     |      This method is deprecated, use the name attribute instead.\n",
      "     |  \n",
      "     |  isDaemon(self)\n",
      "     |      Return whether this thread is a daemon.\n",
      "     |      \n",
      "     |      This method is deprecated, use the daemon attribute instead.\n",
      "     |  \n",
      "     |  is_alive(self)\n",
      "     |      Return whether the thread is alive.\n",
      "     |      \n",
      "     |      This method returns True just before the run() method starts until just\n",
      "     |      after the run() method terminates. See also the module function\n",
      "     |      enumerate().\n",
      "     |  \n",
      "     |  join(self, timeout=None)\n",
      "     |      Wait until the thread terminates.\n",
      "     |      \n",
      "     |      This blocks the calling thread until the thread whose join() method is\n",
      "     |      called terminates -- either normally or through an unhandled exception\n",
      "     |      or until the optional timeout occurs.\n",
      "     |      \n",
      "     |      When the timeout argument is present and not None, it should be a\n",
      "     |      floating point number specifying a timeout for the operation in seconds\n",
      "     |      (or fractions thereof). As join() always returns None, you must call\n",
      "     |      is_alive() after join() to decide whether a timeout happened -- if the\n",
      "     |      thread is still alive, the join() call timed out.\n",
      "     |      \n",
      "     |      When the timeout argument is not present or None, the operation will\n",
      "     |      block until the thread terminates.\n",
      "     |      \n",
      "     |      A thread can be join()ed many times.\n",
      "     |      \n",
      "     |      join() raises a RuntimeError if an attempt is made to join the current\n",
      "     |      thread as that would cause a deadlock. It is also an error to join() a\n",
      "     |      thread before it has been started and attempts to do so raises the same\n",
      "     |      exception.\n",
      "     |  \n",
      "     |  run(self)\n",
      "     |      Method representing the thread's activity.\n",
      "     |      \n",
      "     |      You may override this method in a subclass. The standard run() method\n",
      "     |      invokes the callable object passed to the object's constructor as the\n",
      "     |      target argument, if any, with sequential and keyword arguments taken\n",
      "     |      from the args and kwargs arguments, respectively.\n",
      "     |  \n",
      "     |  setDaemon(self, daemonic)\n",
      "     |      Set whether this thread is a daemon.\n",
      "     |      \n",
      "     |      This method is deprecated, use the .daemon property instead.\n",
      "     |  \n",
      "     |  setName(self, name)\n",
      "     |      Set the name string for this thread.\n",
      "     |      \n",
      "     |      This method is deprecated, use the name attribute instead.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from threading.Thread:\n",
      "     |  \n",
      "     |  ident\n",
      "     |      Thread identifier of this thread or None if it has not been started.\n",
      "     |      \n",
      "     |      This is a nonzero integer. See the get_ident() function. Thread\n",
      "     |      identifiers may be recycled when a thread exits and another thread is\n",
      "     |      created. The identifier is available even after the thread has exited.\n",
      "     |  \n",
      "     |  native_id\n",
      "     |      Native integral thread ID of this thread, or None if it has not been started.\n",
      "     |      \n",
      "     |      This is a non-negative integer. See the get_native_id() function.\n",
      "     |      This represents the Thread ID as reported by the kernel.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from threading.Thread:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  daemon\n",
      "     |      A boolean value indicating whether this thread is a daemon thread.\n",
      "     |      \n",
      "     |      This must be set before start() is called, otherwise RuntimeError is\n",
      "     |      raised. Its initial value is inherited from the creating thread; the\n",
      "     |      main thread is not a daemon thread and therefore all threads created in\n",
      "     |      the main thread default to daemon = False.\n",
      "     |      \n",
      "     |      The entire Python program exits when only daemon threads are left.\n",
      "     |  \n",
      "     |  name\n",
      "     |      A string used for identification purposes only.\n",
      "     |      \n",
      "     |      It has no semantics. Multiple threads may be given the same name. The\n",
      "     |      initial name is set by the constructor.\n",
      "    \n",
      "    class MarshalSerializer(FramedSerializer)\n",
      "     |  Serializes objects using Python's Marshal serializer:\n",
      "     |  \n",
      "     |      http://docs.python.org/2/library/marshal.html\n",
      "     |  \n",
      "     |  This serializer is faster than CloudPickleSerializer but supports fewer datatypes.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MarshalSerializer\n",
      "     |      FramedSerializer\n",
      "     |      Serializer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  dumps(self, obj)\n",
      "     |      Serialize an object into a byte array.\n",
      "     |      When batching is used, this will be called with an array of objects.\n",
      "     |  \n",
      "     |  loads(self, obj)\n",
      "     |      Deserialize an object from a byte array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from FramedSerializer:\n",
      "     |  \n",
      "     |  dump_stream(self, iterator, stream)\n",
      "     |      Serialize an iterator of objects to the output stream.\n",
      "     |  \n",
      "     |  load_stream(self, stream)\n",
      "     |      Return an iterator of deserialized objects from the input stream.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Serializer:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Serializer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Profiler(builtins.object)\n",
      "     |  Profiler(ctx: 'SparkContext') -> None\n",
      "     |  \n",
      "     |  PySpark supports custom profilers, this is to allow for different profilers to\n",
      "     |  be used as well as outputting to different formats than what is provided in the\n",
      "     |  BasicProfiler.\n",
      "     |  \n",
      "     |  A custom profiler has to define or inherit the following methods:\n",
      "     |      profile - will produce a system profile of some sort.\n",
      "     |      stats - return the collected stats.\n",
      "     |      dump - dumps the profiles to a path\n",
      "     |      add - adds a profile to the existing accumulated profile\n",
      "     |  \n",
      "     |  The profiler class is chosen when creating a SparkContext\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from pyspark import SparkConf, SparkContext\n",
      "     |  >>> from pyspark import BasicProfiler\n",
      "     |  >>> class MyCustomProfiler(BasicProfiler):\n",
      "     |  ...     def show(self, id):\n",
      "     |  ...         print(\"My custom profiles for RDD:%s\" % id)\n",
      "     |  ...\n",
      "     |  >>> conf = SparkConf().set(\"spark.python.profile\", \"true\")\n",
      "     |  >>> sc = SparkContext('local', 'test', conf=conf, profiler_cls=MyCustomProfiler)\n",
      "     |  >>> sc.parallelize(range(1000)).map(lambda x: 2 * x).take(10)\n",
      "     |  [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
      "     |  >>> sc.parallelize(range(1000)).count()\n",
      "     |  1000\n",
      "     |  >>> sc.show_profiles()\n",
      "     |  My custom profiles for RDD:1\n",
      "     |  My custom profiles for RDD:3\n",
      "     |  >>> sc.stop()\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  This API is a developer API.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, ctx: 'SparkContext') -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  dump(self, id: int, path: str) -> None\n",
      "     |      Dump the profile into path\n",
      "     |  \n",
      "     |  profile(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -> Any\n",
      "     |      Do profiling on the function `func`\n",
      "     |  \n",
      "     |  show(self, id: int) -> None\n",
      "     |      Print the profile stats to stdout\n",
      "     |  \n",
      "     |  stats(self) -> Union[pstats.Stats, Dict]\n",
      "     |      Return the collected profiling stats\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class RDD(typing.Generic)\n",
      "     |  RDD(jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n",
      "     |  \n",
      "     |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
      "     |  Represents an immutable, partitioned collection of elements that can be\n",
      "     |  operated on in parallel.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RDD\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __add__(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n",
      "     |      Return the union of this RDD and another one.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      "     |      >>> (rdd + rdd).collect()\n",
      "     |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      "     |  \n",
      "     |  __getnewargs__(self) -> NoReturn\n",
      "     |  \n",
      "     |  __init__(self, jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  aggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U]) -> ~U\n",
      "     |      Aggregate the elements of each partition, and then the results for all\n",
      "     |      the partitions, using a given combine functions and a neutral \"zero\n",
      "     |      value.\"\n",
      "     |      \n",
      "     |      The functions ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n",
      "     |      as its result value to avoid object allocation; however, it should not\n",
      "     |      modify ``t2``.\n",
      "     |      \n",
      "     |      The first function (seqOp) can return a different result type, U, than\n",
      "     |      the type of this RDD. Thus, we need one operation for merging a T into\n",
      "     |      an U and one operation for merging two U\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      zeroValue : U\n",
      "     |          the initial value for the accumulated result of each partition\n",
      "     |      seqOp : function\n",
      "     |          a function used to accumulate results within a partition\n",
      "     |      combOp : function\n",
      "     |          an associative function used to combine results from different partitions\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      U\n",
      "     |          the aggregated result\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.reduce`\n",
      "     |      :meth:`RDD.fold`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
      "     |      >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
      "     |      >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n",
      "     |      (10, 4)\n",
      "     |      >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n",
      "     |      (0, 0)\n",
      "     |  \n",
      "     |  aggregateByKey(self: 'RDD[Tuple[K, V]]', zeroValue: ~U, seqFunc: Callable[[~U, ~V], ~U], combFunc: Callable[[~U, ~U], ~U], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f4ab007a520>) -> 'RDD[Tuple[K, U]]'\n",
      "     |      Aggregate the values of each key, using given combine functions and a neutral\n",
      "     |      \"zero value\". This function can return a different result type, U, than the type\n",
      "     |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n",
      "     |      a U and one operation for merging two U's, The former operation is used for merging\n",
      "     |      values within a partition, and the latter is used for merging values between\n",
      "     |      partitions. To avoid memory allocation, both of these functions are\n",
      "     |      allowed to modify and return their first argument instead of creating a new U.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      zeroValue : U\n",
      "     |          the initial value for the accumulated result of each partition\n",
      "     |      seqFunc : function\n",
      "     |          a function to merge a V into a U\n",
      "     |      combFunc : function\n",
      "     |          a function to combine two U's into a single one\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      partitionFunc : function, optional, default `portable_hash`\n",
      "     |          function to compute the partition index\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` containing the keys and the aggregated result for each key\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.reduceByKey`\n",
      "     |      :meth:`RDD.combineByKey`\n",
      "     |      :meth:`RDD.foldByKey`\n",
      "     |      :meth:`RDD.groupByKey`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
      "     |      >>> seqFunc = (lambda x, y: (x[0] + y, x[1] + 1))\n",
      "     |      >>> combFunc = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
      "     |      >>> sorted(rdd.aggregateByKey((0, 0), seqFunc, combFunc).collect())\n",
      "     |      [('a', (3, 2)), ('b', (1, 1))]\n",
      "     |  \n",
      "     |  barrier(self: 'RDD[T]') -> 'RDDBarrier[T]'\n",
      "     |      Marks the current stage as a barrier stage, where Spark must launch all tasks together.\n",
      "     |      In case of a task failure, instead of only restarting the failed task, Spark will abort the\n",
      "     |      entire stage and relaunch all tasks for this stage.\n",
      "     |      The barrier execution mode feature is experimental and it only handles limited scenarios.\n",
      "     |      Please read the linked SPIP and design docs to understand the limitations and future plans.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDDBarrier`\n",
      "     |          instance that provides actions within a barrier stage.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :class:`pyspark.BarrierTaskContext`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For additional information see\n",
      "     |      \n",
      "     |      - `SPIP: Barrier Execution Mode <http://jira.apache.org/jira/browse/SPARK-24374>`_\n",
      "     |      - `Design Doc <https://jira.apache.org/jira/browse/SPARK-24582>`_\n",
      "     |      \n",
      "     |      This API is experimental\n",
      "     |  \n",
      "     |  cache(self: 'RDD[T]') -> 'RDD[T]'\n",
      "     |      Persist this RDD with the default storage level (`MEMORY_ONLY`).\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          The same :class:`RDD` with storage level set to `MEMORY_ONLY`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.persist`\n",
      "     |      :meth:`RDD.unpersist`\n",
      "     |      :meth:`RDD.getStorageLevel`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.range(5)\n",
      "     |      >>> rdd2 = rdd.cache()\n",
      "     |      >>> rdd2 is rdd\n",
      "     |      True\n",
      "     |      >>> str(rdd.getStorageLevel())\n",
      "     |      'Memory Serialized 1x Replicated'\n",
      "     |      >>> _ = rdd.unpersist()\n",
      "     |  \n",
      "     |  cartesian(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n",
      "     |      Return the Cartesian product of this RDD and another one, that is, the\n",
      "     |      RDD of all pairs of elements ``(a, b)`` where ``a`` is in `self` and\n",
      "     |      ``b`` is in `other`.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`RDD`\n",
      "     |          another :class:`RDD`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          the Cartesian product of this :class:`RDD` and another one\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`pyspark.sql.DataFrame.crossJoin`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([1, 2])\n",
      "     |      >>> sorted(rdd.cartesian(rdd).collect())\n",
      "     |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
      "     |  \n",
      "     |  checkpoint(self) -> None\n",
      "     |      Mark this RDD for checkpointing. It will be saved to a file inside the\n",
      "     |      checkpoint directory set with :meth:`SparkContext.setCheckpointDir` and\n",
      "     |      all references to its parent RDDs will be removed. This function must\n",
      "     |      be called before any job has been executed on this RDD. It is strongly\n",
      "     |      recommended that this RDD is persisted in memory, otherwise saving it\n",
      "     |      on a file will require recomputation.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.isCheckpointed`\n",
      "     |      :meth:`RDD.getCheckpointFile`\n",
      "     |      :meth:`RDD.localCheckpoint`\n",
      "     |      :meth:`SparkContext.setCheckpointDir`\n",
      "     |      :meth:`SparkContext.getCheckpointDir`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.range(5)\n",
      "     |      >>> rdd.is_checkpointed\n",
      "     |      False\n",
      "     |      >>> rdd.getCheckpointFile() == None\n",
      "     |      True\n",
      "     |      \n",
      "     |      >>> rdd.checkpoint()\n",
      "     |      >>> rdd.is_checkpointed\n",
      "     |      True\n",
      "     |      >>> rdd.getCheckpointFile() == None\n",
      "     |      True\n",
      "     |      \n",
      "     |      >>> rdd.count()\n",
      "     |      5\n",
      "     |      >>> rdd.is_checkpointed\n",
      "     |      True\n",
      "     |      >>> rdd.getCheckpointFile() == None\n",
      "     |      False\n",
      "     |  \n",
      "     |  cleanShuffleDependencies(self, blocking: bool = False) -> None\n",
      "     |      Removes an RDD's shuffles and it's non-persisted ancestors.\n",
      "     |      \n",
      "     |      When running without a shuffle service, cleaning up shuffle files enables downscaling.\n",
      "     |      If you use the RDD after this call, you should checkpoint and materialize it first.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      blocking : bool, optional, default False\n",
      "     |         whether to block on shuffle cleanup tasks\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is a developer API.\n",
      "     |  \n",
      "     |  coalesce(self: 'RDD[T]', numPartitions: int, shuffle: bool = False) -> 'RDD[T]'\n",
      "     |      Return a new RDD that is reduced into `numPartitions` partitions.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      shuffle : bool, optional, default False\n",
      "     |          whether to add a shuffle step\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` that is reduced into `numPartitions` partitions\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.repartition`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n",
      "     |      [[1], [2, 3], [4, 5]]\n",
      "     |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n",
      "     |      [[1, 2, 3, 4, 5]]\n",
      "     |  \n",
      "     |  cogroup(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[ResultIterable[V], ResultIterable[U]]]]'\n",
      "     |      For each key k in `self` or `other`, return a resulting RDD that\n",
      "     |      contains a tuple with the list of values for that key in `self` as\n",
      "     |      well as `other`.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`RDD`\n",
      "     |          another :class:`RDD`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` containing the keys and cogrouped values\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.groupWith`\n",
      "     |      :meth:`RDD.join`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      "     |      >>> rdd2 = sc.parallelize([(\"a\", 2)])\n",
      "     |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(rdd1.cogroup(rdd2).collect()))]\n",
      "     |      [('a', ([1], [2])), ('b', ([4], []))]\n",
      "     |  \n",
      "     |  collect(self: 'RDD[T]') -> List[~T]\n",
      "     |      Return a list that contains all the elements in this RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          a list containing all the elements\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method should only be used if the resulting array is expected\n",
      "     |      to be small, as all the data is loaded into the driver's memory.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.toLocalIterator`\n",
      "     |      :meth:`pyspark.sql.DataFrame.collect`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.range(5).collect()\n",
      "     |      [0, 1, 2, 3, 4]\n",
      "     |      >>> sc.parallelize([\"x\", \"y\", \"z\"]).collect()\n",
      "     |      ['x', 'y', 'z']\n",
      "     |  \n",
      "     |  collectAsMap(self: 'RDD[Tuple[K, V]]') -> Dict[~K, ~V]\n",
      "     |      Return the key-value pairs in this RDD to the master as a dictionary.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`dict`\n",
      "     |          a dictionary of (key, value) pairs\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.countByValue`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method should only be used if the resulting data is expected\n",
      "     |      to be small, as all the data is loaded into the driver's memory.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
      "     |      >>> m[1]\n",
      "     |      2\n",
      "     |      >>> m[3]\n",
      "     |      4\n",
      "     |  \n",
      "     |  collectWithJobGroup(self: 'RDD[T]', groupId: str, description: str, interruptOnCancel: bool = False) -> 'List[T]'\n",
      "     |      When collect rdd, use this method to specify job group.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      .. deprecated:: 3.1.0\n",
      "     |          Use :class:`pyspark.InheritableThread` with the pinned thread mode enabled.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      groupId : str\n",
      "     |          The group ID to assign.\n",
      "     |      description : str\n",
      "     |          The description to set for the job group.\n",
      "     |      interruptOnCancel : bool, optional, default False\n",
      "     |          whether to interrupt jobs on job cancellation.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          a list containing all the elements\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.collect`\n",
      "     |      :meth:`SparkContext.setJobGroup`\n",
      "     |  \n",
      "     |  combineByKey(self: 'RDD[Tuple[K, V]]', createCombiner: Callable[[~V], ~U], mergeValue: Callable[[~U, ~V], ~U], mergeCombiners: Callable[[~U, ~U], ~U], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f4ab007a520>) -> 'RDD[Tuple[K, U]]'\n",
      "     |      Generic function to combine the elements for each key using a custom\n",
      "     |      set of aggregation functions.\n",
      "     |      \n",
      "     |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n",
      "     |      type\" C.\n",
      "     |      \n",
      "     |      To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n",
      "     |      modify and return their first argument instead of creating a new C.\n",
      "     |      \n",
      "     |      In addition, users can control the partitioning of the output RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      createCombiner : function\n",
      "     |          a function to turns a V into a C\n",
      "     |      mergeValue : function\n",
      "     |          a function to merge a V into a C\n",
      "     |      mergeCombiners : function\n",
      "     |          a function to combine two C's into a single one\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      partitionFunc : function, optional, default `portable_hash`\n",
      "     |          function to compute the partition index\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` containing the keys and the aggregated result for each key\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.reduceByKey`\n",
      "     |      :meth:`RDD.aggregateByKey`\n",
      "     |      :meth:`RDD.foldByKey`\n",
      "     |      :meth:`RDD.groupByKey`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      V and C can be different -- for example, one might group an RDD of type\n",
      "     |          (Int, Int) into an RDD of type (Int, List[Int]).\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
      "     |      >>> def to_list(a):\n",
      "     |      ...     return [a]\n",
      "     |      ...\n",
      "     |      >>> def append(a, b):\n",
      "     |      ...     a.append(b)\n",
      "     |      ...     return a\n",
      "     |      ...\n",
      "     |      >>> def extend(a, b):\n",
      "     |      ...     a.extend(b)\n",
      "     |      ...     return a\n",
      "     |      ...\n",
      "     |      >>> sorted(rdd.combineByKey(to_list, append, extend).collect())\n",
      "     |      [('a', [1, 2]), ('b', [1])]\n",
      "     |  \n",
      "     |  count(self) -> int\n",
      "     |      Return the number of elements in this RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          the number of elements\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.countApprox`\n",
      "     |      :meth:`pyspark.sql.DataFrame.count`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.parallelize([2, 3, 4]).count()\n",
      "     |      3\n",
      "     |  \n",
      "     |  countApprox(self, timeout: int, confidence: float = 0.95) -> int\n",
      "     |      Approximate version of count() that returns a potentially incomplete\n",
      "     |      result within a timeout, even if not all tasks have finished.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      timeout : int\n",
      "     |          maximum time to wait for the job, in milliseconds\n",
      "     |      confidence : float\n",
      "     |          the desired statistical confidence in the result\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          a potentially incomplete result, with error bounds\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.count`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      "     |      >>> rdd.countApprox(1000, 1.0)\n",
      "     |      1000\n",
      "     |  \n",
      "     |  countApproxDistinct(self: 'RDD[T]', relativeSD: float = 0.05) -> int\n",
      "     |      Return approximate number of distinct elements in the RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      relativeSD : float, optional\n",
      "     |          Relative accuracy. Smaller values create\n",
      "     |          counters that require more space.\n",
      "     |          It must be greater than 0.000017.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          approximate number of distinct elements\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.distinct`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The algorithm used is based on streamlib's implementation of\n",
      "     |      `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n",
      "     |      of The Art Cardinality Estimation Algorithm\", available here\n",
      "     |      <https://doi.org/10.1145/2452376.2452456>`_.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n",
      "     |      >>> 900 < n < 1100\n",
      "     |      True\n",
      "     |      >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n",
      "     |      >>> 16 < n < 24\n",
      "     |      True\n",
      "     |  \n",
      "     |  countByKey(self: 'RDD[Tuple[K, V]]') -> Dict[~K, int]\n",
      "     |      Count the number of elements for each key, and return the result to the\n",
      "     |      master as a dictionary.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dict\n",
      "     |          a dictionary of (key, count) pairs\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.collectAsMap`\n",
      "     |      :meth:`RDD.countByValue`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      "     |      >>> sorted(rdd.countByKey().items())\n",
      "     |      [('a', 2), ('b', 1)]\n",
      "     |  \n",
      "     |  countByValue(self: 'RDD[K]') -> Dict[~K, int]\n",
      "     |      Return the count of each unique value in this RDD as a dictionary of\n",
      "     |      (value, count) pairs.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dict\n",
      "     |          a dictionary of (value, count) pairs\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.collectAsMap`\n",
      "     |      :meth:`RDD.countByKey`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n",
      "     |      [(1, 2), (2, 3)]\n",
      "     |  \n",
      "     |  distinct(self: 'RDD[T]', numPartitions: Optional[int] = None) -> 'RDD[T]'\n",
      "     |      Return a new RDD containing the distinct elements in this RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a new :class:`RDD` containing the distinct elements\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.countApproxDistinct`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
      "     |      [1, 2, 3]\n",
      "     |  \n",
      "     |  filter(self: 'RDD[T]', f: Callable[[~T], bool]) -> 'RDD[T]'\n",
      "     |      Return a new RDD containing only the elements that satisfy a predicate.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |          a function to run on each element of the RDD\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a new :class:`RDD` by applying a function to each element\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.map`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
      "     |      >>> rdd.filter(lambda x: x % 2 == 0).collect()\n",
      "     |      [2, 4]\n",
      "     |  \n",
      "     |  first(self: 'RDD[T]') -> ~T\n",
      "     |      Return the first element in this RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T\n",
      "     |          the first element\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.take`\n",
      "     |      :meth:`pyspark.sql.DataFrame.first`\n",
      "     |      :meth:`pyspark.sql.DataFrame.head`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.parallelize([2, 3, 4]).first()\n",
      "     |      2\n",
      "     |      >>> sc.parallelize([]).first()\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      ValueError: RDD is empty\n",
      "     |  \n",
      "     |  flatMap(self: 'RDD[T]', f: Callable[[~T], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
      "     |      Return a new RDD by first applying a function to all elements of this\n",
      "     |      RDD, and then flattening the results.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |          a function to turn a T into a sequence of U\n",
      "     |      preservesPartitioning : bool, optional, default False\n",
      "     |          indicates whether the input function preserves the partitioner,\n",
      "     |          which should be False unless this is a pair RDD and the input\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a new :class:`RDD` by applying a function to all elements\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.map`\n",
      "     |      :meth:`RDD.mapPartitions`\n",
      "     |      :meth:`RDD.mapPartitionsWithIndex`\n",
      "     |      :meth:`RDD.mapPartitionsWithSplit`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([2, 3, 4])\n",
      "     |      >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
      "     |      [1, 1, 1, 2, 2, 3]\n",
      "     |      >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
      "     |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
      "     |  \n",
      "     |  flatMapValues(self: 'RDD[Tuple[K, V]]', f: Callable[[~V], Iterable[~U]]) -> 'RDD[Tuple[K, U]]'\n",
      "     |      Pass each value in the key-value pair RDD through a flatMap function\n",
      "     |      without changing the keys; this also retains the original RDD's\n",
      "     |      partitioning.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |         a function to turn a V into a sequence of U\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` containing the keys and the flat-mapped value\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.flatMap`\n",
      "     |      :meth:`RDD.mapValues`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n",
      "     |      >>> def f(x): return x\n",
      "     |      ...\n",
      "     |      >>> rdd.flatMapValues(f).collect()\n",
      "     |      [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n",
      "     |  \n",
      "     |  fold(self: 'RDD[T]', zeroValue: ~T, op: Callable[[~T, ~T], ~T]) -> ~T\n",
      "     |      Aggregate the elements of each partition, and then the results for all\n",
      "     |      the partitions, using a given associative function and a neutral \"zero value.\"\n",
      "     |      \n",
      "     |      The function ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n",
      "     |      as its result value to avoid object allocation; however, it should not\n",
      "     |      modify ``t2``.\n",
      "     |      \n",
      "     |      This behaves somewhat differently from fold operations implemented\n",
      "     |      for non-distributed collections in functional languages like Scala.\n",
      "     |      This fold operation may be applied to partitions individually, and then\n",
      "     |      fold those results into the final result, rather than apply the fold\n",
      "     |      to each element sequentially in some defined ordering. For functions\n",
      "     |      that are not commutative, the result may differ from that of a fold\n",
      "     |      applied to a non-distributed collection.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      zeroValue : T\n",
      "     |          the initial value for the accumulated result of each partition\n",
      "     |      op : function\n",
      "     |          a function used to both accumulate results within a partition and combine\n",
      "     |          results from different partitions\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T\n",
      "     |          the aggregated result\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.reduce`\n",
      "     |      :meth:`RDD.aggregate`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from operator import add\n",
      "     |      >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n",
      "     |      15\n",
      "     |  \n",
      "     |  foldByKey(self: 'RDD[Tuple[K, V]]', zeroValue: ~V, func: Callable[[~V, ~V], ~V], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f4ab007a520>) -> 'RDD[Tuple[K, V]]'\n",
      "     |      Merge the values for each key using an associative function \"func\"\n",
      "     |      and a neutral \"zeroValue\" which may be added to the result an\n",
      "     |      arbitrary number of times, and must not change the result\n",
      "     |      (e.g., 0 for addition, or 1 for multiplication.).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      zeroValue : V\n",
      "     |          the initial value for the accumulated result of each partition\n",
      "     |      func : function\n",
      "     |          a function to combine two V's into a single one\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      partitionFunc : function, optional, default `portable_hash`\n",
      "     |          function to compute the partition index\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` containing the keys and the aggregated result for each key\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.reduceByKey`\n",
      "     |      :meth:`RDD.combineByKey`\n",
      "     |      :meth:`RDD.aggregateByKey`\n",
      "     |      :meth:`RDD.groupByKey`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      "     |      >>> from operator import add\n",
      "     |      >>> sorted(rdd.foldByKey(0, add).collect())\n",
      "     |      [('a', 2), ('b', 1)]\n",
      "     |  \n",
      "     |  foreach(self: 'RDD[T]', f: Callable[[~T], NoneType]) -> None\n",
      "     |      Applies a function to all elements of this RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |          a function applied to each element\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.foreachPartition`\n",
      "     |      :meth:`pyspark.sql.DataFrame.foreach`\n",
      "     |      :meth:`pyspark.sql.DataFrame.foreachPartition`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> def f(x): print(x)\n",
      "     |      ...\n",
      "     |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
      "     |  \n",
      "     |  foreachPartition(self: 'RDD[T]', f: Callable[[Iterable[~T]], NoneType]) -> None\n",
      "     |      Applies a function to each partition of this RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |          a function applied to each partition\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.foreach`\n",
      "     |      :meth:`pyspark.sql.DataFrame.foreach`\n",
      "     |      :meth:`pyspark.sql.DataFrame.foreachPartition`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> def f(iterator):\n",
      "     |      ...     for x in iterator:\n",
      "     |      ...          print(x)\n",
      "     |      ...\n",
      "     |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n",
      "     |  \n",
      "     |  fullOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[Optional[V], Optional[U]]]]'\n",
      "     |      Perform a right outer join of `self` and `other`.\n",
      "     |      \n",
      "     |      For each element (k, v) in `self`, the resulting RDD will either\n",
      "     |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n",
      "     |      (k, (v, None)) if no elements in `other` have key k.\n",
      "     |      \n",
      "     |      Similarly, for each element (k, w) in `other`, the resulting RDD will\n",
      "     |      either contain all pairs (k, (v, w)) for v in `self`, or the pair\n",
      "     |      (k, (None, w)) if no elements in `self` have key k.\n",
      "     |      \n",
      "     |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`RDD`\n",
      "     |          another :class:`RDD`\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` containing all pairs of elements with matching keys\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.join`\n",
      "     |      :meth:`RDD.leftOuterJoin`\n",
      "     |      :meth:`RDD.fullOuterJoin`\n",
      "     |      :meth:`pyspark.sql.DataFrame.join`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      "     |      >>> rdd2 = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
      "     |      >>> sorted(rdd1.fullOuterJoin(rdd2).collect())\n",
      "     |      [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n",
      "     |  \n",
      "     |  getCheckpointFile(self) -> Optional[str]\n",
      "     |      Gets the name of the file to which this RDD was checkpointed\n",
      "     |      \n",
      "     |      Not defined if RDD is checkpointed locally.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      str\n",
      "     |          the name of the file to which this :class:`RDD` was checkpointed\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.checkpoint`\n",
      "     |      :meth:`SparkContext.setCheckpointDir`\n",
      "     |      :meth:`SparkContext.getCheckpointDir`\n",
      "     |  \n",
      "     |  getNumPartitions(self) -> int\n",
      "     |      Returns the number of partitions in RDD\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          number of partitions\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      "     |      >>> rdd.getNumPartitions()\n",
      "     |      2\n",
      "     |  \n",
      "     |  getResourceProfile(self) -> Optional[pyspark.resource.profile.ResourceProfile]\n",
      "     |      Get the :class:`pyspark.resource.ResourceProfile` specified with this RDD or None\n",
      "     |      if it wasn't specified.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      class:`pyspark.resource.ResourceProfile`\n",
      "     |          The user specified profile or None if none were specified\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.withResources`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is experimental\n",
      "     |  \n",
      "     |  getStorageLevel(self) -> pyspark.storagelevel.StorageLevel\n",
      "     |      Get the RDD's current storage level.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`StorageLevel`\n",
      "     |          current :class:`StorageLevel`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.name`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([1,2])\n",
      "     |      >>> rdd.getStorageLevel()\n",
      "     |      StorageLevel(False, False, False, False, 1)\n",
      "     |      >>> print(rdd.getStorageLevel())\n",
      "     |      Serialized 1x Replicated\n",
      "     |  \n",
      "     |  glom(self: 'RDD[T]') -> 'RDD[List[T]]'\n",
      "     |      Return an RDD created by coalescing all elements within each partition\n",
      "     |      into a list.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a new :class:`RDD` coalescing all elements within each partition into a list\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      "     |      >>> sorted(rdd.glom().collect())\n",
      "     |      [[1, 2], [3, 4]]\n",
      "     |  \n",
      "     |  groupBy(self: 'RDD[T]', f: Callable[[~T], ~K], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f4ab007a520>) -> 'RDD[Tuple[K, Iterable[T]]]'\n",
      "     |      Return an RDD of grouped items.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |          a function to compute the key\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      partitionFunc : function, optional, default `portable_hash`\n",
      "     |          a function to compute the partition index\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a new :class:`RDD` of grouped items\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.groupByKey`\n",
      "     |      :meth:`pyspark.sql.DataFrame.groupBy`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
      "     |      >>> result = rdd.groupBy(lambda x: x % 2).collect()\n",
      "     |      >>> sorted([(x, sorted(y)) for (x, y) in result])\n",
      "     |      [(0, [2, 8]), (1, [1, 1, 3, 5])]\n",
      "     |  \n",
      "     |  groupByKey(self: 'RDD[Tuple[K, V]]', numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f4ab007a520>) -> 'RDD[Tuple[K, Iterable[V]]]'\n",
      "     |      Group the values for each key in the RDD into a single sequence.\n",
      "     |      Hash-partitions the resulting RDD with numPartitions partitions.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      partitionFunc : function, optional, default `portable_hash`\n",
      "     |          function to compute the partition index\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` containing the keys and the grouped result for each key\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.reduceByKey`\n",
      "     |      :meth:`RDD.combineByKey`\n",
      "     |      :meth:`RDD.aggregateByKey`\n",
      "     |      :meth:`RDD.foldByKey`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If you are grouping in order to perform an aggregation (such as a\n",
      "     |      sum or average) over each key, using reduceByKey or aggregateByKey will\n",
      "     |      provide much better performance.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      "     |      >>> sorted(rdd.groupByKey().mapValues(len).collect())\n",
      "     |      [('a', 2), ('b', 1)]\n",
      "     |      >>> sorted(rdd.groupByKey().mapValues(list).collect())\n",
      "     |      [('a', [1, 1]), ('b', [1])]\n",
      "     |  \n",
      "     |  groupWith(self: 'RDD[Tuple[Any, Any]]', other: 'RDD[Tuple[Any, Any]]', *others: 'RDD[Tuple[Any, Any]]') -> 'RDD[Tuple[Any, Tuple[ResultIterable[Any], ...]]]'\n",
      "     |      Alias for cogroup but with support for multiple RDDs.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`RDD`\n",
      "     |          another :class:`RDD`\n",
      "     |      others : :class:`RDD`\n",
      "     |          other :class:`RDD`\\s\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` containing the keys and cogrouped values\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.cogroup`\n",
      "     |      :meth:`RDD.join`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd1 = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n",
      "     |      >>> rdd2 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      "     |      >>> rdd3 = sc.parallelize([(\"a\", 2)])\n",
      "     |      >>> rdd4 = sc.parallelize([(\"b\", 42)])\n",
      "     |      >>> [(x, tuple(map(list, y))) for x, y in\n",
      "     |      ...     sorted(list(rdd1.groupWith(rdd2, rdd3, rdd4).collect()))]\n",
      "     |      [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n",
      "     |  \n",
      "     |  histogram(self: 'RDD[S]', buckets: Union[int, List[ForwardRef('S')], Tuple[ForwardRef('S'), ...]]) -> Tuple[Sequence[ForwardRef('S')], List[int]]\n",
      "     |      Compute a histogram using the provided buckets. The buckets\n",
      "     |      are all open to the right except for the last which is closed.\n",
      "     |      e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n",
      "     |      which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n",
      "     |      and 50 we would have a histogram of 1,0,1.\n",
      "     |      \n",
      "     |      If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n",
      "     |      this can be switched from an O(log n) insertion to O(1) per\n",
      "     |      element (where n is the number of buckets).\n",
      "     |      \n",
      "     |      Buckets must be sorted, not contain any duplicates, and have\n",
      "     |      at least two elements.\n",
      "     |      \n",
      "     |      If `buckets` is a number, it will generate buckets which are\n",
      "     |      evenly spaced between the minimum and maximum of the RDD. For\n",
      "     |      example, if the min value is 0 and the max is 100, given `buckets`\n",
      "     |      as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n",
      "     |      be at least 1. An exception is raised if the RDD contains infinity.\n",
      "     |      If the elements in the RDD do not vary (max == min), a single bucket\n",
      "     |      will be used.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      buckets : int, or list, or tuple\n",
      "     |          if `buckets` is a number, it computes a histogram of the data using\n",
      "     |          `buckets` number of buckets evenly, otherwise, `buckets` is the provided\n",
      "     |          buckets to bin the data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      tuple\n",
      "     |          a tuple of buckets and histogram\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.stats`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize(range(51))\n",
      "     |      >>> rdd.histogram(2)\n",
      "     |      ([0, 25, 50], [25, 26])\n",
      "     |      >>> rdd.histogram([0, 5, 25, 50])\n",
      "     |      ([0, 5, 25, 50], [5, 20, 26])\n",
      "     |      >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n",
      "     |      ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n",
      "     |      >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n",
      "     |      >>> rdd.histogram((\"a\", \"b\", \"c\"))\n",
      "     |      (('a', 'b', 'c'), [2, 2])\n",
      "     |  \n",
      "     |  id(self) -> int\n",
      "     |      A unique ID for this RDD (within its SparkContext).\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          The unique ID for this :class:`RDD`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.range(5)\n",
      "     |      >>> rdd.id()  # doctest: +SKIP\n",
      "     |      3\n",
      "     |  \n",
      "     |  intersection(self: 'RDD[T]', other: 'RDD[T]') -> 'RDD[T]'\n",
      "     |      Return the intersection of this RDD and another one. The output will\n",
      "     |      not contain any duplicate elements, even if the input RDDs did.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`RDD`\n",
      "     |          another :class:`RDD`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          the intersection of this :class:`RDD` and another one\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`pyspark.sql.DataFrame.intersect`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method performs a shuffle internally.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
      "     |      >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
      "     |      >>> rdd1.intersection(rdd2).collect()\n",
      "     |      [1, 2, 3]\n",
      "     |  \n",
      "     |  isCheckpointed(self) -> bool\n",
      "     |      Return whether this RDD is checkpointed and materialized, either reliably or locally.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          whether this :class:`RDD` is checkpointed and materialized, either reliably or locally\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.checkpoint`\n",
      "     |      :meth:`RDD.getCheckpointFile`\n",
      "     |      :meth:`SparkContext.setCheckpointDir`\n",
      "     |      :meth:`SparkContext.getCheckpointDir`\n",
      "     |  \n",
      "     |  isEmpty(self) -> bool\n",
      "     |      Returns true if and only if the RDD contains no elements at all.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          whether the :class:`RDD` is empty\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.first`\n",
      "     |      :meth:`pyspark.sql.DataFrame.isEmpty`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      An RDD may be empty even when it has at least 1 partition.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.parallelize([]).isEmpty()\n",
      "     |      True\n",
      "     |      >>> sc.parallelize([1]).isEmpty()\n",
      "     |      False\n",
      "     |  \n",
      "     |  isLocallyCheckpointed(self) -> bool\n",
      "     |      Return whether this RDD is marked for local checkpointing.\n",
      "     |      \n",
      "     |      Exposed for testing.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.2.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          whether this :class:`RDD` is marked for local checkpointing\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.localCheckpoint`\n",
      "     |  \n",
      "     |  join(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[V, U]]]'\n",
      "     |      Return an RDD containing all pairs of elements with matching keys in\n",
      "     |      `self` and `other`.\n",
      "     |      \n",
      "     |      Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n",
      "     |      (k, v1) is in `self` and (k, v2) is in `other`.\n",
      "     |      \n",
      "     |      Performs a hash join across the cluster.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`RDD`\n",
      "     |          another :class:`RDD`\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` containing all pairs of elements with matching keys\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.leftOuterJoin`\n",
      "     |      :meth:`RDD.rightOuterJoin`\n",
      "     |      :meth:`RDD.fullOuterJoin`\n",
      "     |      :meth:`RDD.cogroup`\n",
      "     |      :meth:`RDD.groupWith`\n",
      "     |      :meth:`pyspark.sql.DataFrame.join`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      "     |      >>> rdd2 = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
      "     |      >>> sorted(rdd1.join(rdd2).collect())\n",
      "     |      [('a', (1, 2)), ('a', (1, 3))]\n",
      "     |  \n",
      "     |  keyBy(self: 'RDD[T]', f: Callable[[~T], ~K]) -> 'RDD[Tuple[K, T]]'\n",
      "     |      Creates tuples of the elements in this RDD by applying `f`.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.9.1\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |          a function to compute the key\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` with the elements from this that are not in `other`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.map`\n",
      "     |      :meth:`RDD.keys`\n",
      "     |      :meth:`RDD.values`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd1 = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n",
      "     |      >>> rdd2 = sc.parallelize(zip(range(0,5), range(0,5)))\n",
      "     |      >>> [(x, list(map(list, y))) for x, y in sorted(rdd1.cogroup(rdd2).collect())]\n",
      "     |      [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]\n",
      "     |  \n",
      "     |  keys(self: 'RDD[Tuple[K, V]]') -> 'RDD[K]'\n",
      "     |      Return an RDD with the keys of each tuple.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` only containing the keys\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.values`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
      "     |      >>> rdd.collect()\n",
      "     |      [1, 3]\n",
      "     |  \n",
      "     |  leftOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[V, Optional[U]]]]'\n",
      "     |      Perform a left outer join of `self` and `other`.\n",
      "     |      \n",
      "     |      For each element (k, v) in `self`, the resulting RDD will either\n",
      "     |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n",
      "     |      (k, (v, None)) if no elements in `other` have key k.\n",
      "     |      \n",
      "     |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`RDD`\n",
      "     |          another :class:`RDD`\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` containing all pairs of elements with matching keys\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.join`\n",
      "     |      :meth:`RDD.rightOuterJoin`\n",
      "     |      :meth:`RDD.fullOuterJoin`\n",
      "     |      :meth:`pyspark.sql.DataFrame.join`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      "     |      >>> rdd2 = sc.parallelize([(\"a\", 2)])\n",
      "     |      >>> sorted(rdd1.leftOuterJoin(rdd2).collect())\n",
      "     |      [('a', (1, 2)), ('b', (4, None))]\n",
      "     |  \n",
      "     |  localCheckpoint(self) -> None\n",
      "     |      Mark this RDD for local checkpointing using Spark's existing caching layer.\n",
      "     |      \n",
      "     |      This method is for users who wish to truncate RDD lineages while skipping the expensive\n",
      "     |      step of replicating the materialized data in a reliable distributed file system. This is\n",
      "     |      useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).\n",
      "     |      \n",
      "     |      Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed\n",
      "     |      data is written to ephemeral local storage in the executors instead of to a reliable,\n",
      "     |      fault-tolerant storage. The effect is that if an executor fails during the computation,\n",
      "     |      the checkpointed data may no longer be accessible, causing an irrecoverable job failure.\n",
      "     |      \n",
      "     |      This is NOT safe to use with dynamic allocation, which removes executors along\n",
      "     |      with their cached blocks. If you must use both features, you are advised to set\n",
      "     |      `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value.\n",
      "     |      \n",
      "     |      The checkpoint directory set through :meth:`SparkContext.setCheckpointDir` is not used.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.2.0\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.checkpoint`\n",
      "     |      :meth:`RDD.isLocallyCheckpointed`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.range(5)\n",
      "     |      >>> rdd.isLocallyCheckpointed()\n",
      "     |      False\n",
      "     |      \n",
      "     |      >>> rdd.localCheckpoint()\n",
      "     |      >>> rdd.isLocallyCheckpointed()\n",
      "     |      True\n",
      "     |  \n",
      "     |  lookup(self: 'RDD[Tuple[K, V]]', key: ~K) -> List[~V]\n",
      "     |      Return the list of values in the RDD for key `key`. This operation\n",
      "     |      is done efficiently if the RDD has a known partitioner by only\n",
      "     |      searching the partition that the key maps to.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key : K\n",
      "     |          the key to look up\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          the list of values in the :class:`RDD` for key `key`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> l = range(1000)\n",
      "     |      >>> rdd = sc.parallelize(zip(l, l), 10)\n",
      "     |      >>> rdd.lookup(42)  # slow\n",
      "     |      [42]\n",
      "     |      >>> sorted = rdd.sortByKey()\n",
      "     |      >>> sorted.lookup(42)  # fast\n",
      "     |      [42]\n",
      "     |      >>> sorted.lookup(1024)\n",
      "     |      []\n",
      "     |      >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n",
      "     |      >>> list(rdd2.lookup(('a', 'b'))[0])\n",
      "     |      ['c']\n",
      "     |  \n",
      "     |  map(self: 'RDD[T]', f: Callable[[~T], ~U], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
      "     |      Return a new RDD by applying a function to each element of this RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |          a function to run on each element of the RDD\n",
      "     |      preservesPartitioning : bool, optional, default False\n",
      "     |          indicates whether the input function preserves the partitioner,\n",
      "     |          which should be False unless this is a pair RDD and the input\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a new :class:`RDD` by applying a function to all elements\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.flatMap`\n",
      "     |      :meth:`RDD.mapPartitions`\n",
      "     |      :meth:`RDD.mapPartitionsWithIndex`\n",
      "     |      :meth:`RDD.mapPartitionsWithSplit`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      "     |      >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n",
      "     |      [('a', 1), ('b', 1), ('c', 1)]\n",
      "     |  \n",
      "     |  mapPartitions(self: 'RDD[T]', f: Callable[[Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
      "     |      Return a new RDD by applying a function to each partition of this RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |          a function to run on each partition of the RDD\n",
      "     |      preservesPartitioning : bool, optional, default False\n",
      "     |          indicates whether the input function preserves the partitioner,\n",
      "     |          which should be False unless this is a pair RDD and the input\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a new :class:`RDD` by applying a function to each partition\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.map`\n",
      "     |      :meth:`RDD.flatMap`\n",
      "     |      :meth:`RDD.mapPartitionsWithIndex`\n",
      "     |      :meth:`RDD.mapPartitionsWithSplit`\n",
      "     |      :meth:`RDDBarrier.mapPartitions`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      "     |      >>> def f(iterator): yield sum(iterator)\n",
      "     |      ...\n",
      "     |      >>> rdd.mapPartitions(f).collect()\n",
      "     |      [3, 7]\n",
      "     |  \n",
      "     |  mapPartitionsWithIndex(self: 'RDD[T]', f: Callable[[int, Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
      "     |      Return a new RDD by applying a function to each partition of this RDD,\n",
      "     |      while tracking the index of the original partition.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |          a function to run on each partition of the RDD\n",
      "     |      preservesPartitioning : bool, optional, default False\n",
      "     |          indicates whether the input function preserves the partitioner,\n",
      "     |          which should be False unless this is a pair RDD and the input\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a new :class:`RDD` by applying a function to each partition\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.map`\n",
      "     |      :meth:`RDD.flatMap`\n",
      "     |      :meth:`RDD.mapPartitions`\n",
      "     |      :meth:`RDD.mapPartitionsWithSplit`\n",
      "     |      :meth:`RDDBarrier.mapPartitionsWithIndex`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      "     |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      "     |      ...\n",
      "     |      >>> rdd.mapPartitionsWithIndex(f).sum()\n",
      "     |      6\n",
      "     |  \n",
      "     |  mapPartitionsWithSplit(self: 'RDD[T]', f: Callable[[int, Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
      "     |      Return a new RDD by applying a function to each partition of this RDD,\n",
      "     |      while tracking the index of the original partition.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      .. deprecated:: 0.9.0\n",
      "     |          use meth:`RDD.mapPartitionsWithIndex` instead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |          a function to run on each partition of the RDD\n",
      "     |      preservesPartitioning : bool, optional, default False\n",
      "     |          indicates whether the input function preserves the partitioner,\n",
      "     |          which should be False unless this is a pair RDD and the input\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a new :class:`RDD` by applying a function to each partition\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.map`\n",
      "     |      :meth:`RDD.flatMap`\n",
      "     |      :meth:`RDD.mapPartitions`\n",
      "     |      :meth:`RDD.mapPartitionsWithIndex`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      "     |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      "     |      ...\n",
      "     |      >>> rdd.mapPartitionsWithSplit(f).sum()\n",
      "     |      6\n",
      "     |  \n",
      "     |  mapValues(self: 'RDD[Tuple[K, V]]', f: Callable[[~V], ~U]) -> 'RDD[Tuple[K, U]]'\n",
      "     |      Pass each value in the key-value pair RDD through a map function\n",
      "     |      without changing the keys; this also retains the original RDD's\n",
      "     |      partitioning.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |         a function to turn a V into a U\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` containing the keys and the mapped value\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.map`\n",
      "     |      :meth:`RDD.flatMapValues`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
      "     |      >>> def f(x): return len(x)\n",
      "     |      ...\n",
      "     |      >>> rdd.mapValues(f).collect()\n",
      "     |      [('a', 3), ('b', 1)]\n",
      "     |  \n",
      "     |  max(self: 'RDD[T]', key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> ~T\n",
      "     |      Find the maximum item in this RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key : function, optional\n",
      "     |          A function used to generate key for comparing\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T\n",
      "     |          the maximum item\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.min`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n",
      "     |      >>> rdd.max()\n",
      "     |      43.0\n",
      "     |      >>> rdd.max(key=str)\n",
      "     |      5.0\n",
      "     |  \n",
      "     |  mean(self: 'RDD[NumberOrArray]') -> float\n",
      "     |      Compute the mean of this RDD's elements.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.9.1\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float\n",
      "     |          the mean of all elements\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.stats`\n",
      "     |      :meth:`RDD.sum`\n",
      "     |      :meth:`RDD.meanApprox`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.parallelize([1, 2, 3]).mean()\n",
      "     |      2.0\n",
      "     |  \n",
      "     |  meanApprox(self: 'RDD[Union[float, int]]', timeout: int, confidence: float = 0.95) -> pyspark.rdd.BoundedFloat\n",
      "     |      Approximate operation to return the mean within a timeout\n",
      "     |      or meet the confidence.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      timeout : int\n",
      "     |          maximum time to wait for the job, in milliseconds\n",
      "     |      confidence : float\n",
      "     |          the desired statistical confidence in the result\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`BoundedFloat`\n",
      "     |          a potentially incomplete result, with error bounds\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.mean`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      "     |      >>> r = sum(range(1000)) / 1000.0\n",
      "     |      >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n",
      "     |      True\n",
      "     |  \n",
      "     |  min(self: 'RDD[T]', key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> ~T\n",
      "     |      Find the minimum item in this RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key : function, optional\n",
      "     |          A function used to generate key for comparing\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T\n",
      "     |          the minimum item\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.max`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n",
      "     |      >>> rdd.min()\n",
      "     |      2.0\n",
      "     |      >>> rdd.min(key=str)\n",
      "     |      10.0\n",
      "     |  \n",
      "     |  name(self) -> Optional[str]\n",
      "     |      Return the name of this RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      str\n",
      "     |          :class:`RDD` name\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.setName`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.range(5)\n",
      "     |      >>> rdd.name() == None\n",
      "     |      True\n",
      "     |  \n",
      "     |  partitionBy(self: 'RDD[Tuple[K, V]]', numPartitions: Optional[int], partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f4ab007a520>) -> 'RDD[Tuple[K, V]]'\n",
      "     |      Return a copy of the RDD partitioned using the specified partitioner.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      partitionFunc : function, optional, default `portable_hash`\n",
      "     |          function to compute the partition index\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` partitioned using the specified partitioner\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.repartition`\n",
      "     |      :meth:`RDD.repartitionAndSortWithinPartitions`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
      "     |      >>> sets = pairs.partitionBy(2).glom().collect()\n",
      "     |      >>> len(set(sets[0]).intersection(set(sets[1])))\n",
      "     |      0\n",
      "     |  \n",
      "     |  persist(self: 'RDD[T]', storageLevel: pyspark.storagelevel.StorageLevel = StorageLevel(False, True, False, False, 1)) -> 'RDD[T]'\n",
      "     |      Set this RDD's storage level to persist its values across operations\n",
      "     |      after the first time it is computed. This can only be used to assign\n",
      "     |      a new storage level if the RDD does not have a storage level set yet.\n",
      "     |      If no storage level is specified defaults to (`MEMORY_ONLY`).\n",
      "     |      \n",
      "     |      .. versionadded:: 0.9.1\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      storageLevel : :class:`StorageLevel`, default `MEMORY_ONLY`\n",
      "     |          the target storage level\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          The same :class:`RDD` with storage level set to `storageLevel`.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.cache`\n",
      "     |      :meth:`RDD.unpersist`\n",
      "     |      :meth:`RDD.getStorageLevel`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      "     |      >>> rdd.persist().is_cached\n",
      "     |      True\n",
      "     |      >>> str(rdd.getStorageLevel())\n",
      "     |      'Memory Serialized 1x Replicated'\n",
      "     |      >>> _ = rdd.unpersist()\n",
      "     |      >>> rdd.is_cached\n",
      "     |      False\n",
      "     |      \n",
      "     |      >>> from pyspark import StorageLevel\n",
      "     |      >>> rdd2 = sc.range(5)\n",
      "     |      >>> _ = rdd2.persist(StorageLevel.MEMORY_AND_DISK)\n",
      "     |      >>> rdd2.is_cached\n",
      "     |      True\n",
      "     |      >>> str(rdd2.getStorageLevel())\n",
      "     |      'Disk Memory Serialized 1x Replicated'\n",
      "     |      \n",
      "     |      Can not override existing storage level\n",
      "     |      \n",
      "     |      >>> _ = rdd2.persist(StorageLevel.MEMORY_ONLY_2)\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      py4j.protocol.Py4JJavaError: ...\n",
      "     |      \n",
      "     |      Assign another storage level after `unpersist`\n",
      "     |      \n",
      "     |      >>> _ = rdd2.unpersist()\n",
      "     |      >>> rdd2.is_cached\n",
      "     |      False\n",
      "     |      >>> _ = rdd2.persist(StorageLevel.MEMORY_ONLY_2)\n",
      "     |      >>> str(rdd2.getStorageLevel())\n",
      "     |      'Memory Serialized 2x Replicated'\n",
      "     |      >>> rdd2.is_cached\n",
      "     |      True\n",
      "     |      >>> _ = rdd2.unpersist()\n",
      "     |  \n",
      "     |  pipe(self, command: str, env: Optional[Dict[str, str]] = None, checkCode: bool = False) -> 'RDD[str]'\n",
      "     |      Return an RDD created by piping elements to a forked external process.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      command : str\n",
      "     |          command to run.\n",
      "     |      env : dict, optional\n",
      "     |          environment variables to set.\n",
      "     |      checkCode : bool, optional\n",
      "     |          whether to check the return value of the shell command.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a new :class:`RDD` of strings\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n",
      "     |      ['1', '2', '', '3']\n",
      "     |  \n",
      "     |  randomSplit(self: 'RDD[T]', weights: Sequence[Union[int, float]], seed: Optional[int] = None) -> 'List[RDD[T]]'\n",
      "     |      Randomly splits this RDD with the provided weights.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      weights : list\n",
      "     |          weights for splits, will be normalized if they don't sum to 1\n",
      "     |      seed : int, optional\n",
      "     |          random seed\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          split :class:`RDD`\\s in a list\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`pyspark.sql.DataFrame.randomSplit`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize(range(500), 1)\n",
      "     |      >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
      "     |      >>> len(rdd1.collect() + rdd2.collect())\n",
      "     |      500\n",
      "     |      >>> 150 < rdd1.count() < 250\n",
      "     |      True\n",
      "     |      >>> 250 < rdd2.count() < 350\n",
      "     |      True\n",
      "     |  \n",
      "     |  reduce(self: 'RDD[T]', f: Callable[[~T, ~T], ~T]) -> ~T\n",
      "     |      Reduces the elements of this RDD using the specified commutative and\n",
      "     |      associative binary operator. Currently reduces partitions locally.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |          the reduce function\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T\n",
      "     |          the aggregated result\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.treeReduce`\n",
      "     |      :meth:`RDD.aggregate`\n",
      "     |      :meth:`RDD.treeAggregate`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from operator import add\n",
      "     |      >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
      "     |      15\n",
      "     |      >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n",
      "     |      10\n",
      "     |      >>> sc.parallelize([]).reduce(add)\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      ValueError: Can not reduce() empty RDD\n",
      "     |  \n",
      "     |  reduceByKey(self: 'RDD[Tuple[K, V]]', func: Callable[[~V, ~V], ~V], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f4ab007a520>) -> 'RDD[Tuple[K, V]]'\n",
      "     |      Merge the values for each key using an associative and commutative reduce function.\n",
      "     |      \n",
      "     |      This will also perform the merging locally on each mapper before\n",
      "     |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      "     |      \n",
      "     |      Output will be partitioned with `numPartitions` partitions, or\n",
      "     |      the default parallelism level if `numPartitions` is not specified.\n",
      "     |      Default partitioner is hash-partition.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : function\n",
      "     |          the reduce function\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      partitionFunc : function, optional, default `portable_hash`\n",
      "     |          function to compute the partition index\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` containing the keys and the aggregated result for each key\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.reduceByKeyLocally`\n",
      "     |      :meth:`RDD.combineByKey`\n",
      "     |      :meth:`RDD.aggregateByKey`\n",
      "     |      :meth:`RDD.foldByKey`\n",
      "     |      :meth:`RDD.groupByKey`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from operator import add\n",
      "     |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      "     |      >>> sorted(rdd.reduceByKey(add).collect())\n",
      "     |      [('a', 2), ('b', 1)]\n",
      "     |  \n",
      "     |  reduceByKeyLocally(self: 'RDD[Tuple[K, V]]', func: Callable[[~V, ~V], ~V]) -> Dict[~K, ~V]\n",
      "     |      Merge the values for each key using an associative and commutative reduce function, but\n",
      "     |      return the results immediately to the master as a dictionary.\n",
      "     |      \n",
      "     |      This will also perform the merging locally on each mapper before\n",
      "     |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : function\n",
      "     |          the reduce function\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dict\n",
      "     |          a dict containing the keys and the aggregated result for each key\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.reduceByKey`\n",
      "     |      :meth:`RDD.aggregateByKey`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from operator import add\n",
      "     |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      "     |      >>> sorted(rdd.reduceByKeyLocally(add).items())\n",
      "     |      [('a', 2), ('b', 1)]\n",
      "     |  \n",
      "     |  repartition(self: 'RDD[T]', numPartitions: int) -> 'RDD[T]'\n",
      "     |       Return a new RDD that has exactly numPartitions partitions.\n",
      "     |      \n",
      "     |       Can increase or decrease the level of parallelism in this RDD.\n",
      "     |       Internally, this uses a shuffle to redistribute data.\n",
      "     |       If you are decreasing the number of partitions in this RDD, consider\n",
      "     |       using `coalesce`, which can avoid performing a shuffle.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` with exactly numPartitions partitions\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.coalesce`\n",
      "     |      :meth:`RDD.partitionBy`\n",
      "     |      :meth:`RDD.repartitionAndSortWithinPartitions`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |       >>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
      "     |       >>> sorted(rdd.glom().collect())\n",
      "     |       [[1], [2, 3], [4, 5], [6, 7]]\n",
      "     |       >>> len(rdd.repartition(2).glom().collect())\n",
      "     |       2\n",
      "     |       >>> len(rdd.repartition(10).glom().collect())\n",
      "     |       10\n",
      "     |  \n",
      "     |  repartitionAndSortWithinPartitions(self: 'RDD[Tuple[Any, Any]]', numPartitions: Optional[int] = None, partitionFunc: Callable[[Any], int] = <function portable_hash at 0x7f4ab007a520>, ascending: bool = True, keyfunc: Callable[[Any], Any] = <function RDD.<lambda> at 0x7f4a9018d440>) -> 'RDD[Tuple[Any, Any]]'\n",
      "     |      Repartition the RDD according to the given partitioner and, within each resulting partition,\n",
      "     |      sort records by their keys.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      partitionFunc : function, optional, default `portable_hash`\n",
      "     |          a function to compute the partition index\n",
      "     |      ascending : bool, optional, default True\n",
      "     |          sort the keys in ascending or descending order\n",
      "     |      keyfunc : function, optional, default identity mapping\n",
      "     |          a function to compute the key\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a new :class:`RDD`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.repartition`\n",
      "     |      :meth:`RDD.partitionBy`\n",
      "     |      :meth:`RDD.sortBy`\n",
      "     |      :meth:`RDD.sortByKey`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
      "     |      >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n",
      "     |      >>> rdd2.glom().collect()\n",
      "     |      [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n",
      "     |  \n",
      "     |  rightOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[Optional[V], U]]]'\n",
      "     |      Perform a right outer join of `self` and `other`.\n",
      "     |      \n",
      "     |      For each element (k, w) in `other`, the resulting RDD will either\n",
      "     |      contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n",
      "     |      if no elements in `self` have key k.\n",
      "     |      \n",
      "     |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`RDD`\n",
      "     |          another :class:`RDD`\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` containing all pairs of elements with matching keys\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.join`\n",
      "     |      :meth:`RDD.leftOuterJoin`\n",
      "     |      :meth:`RDD.fullOuterJoin`\n",
      "     |      :meth:`pyspark.sql.DataFrame.join`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      "     |      >>> rdd2 = sc.parallelize([(\"a\", 2)])\n",
      "     |      >>> sorted(rdd2.rightOuterJoin(rdd1).collect())\n",
      "     |      [('a', (2, 1)), ('b', (None, 4))]\n",
      "     |  \n",
      "     |  sample(self: 'RDD[T]', withReplacement: bool, fraction: float, seed: Optional[int] = None) -> 'RDD[T]'\n",
      "     |      Return a sampled subset of this RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      withReplacement : bool\n",
      "     |          can elements be sampled multiple times (replaced when sampled out)\n",
      "     |      fraction : float\n",
      "     |          expected size of the sample as a fraction of this RDD's size\n",
      "     |          without replacement: probability that each element is chosen; fraction must be [0, 1]\n",
      "     |          with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
      "     |      seed : int, optional\n",
      "     |          seed for the random number generator\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a new :class:`RDD` containing a sampled subset of elements\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.takeSample`\n",
      "     |      :meth:`RDD.sampleByKey`\n",
      "     |      :meth:`pyspark.sql.DataFrame.sample`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This is not guaranteed to provide exactly the fraction specified of the total\n",
      "     |      count of the given :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize(range(100), 4)\n",
      "     |      >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n",
      "     |      True\n",
      "     |  \n",
      "     |  sampleByKey(self: 'RDD[Tuple[K, V]]', withReplacement: bool, fractions: Dict[~K, Union[float, int]], seed: Optional[int] = None) -> 'RDD[Tuple[K, V]]'\n",
      "     |      Return a subset of this RDD sampled by key (via stratified sampling).\n",
      "     |      Create a sample of this RDD using variable sampling rates for\n",
      "     |      different keys as specified by fractions, a key to sampling rate map.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      withReplacement : bool\n",
      "     |          whether to sample with or without replacement\n",
      "     |      fractions : dict\n",
      "     |          map of specific keys to sampling rates\n",
      "     |      seed : int, optional\n",
      "     |          seed for the random number generator\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` containing the stratified sampling result\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.sample`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n",
      "     |      >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n",
      "     |      >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n",
      "     |      >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n",
      "     |      True\n",
      "     |      >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n",
      "     |      True\n",
      "     |      >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n",
      "     |      True\n",
      "     |  \n",
      "     |  sampleStdev(self: 'RDD[NumberOrArray]') -> float\n",
      "     |      Compute the sample standard deviation of this RDD's elements (which\n",
      "     |      corrects for bias in estimating the standard deviation by dividing by\n",
      "     |      N-1 instead of N).\n",
      "     |      \n",
      "     |      .. versionadded:: 0.9.1\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float\n",
      "     |          the sample standard deviation of all elements\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.stats`\n",
      "     |      :meth:`RDD.stdev`\n",
      "     |      :meth:`RDD.variance`\n",
      "     |      :meth:`RDD.sampleVariance`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.parallelize([1, 2, 3]).sampleStdev()\n",
      "     |      1.0\n",
      "     |  \n",
      "     |  sampleVariance(self: 'RDD[NumberOrArray]') -> float\n",
      "     |      Compute the sample variance of this RDD's elements (which corrects\n",
      "     |      for bias in estimating the variance by dividing by N-1 instead of N).\n",
      "     |      \n",
      "     |      .. versionadded:: 0.9.1\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float\n",
      "     |          the sample variance of all elements\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.stats`\n",
      "     |      :meth:`RDD.variance`\n",
      "     |      :meth:`RDD.stdev`\n",
      "     |      :meth:`RDD.sampleStdev`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.parallelize([1, 2, 3]).sampleVariance()\n",
      "     |      1.0\n",
      "     |  \n",
      "     |  saveAsHadoopDataset(self: 'RDD[Tuple[K, V]]', conf: Dict[str, str], keyConverter: Optional[str] = None, valueConverter: Optional[str] = None) -> None\n",
      "     |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      "     |      system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n",
      "     |      converted for output using either user specified converters or, by default,\n",
      "     |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      conf : dict\n",
      "     |          Hadoop job configuration\n",
      "     |      keyConverter : str, optional\n",
      "     |          fully qualified classname of key converter (None by default)\n",
      "     |      valueConverter : str, optional\n",
      "     |          fully qualified classname of value converter (None by default)\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.hadoopRDD`\n",
      "     |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n",
      "     |      :meth:`RDD.saveAsHadoopFile`\n",
      "     |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
      "     |      :meth:`RDD.saveAsSequenceFile`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      \n",
      "     |      Set the related classes\n",
      "     |      \n",
      "     |      >>> output_format_class = \"org.apache.hadoop.mapred.TextOutputFormat\"\n",
      "     |      >>> input_format_class = \"org.apache.hadoop.mapred.TextInputFormat\"\n",
      "     |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
      "     |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     path = os.path.join(d, \"old_hadoop_file\")\n",
      "     |      ...\n",
      "     |      ...     # Create the conf for writing\n",
      "     |      ...     write_conf = {\n",
      "     |      ...         \"mapred.output.format.class\": output_format_class,\n",
      "     |      ...         \"mapreduce.job.output.key.class\": key_class,\n",
      "     |      ...         \"mapreduce.job.output.value.class\": value_class,\n",
      "     |      ...         \"mapreduce.output.fileoutputformat.outputdir\": path,\n",
      "     |      ...     }\n",
      "     |      ...\n",
      "     |      ...     # Write a temporary Hadoop file\n",
      "     |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      "     |      ...     rdd.saveAsHadoopDataset(conf=write_conf)\n",
      "     |      ...\n",
      "     |      ...     # Create the conf for reading\n",
      "     |      ...     read_conf = {\"mapreduce.input.fileinputformat.inputdir\": path}\n",
      "     |      ...\n",
      "     |      ...     # Load this Hadoop file as an RDD\n",
      "     |      ...     loaded = sc.hadoopRDD(input_format_class, key_class, value_class, conf=read_conf)\n",
      "     |      ...     sorted(loaded.collect())\n",
      "     |      [(0, '1\\t'), (0, '1\\ta'), (0, '3\\tx')]\n",
      "     |  \n",
      "     |  saveAsHadoopFile(self: 'RDD[Tuple[K, V]]', path: str, outputFormatClass: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, compressionCodecClass: Optional[str] = None) -> None\n",
      "     |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      "     |      system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n",
      "     |      will be inferred if not specified. Keys and values are converted for output using either\n",
      "     |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n",
      "     |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n",
      "     |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          path to Hadoop file\n",
      "     |      outputFormatClass : str\n",
      "     |          fully qualified classname of Hadoop OutputFormat\n",
      "     |          (e.g. \"org.apache.hadoop.mapred.SequenceFileOutputFormat\")\n",
      "     |      keyClass : str, optional\n",
      "     |          fully qualified classname of key Writable class\n",
      "     |          (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      "     |      valueClass : str, optional\n",
      "     |          fully qualified classname of value Writable class\n",
      "     |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      "     |      keyConverter : str, optional\n",
      "     |          fully qualified classname of key converter (None by default)\n",
      "     |      valueConverter : str, optional\n",
      "     |          fully qualified classname of value converter (None by default)\n",
      "     |      conf : dict, optional\n",
      "     |          (None by default)\n",
      "     |      compressionCodecClass : str\n",
      "     |          fully qualified classname of the compression codec class\n",
      "     |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.hadoopFile`\n",
      "     |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
      "     |      :meth:`RDD.saveAsHadoopDataset`\n",
      "     |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n",
      "     |      :meth:`RDD.saveAsSequenceFile`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      \n",
      "     |      Set the related classes\n",
      "     |      \n",
      "     |      >>> output_format_class = \"org.apache.hadoop.mapred.TextOutputFormat\"\n",
      "     |      >>> input_format_class = \"org.apache.hadoop.mapred.TextInputFormat\"\n",
      "     |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
      "     |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     path = os.path.join(d, \"old_hadoop_file\")\n",
      "     |      ...\n",
      "     |      ...     # Write a temporary Hadoop file\n",
      "     |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      "     |      ...     rdd.saveAsHadoopFile(path, output_format_class, key_class, value_class)\n",
      "     |      ...\n",
      "     |      ...     # Load this Hadoop file as an RDD\n",
      "     |      ...     loaded = sc.hadoopFile(path, input_format_class, key_class, value_class)\n",
      "     |      ...     sorted(loaded.collect())\n",
      "     |      [(0, '1\\t'), (0, '1\\ta'), (0, '3\\tx')]\n",
      "     |  \n",
      "     |  saveAsNewAPIHadoopDataset(self: 'RDD[Tuple[K, V]]', conf: Dict[str, str], keyConverter: Optional[str] = None, valueConverter: Optional[str] = None) -> None\n",
      "     |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      "     |      system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n",
      "     |      converted for output using either user specified converters or, by default,\n",
      "     |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      conf : dict\n",
      "     |          Hadoop job configuration\n",
      "     |      keyConverter : str, optional\n",
      "     |          fully qualified classname of key converter (None by default)\n",
      "     |      valueConverter : str, optional\n",
      "     |          fully qualified classname of value converter (None by default)\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.newAPIHadoopRDD`\n",
      "     |      :meth:`RDD.saveAsHadoopDataset`\n",
      "     |      :meth:`RDD.saveAsHadoopFile`\n",
      "     |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
      "     |      :meth:`RDD.saveAsSequenceFile`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      \n",
      "     |      Set the related classes\n",
      "     |      \n",
      "     |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n",
      "     |      >>> input_format_class = \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\"\n",
      "     |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
      "     |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     path = os.path.join(d, \"new_hadoop_file\")\n",
      "     |      ...\n",
      "     |      ...     # Create the conf for writing\n",
      "     |      ...     write_conf = {\n",
      "     |      ...         \"mapreduce.job.outputformat.class\": (output_format_class),\n",
      "     |      ...         \"mapreduce.job.output.key.class\": key_class,\n",
      "     |      ...         \"mapreduce.job.output.value.class\": value_class,\n",
      "     |      ...         \"mapreduce.output.fileoutputformat.outputdir\": path,\n",
      "     |      ...     }\n",
      "     |      ...\n",
      "     |      ...     # Write a temporary Hadoop file\n",
      "     |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      "     |      ...     rdd.saveAsNewAPIHadoopDataset(conf=write_conf)\n",
      "     |      ...\n",
      "     |      ...     # Create the conf for reading\n",
      "     |      ...     read_conf = {\"mapreduce.input.fileinputformat.inputdir\": path}\n",
      "     |      ...\n",
      "     |      ...     # Load this Hadoop file as an RDD\n",
      "     |      ...     loaded = sc.newAPIHadoopRDD(input_format_class,\n",
      "     |      ...         key_class, value_class, conf=read_conf)\n",
      "     |      ...     sorted(loaded.collect())\n",
      "     |      [(1, ''), (1, 'a'), (3, 'x')]\n",
      "     |  \n",
      "     |  saveAsNewAPIHadoopFile(self: 'RDD[Tuple[K, V]]', path: str, outputFormatClass: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None) -> None\n",
      "     |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      "     |      system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n",
      "     |      will be inferred if not specified. Keys and values are converted for output using either\n",
      "     |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n",
      "     |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n",
      "     |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          path to Hadoop file\n",
      "     |      outputFormatClass : str\n",
      "     |          fully qualified classname of Hadoop OutputFormat\n",
      "     |          (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n",
      "     |      keyClass : str, optional\n",
      "     |          fully qualified classname of key Writable class\n",
      "     |           (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      "     |      valueClass : str, optional\n",
      "     |          fully qualified classname of value Writable class\n",
      "     |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      "     |      keyConverter : str, optional\n",
      "     |          fully qualified classname of key converter (None by default)\n",
      "     |      valueConverter : str, optional\n",
      "     |          fully qualified classname of value converter (None by default)\n",
      "     |      conf : dict, optional\n",
      "     |          Hadoop job configuration (None by default)\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.newAPIHadoopFile`\n",
      "     |      :meth:`RDD.saveAsHadoopDataset`\n",
      "     |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n",
      "     |      :meth:`RDD.saveAsHadoopFile`\n",
      "     |      :meth:`RDD.saveAsSequenceFile`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      \n",
      "     |      Set the class of output format\n",
      "     |      \n",
      "     |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     path = os.path.join(d, \"hadoop_file\")\n",
      "     |      ...\n",
      "     |      ...     # Write a temporary Hadoop file\n",
      "     |      ...     rdd = sc.parallelize([(1, {3.0: \"bb\"}), (2, {1.0: \"aa\"}), (3, {2.0: \"dd\"})])\n",
      "     |      ...     rdd.saveAsNewAPIHadoopFile(path, output_format_class)\n",
      "     |      ...\n",
      "     |      ...     # Load this Hadoop file as an RDD\n",
      "     |      ...     sorted(sc.sequenceFile(path).collect())\n",
      "     |      [(1, {3.0: 'bb'}), (2, {1.0: 'aa'}), (3, {2.0: 'dd'})]\n",
      "     |  \n",
      "     |  saveAsPickleFile(self, path: str, batchSize: int = 10) -> None\n",
      "     |      Save this RDD as a SequenceFile of serialized objects. The serializer\n",
      "     |      used is :class:`pyspark.serializers.CPickleSerializer`, default batch size\n",
      "     |      is 10.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          path to pickled file\n",
      "     |      batchSize : int, optional, default 10\n",
      "     |          the number of Python objects represented as a single Java object.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.pickleFile`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     path = os.path.join(d, \"pickle_file\")\n",
      "     |      ...\n",
      "     |      ...     # Write a temporary pickled file\n",
      "     |      ...     sc.parallelize(range(10)).saveAsPickleFile(path, 3)\n",
      "     |      ...\n",
      "     |      ...     # Load picked file as an RDD\n",
      "     |      ...     sorted(sc.pickleFile(path, 3).collect())\n",
      "     |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "     |  \n",
      "     |  saveAsSequenceFile(self: 'RDD[Tuple[K, V]]', path: str, compressionCodecClass: Optional[str] = None) -> None\n",
      "     |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      "     |      system, using the \"org.apache.hadoop.io.Writable\" types that we convert from the\n",
      "     |      RDD's key and value types. The mechanism is as follows:\n",
      "     |      \n",
      "     |          1. Pickle is used to convert pickled Python RDD into RDD of Java objects.\n",
      "     |          2. Keys and values of this Java RDD are converted to Writables and written out.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          path to sequence file\n",
      "     |      compressionCodecClass : str, optional\n",
      "     |          fully qualified classname of the compression codec class\n",
      "     |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.sequenceFile`\n",
      "     |      :meth:`RDD.saveAsHadoopFile`\n",
      "     |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
      "     |      :meth:`RDD.saveAsHadoopDataset`\n",
      "     |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n",
      "     |      :meth:`RDD.saveAsSequenceFile`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      \n",
      "     |      Set the related classes\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     path = os.path.join(d, \"sequence_file\")\n",
      "     |      ...\n",
      "     |      ...     # Write a temporary sequence file\n",
      "     |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      "     |      ...     rdd.saveAsSequenceFile(path)\n",
      "     |      ...\n",
      "     |      ...     # Load this sequence file as an RDD\n",
      "     |      ...     loaded = sc.sequenceFile(path)\n",
      "     |      ...     sorted(loaded.collect())\n",
      "     |      [(1, ''), (1, 'a'), (3, 'x')]\n",
      "     |  \n",
      "     |  saveAsTextFile(self, path: str, compressionCodecClass: Optional[str] = None) -> None\n",
      "     |      Save this RDD as a text file, using string representations of elements.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          path to text file\n",
      "     |      compressionCodecClass : str, optional\n",
      "     |          fully qualified classname of the compression codec class\n",
      "     |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.textFile`\n",
      "     |      :meth:`SparkContext.wholeTextFiles`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      >>> from fileinput import input\n",
      "     |      >>> from glob import glob\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d1:\n",
      "     |      ...     path1 = os.path.join(d1, \"text_file1\")\n",
      "     |      ...\n",
      "     |      ...     # Write a temporary text file\n",
      "     |      ...     sc.parallelize(range(10)).saveAsTextFile(path1)\n",
      "     |      ...\n",
      "     |      ...     # Load text file as an RDD\n",
      "     |      ...     ''.join(sorted(input(glob(path1 + \"/part-0000*\"))))\n",
      "     |      '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n",
      "     |      \n",
      "     |      Empty lines are tolerated when saving to text files.\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d2:\n",
      "     |      ...     path2 = os.path.join(d2, \"text2_file2\")\n",
      "     |      ...\n",
      "     |      ...     # Write another temporary text file\n",
      "     |      ...     sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(path2)\n",
      "     |      ...\n",
      "     |      ...     # Load text file as an RDD\n",
      "     |      ...     ''.join(sorted(input(glob(path2 + \"/part-0000*\"))))\n",
      "     |      '\\n\\n\\nbar\\nfoo\\n'\n",
      "     |      \n",
      "     |      Using compressionCodecClass\n",
      "     |      \n",
      "     |      >>> from fileinput import input, hook_compressed\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d3:\n",
      "     |      ...     path3 = os.path.join(d3, \"text3\")\n",
      "     |      ...     codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
      "     |      ...\n",
      "     |      ...     # Write another temporary text file with specified codec\n",
      "     |      ...     sc.parallelize(['foo', 'bar']).saveAsTextFile(path3, codec)\n",
      "     |      ...\n",
      "     |      ...     # Load text file as an RDD\n",
      "     |      ...     result = sorted(input(glob(path3 + \"/part*.gz\"), openhook=hook_compressed))\n",
      "     |      ...     ''.join([r.decode('utf-8') if isinstance(r, bytes) else r for r in result])\n",
      "     |      'bar\\nfoo\\n'\n",
      "     |  \n",
      "     |  setName(self: 'RDD[T]', name: str) -> 'RDD[T]'\n",
      "     |      Assign a name to this RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          new name\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          the same :class:`RDD` with name updated\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.name`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([1, 2])\n",
      "     |      >>> rdd.setName('I am an RDD').name()\n",
      "     |      'I am an RDD'\n",
      "     |  \n",
      "     |  sortBy(self: 'RDD[T]', keyfunc: Callable[[~T], ForwardRef('S')], ascending: bool = True, numPartitions: Optional[int] = None) -> 'RDD[T]'\n",
      "     |      Sorts this RDD by the given keyfunc\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      keyfunc : function\n",
      "     |          a function to compute the key\n",
      "     |      ascending : bool, optional, default True\n",
      "     |          sort the keys in ascending or descending order\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a new :class:`RDD`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.sortByKey`\n",
      "     |      :meth:`pyspark.sql.DataFrame.sort`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      "     |      >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n",
      "     |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      "     |      >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n",
      "     |      [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      "     |  \n",
      "     |  sortByKey(self: 'RDD[Tuple[K, V]]', ascending: Optional[bool] = True, numPartitions: Optional[int] = None, keyfunc: Callable[[Any], Any] = <function RDD.<lambda> at 0x7f4a9018d800>) -> 'RDD[Tuple[K, V]]'\n",
      "     |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.9.1\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ascending : bool, optional, default True\n",
      "     |          sort the keys in ascending or descending order\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      keyfunc : function, optional, default identity mapping\n",
      "     |          a function to compute the key\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a new :class:`RDD`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.sortBy`\n",
      "     |      :meth:`pyspark.sql.DataFrame.sort`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      "     |      >>> sc.parallelize(tmp).sortByKey().first()\n",
      "     |      ('1', 3)\n",
      "     |      >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
      "     |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      "     |      >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
      "     |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      "     |      >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
      "     |      >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
      "     |      >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
      "     |      [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n",
      "     |  \n",
      "     |  stats(self: 'RDD[NumberOrArray]') -> pyspark.statcounter.StatCounter\n",
      "     |      Return a :class:`StatCounter` object that captures the mean, variance\n",
      "     |      and count of the RDD's elements in one operation.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.9.1\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`StatCounter`\n",
      "     |          a :class:`StatCounter` capturing the mean, variance and count of all elements\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.stdev`\n",
      "     |      :meth:`RDD.sampleStdev`\n",
      "     |      :meth:`RDD.variance`\n",
      "     |      :meth:`RDD.sampleVariance`\n",
      "     |      :meth:`RDD.histogram`\n",
      "     |      :meth:`pyspark.sql.DataFrame.stat`\n",
      "     |  \n",
      "     |  stdev(self: 'RDD[NumberOrArray]') -> float\n",
      "     |      Compute the standard deviation of this RDD's elements.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.9.1\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float\n",
      "     |          the standard deviation of all elements\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.stats`\n",
      "     |      :meth:`RDD.sampleStdev`\n",
      "     |      :meth:`RDD.variance`\n",
      "     |      :meth:`RDD.sampleVariance`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.parallelize([1, 2, 3]).stdev()\n",
      "     |      0.816...\n",
      "     |  \n",
      "     |  subtract(self: 'RDD[T]', other: 'RDD[T]', numPartitions: Optional[int] = None) -> 'RDD[T]'\n",
      "     |      Return each value in `self` that is not contained in `other`.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.9.1\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`RDD`\n",
      "     |          another :class:`RDD`\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` with the elements from this that are not in `other`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.subtractByKey`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
      "     |      >>> rdd2 = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      "     |      >>> sorted(rdd1.subtract(rdd2).collect())\n",
      "     |      [('a', 1), ('b', 4), ('b', 5)]\n",
      "     |  \n",
      "     |  subtractByKey(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, Any]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, V]]'\n",
      "     |      Return each (key, value) pair in `self` that has no pair with matching\n",
      "     |      key in `other`.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.9.1\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`RDD`\n",
      "     |          another :class:`RDD`\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions in new :class:`RDD`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` with the pairs from this whose keys are not in `other`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.subtract`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n",
      "     |      >>> rdd2 = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      "     |      >>> sorted(rdd1.subtractByKey(rdd2).collect())\n",
      "     |      [('b', 4), ('b', 5)]\n",
      "     |  \n",
      "     |  sum(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n",
      "     |      Add up the elements in this RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float, int, or complex\n",
      "     |          the sum of all elements\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.mean`\n",
      "     |      :meth:`RDD.sumApprox`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n",
      "     |      6.0\n",
      "     |  \n",
      "     |  sumApprox(self: 'RDD[Union[float, int]]', timeout: int, confidence: float = 0.95) -> pyspark.rdd.BoundedFloat\n",
      "     |      Approximate operation to return the sum within a timeout\n",
      "     |      or meet the confidence.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      timeout : int\n",
      "     |          maximum time to wait for the job, in milliseconds\n",
      "     |      confidence : float\n",
      "     |          the desired statistical confidence in the result\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`BoundedFloat`\n",
      "     |          a potentially incomplete result, with error bounds\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.sum`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      "     |      >>> r = sum(range(1000))\n",
      "     |      >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n",
      "     |      True\n",
      "     |  \n",
      "     |  take(self: 'RDD[T]', num: int) -> List[~T]\n",
      "     |      Take the first num elements of the RDD.\n",
      "     |      \n",
      "     |      It works by first scanning one partition, and use the results from\n",
      "     |      that partition to estimate the number of additional partitions needed\n",
      "     |      to satisfy the limit.\n",
      "     |      \n",
      "     |      Translated from the Scala implementation in RDD#take().\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      num : int\n",
      "     |          first number of elements\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          the first `num` elements\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.first`\n",
      "     |      :meth:`pyspark.sql.DataFrame.take`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method should only be used if the resulting array is expected\n",
      "     |      to be small, as all the data is loaded into the driver's memory.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
      "     |      [2, 3]\n",
      "     |      >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
      "     |      [2, 3, 4, 5, 6]\n",
      "     |      >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
      "     |      [91, 92, 93]\n",
      "     |  \n",
      "     |  takeOrdered(self: 'RDD[T]', num: int, key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> List[~T]\n",
      "     |      Get the N elements from an RDD ordered in ascending order or as\n",
      "     |      specified by the optional key function.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      num : int\n",
      "     |          top N\n",
      "     |      key : function, optional\n",
      "     |          a function used to generate key for comparing\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          the top N elements\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.top`\n",
      "     |      :meth:`RDD.max`\n",
      "     |      :meth:`RDD.min`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method should only be used if the resulting array is expected\n",
      "     |      to be small, as all the data is loaded into the driver's memory.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n",
      "     |      [1, 2, 3, 4, 5, 6]\n",
      "     |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n",
      "     |      [10, 9, 7, 6, 5, 4]\n",
      "     |      >>> sc.emptyRDD().takeOrdered(3)\n",
      "     |      []\n",
      "     |  \n",
      "     |  takeSample(self: 'RDD[T]', withReplacement: bool, num: int, seed: Optional[int] = None) -> List[~T]\n",
      "     |      Return a fixed-size sampled subset of this RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      withReplacement : list\n",
      "     |          whether sampling is done with replacement\n",
      "     |      num : int\n",
      "     |          size of the returned sample\n",
      "     |      seed : int, optional\n",
      "     |          random seed\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          a fixed-size sampled subset of this :class:`RDD` in an array\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.sample`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method should only be used if the resulting array is expected\n",
      "     |      to be small, as all the data is loaded into the driver's memory.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import sys\n",
      "     |      >>> rdd = sc.parallelize(range(0, 10))\n",
      "     |      >>> len(rdd.takeSample(True, 20, 1))\n",
      "     |      20\n",
      "     |      >>> len(rdd.takeSample(False, 5, 2))\n",
      "     |      5\n",
      "     |      >>> len(rdd.takeSample(False, 15, 3))\n",
      "     |      10\n",
      "     |      >>> sc.range(0, 10).takeSample(False, sys.maxsize)\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      ValueError: Sample size cannot be greater than ...\n",
      "     |  \n",
      "     |  toDF(self, schema=None, sampleRatio=None)\n",
      "     |      Converts current :class:`RDD` into a :class:`DataFrame`\n",
      "     |      \n",
      "     |      This is a shorthand for ``spark.createDataFrame(rdd, schema, sampleRatio)``\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "     |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "     |          column names, default is None.  The data type string format equals to\n",
      "     |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "     |          omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
      "     |          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n",
      "     |          We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n",
      "     |      sampleRatio : float, optional\n",
      "     |          the sample ratio of rows used for inferring\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = spark.range(1).rdd.map(lambda x: tuple(x))\n",
      "     |      >>> rdd.collect()\n",
      "     |      [(0,)]\n",
      "     |      >>> rdd.toDF().show()\n",
      "     |      +---+\n",
      "     |      | _1|\n",
      "     |      +---+\n",
      "     |      |  0|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  toDebugString(self) -> Optional[bytes]\n",
      "     |      A description of this RDD and its recursive dependencies for debugging.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bytes\n",
      "     |          debugging information of this :class:`RDD`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.range(5)\n",
      "     |      >>> rdd.toDebugString()\n",
      "     |      b'...PythonRDD...ParallelCollectionRDD...'\n",
      "     |  \n",
      "     |  toLocalIterator(self: 'RDD[T]', prefetchPartitions: bool = False) -> Iterator[~T]\n",
      "     |      Return an iterator that contains all of the elements in this RDD.\n",
      "     |      The iterator will consume as much memory as the largest partition in this RDD.\n",
      "     |      With prefetch it may consume up to the memory of the 2 largest partitions.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      prefetchPartitions : bool, optional\n",
      "     |          If Spark should pre-fetch the next partition\n",
      "     |          before it is needed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`collections.abc.Iterator`\n",
      "     |          an iterator that contains all of the elements in this :class:`RDD`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.collect`\n",
      "     |      :meth:`pyspark.sql.DataFrame.toLocalIterator`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize(range(10))\n",
      "     |      >>> [x for x in rdd.toLocalIterator()]\n",
      "     |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "     |  \n",
      "     |  top(self: 'RDD[T]', num: int, key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> List[~T]\n",
      "     |      Get the top N elements from an RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      num : int\n",
      "     |          top N\n",
      "     |      key : function, optional\n",
      "     |          a function used to generate key for comparing\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          the top N elements\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.takeOrdered`\n",
      "     |      :meth:`RDD.max`\n",
      "     |      :meth:`RDD.min`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method should only be used if the resulting array is expected\n",
      "     |      to be small, as all the data is loaded into the driver's memory.\n",
      "     |      \n",
      "     |      It returns the list sorted in descending order.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n",
      "     |      [12]\n",
      "     |      >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n",
      "     |      [6, 5]\n",
      "     |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n",
      "     |      [4, 3, 2]\n",
      "     |  \n",
      "     |  treeAggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U], depth: int = 2) -> ~U\n",
      "     |      Aggregates the elements of this RDD in a multi-level tree\n",
      "     |      pattern.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      zeroValue : U\n",
      "     |          the initial value for the accumulated result of each partition\n",
      "     |      seqOp : function\n",
      "     |          a function used to accumulate results within a partition\n",
      "     |      combOp : function\n",
      "     |          an associative function used to combine results from different partitions\n",
      "     |      depth : int, optional, default 2\n",
      "     |          suggested depth of the tree\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      U\n",
      "     |          the aggregated result\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.aggregate`\n",
      "     |      :meth:`RDD.treeReduce`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> add = lambda x, y: x + y\n",
      "     |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      "     |      >>> rdd.treeAggregate(0, add, add)\n",
      "     |      -5\n",
      "     |      >>> rdd.treeAggregate(0, add, add, 1)\n",
      "     |      -5\n",
      "     |      >>> rdd.treeAggregate(0, add, add, 2)\n",
      "     |      -5\n",
      "     |      >>> rdd.treeAggregate(0, add, add, 5)\n",
      "     |      -5\n",
      "     |      >>> rdd.treeAggregate(0, add, add, 10)\n",
      "     |      -5\n",
      "     |  \n",
      "     |  treeReduce(self: 'RDD[T]', f: Callable[[~T, ~T], ~T], depth: int = 2) -> ~T\n",
      "     |      Reduces the elements of this RDD in a multi-level tree pattern.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |          the reduce function\n",
      "     |      depth : int, optional, default 2\n",
      "     |          suggested depth of the tree (default: 2)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T\n",
      "     |          the aggregated result\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.reduce`\n",
      "     |      :meth:`RDD.aggregate`\n",
      "     |      :meth:`RDD.treeAggregate`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> add = lambda x, y: x + y\n",
      "     |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      "     |      >>> rdd.treeReduce(add)\n",
      "     |      -5\n",
      "     |      >>> rdd.treeReduce(add, 1)\n",
      "     |      -5\n",
      "     |      >>> rdd.treeReduce(add, 2)\n",
      "     |      -5\n",
      "     |      >>> rdd.treeReduce(add, 5)\n",
      "     |      -5\n",
      "     |      >>> rdd.treeReduce(add, 10)\n",
      "     |      -5\n",
      "     |  \n",
      "     |  union(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n",
      "     |      Return the union of this RDD and another one.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`RDD`\n",
      "     |          another :class:`RDD`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          the union of this :class:`RDD` and another one\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.union`\n",
      "     |      :meth:`pyspark.sql.DataFrame.union`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      "     |      >>> rdd.union(rdd).collect()\n",
      "     |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      "     |  \n",
      "     |  unpersist(self: 'RDD[T]', blocking: bool = False) -> 'RDD[T]'\n",
      "     |      Mark the RDD as non-persistent, and remove all blocks for it from\n",
      "     |      memory and disk.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.9.1\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      blocking : bool, optional, default False\n",
      "     |          whether to block until all blocks are deleted\n",
      "     |      \n",
      "     |          .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          The same :class:`RDD`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.cache`\n",
      "     |      :meth:`RDD.persist`\n",
      "     |      :meth:`RDD.getStorageLevel`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.range(5)\n",
      "     |      >>> rdd.is_cached\n",
      "     |      False\n",
      "     |      >>> _ = rdd.unpersist()\n",
      "     |      >>> rdd.is_cached\n",
      "     |      False\n",
      "     |      >>> _ = rdd.cache()\n",
      "     |      >>> rdd.is_cached\n",
      "     |      True\n",
      "     |      >>> _ = rdd.unpersist()\n",
      "     |      >>> rdd.is_cached\n",
      "     |      False\n",
      "     |      >>> _ = rdd.unpersist()\n",
      "     |  \n",
      "     |  values(self: 'RDD[Tuple[K, V]]') -> 'RDD[V]'\n",
      "     |      Return an RDD with the values of each tuple.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` only containing the values\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.keys`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([(1, 2), (3, 4)]).values()\n",
      "     |      >>> rdd.collect()\n",
      "     |      [2, 4]\n",
      "     |  \n",
      "     |  variance(self: 'RDD[NumberOrArray]') -> float\n",
      "     |      Compute the variance of this RDD's elements.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.9.1\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float\n",
      "     |          the variance of all elements\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.stats`\n",
      "     |      :meth:`RDD.sampleVariance`\n",
      "     |      :meth:`RDD.stdev`\n",
      "     |      :meth:`RDD.sampleStdev`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.parallelize([1, 2, 3]).variance()\n",
      "     |      0.666...\n",
      "     |  \n",
      "     |  withResources(self: 'RDD[T]', profile: pyspark.resource.profile.ResourceProfile) -> 'RDD[T]'\n",
      "     |      Specify a :class:`pyspark.resource.ResourceProfile` to use when calculating this RDD.\n",
      "     |      This is only supported on certain cluster managers and currently requires dynamic\n",
      "     |      allocation to be enabled. It will result in new executors with the resources specified\n",
      "     |      being acquired to calculate the RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      profile : :class:`pyspark.resource.ResourceProfile`\n",
      "     |          a resource profile\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          the same :class:`RDD` with user specified profile\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.getResourceProfile`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is experimental\n",
      "     |  \n",
      "     |  zip(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n",
      "     |      Zips this RDD with another one, returning key-value pairs with the\n",
      "     |      first element in each RDD second element in each RDD, etc. Assumes\n",
      "     |      that the two RDDs have the same number of partitions and the same\n",
      "     |      number of elements in each partition (e.g. one was made through\n",
      "     |      a map on the other).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`RDD`\n",
      "     |          another :class:`RDD`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` containing the zipped key-value pairs\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.zipWithIndex`\n",
      "     |      :meth:`RDD.zipWithUniqueId`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd1 = sc.parallelize(range(0,5))\n",
      "     |      >>> rdd2 = sc.parallelize(range(1000, 1005))\n",
      "     |      >>> rdd1.zip(rdd2).collect()\n",
      "     |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n",
      "     |  \n",
      "     |  zipWithIndex(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n",
      "     |      Zips this RDD with its element indices.\n",
      "     |      \n",
      "     |      The ordering is first based on the partition index and then the\n",
      "     |      ordering of items within each partition. So the first item in\n",
      "     |      the first partition gets index 0, and the last item in the last\n",
      "     |      partition receives the largest index.\n",
      "     |      \n",
      "     |      This method needs to trigger a spark job when this RDD contains\n",
      "     |      more than one partitions.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` containing the zipped key-index pairs\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.zip`\n",
      "     |      :meth:`RDD.zipWithUniqueId`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n",
      "     |      [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n",
      "     |  \n",
      "     |  zipWithUniqueId(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n",
      "     |      Zips this RDD with generated unique Long ids.\n",
      "     |      \n",
      "     |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n",
      "     |      n is the number of partitions. So there may exist gaps, but this\n",
      "     |      method won't trigger a spark job, which is different from\n",
      "     |      :meth:`zipWithIndex`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a :class:`RDD` containing the zipped key-UniqueId pairs\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.zip`\n",
      "     |      :meth:`RDD.zipWithIndex`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n",
      "     |      [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  context\n",
      "     |      The :class:`SparkContext` that this RDD was created on.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`SparkContext`\n",
      "     |          The :class:`SparkContext` that this RDD was created on\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.range(5)\n",
      "     |      >>> rdd.context\n",
      "     |      <SparkContext ...>\n",
      "     |      >>> rdd.context is sc\n",
      "     |      True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  __parameters__ = (+T_co,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |      Parameterizes a generic class.\n",
      "     |      \n",
      "     |      At least, parameterizing a generic class is the *main* thing this method\n",
      "     |      does. For example, for some generic class `Foo`, this is called when we\n",
      "     |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      "     |      \n",
      "     |      However, note that this method is also called when defining generic\n",
      "     |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class RDDBarrier(typing.Generic)\n",
      "     |  RDDBarrier(rdd: pyspark.rdd.RDD[~T])\n",
      "     |  \n",
      "     |  Wraps an RDD in a barrier stage, which forces Spark to launch tasks of this stage together.\n",
      "     |  :class:`RDDBarrier` instances are created by :meth:`RDD.barrier`.\n",
      "     |  \n",
      "     |  .. versionadded:: 2.4.0\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  This API is experimental\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RDDBarrier\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, rdd: pyspark.rdd.RDD[~T])\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  mapPartitions(self, f: Callable[[Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> pyspark.rdd.RDD[~U]\n",
      "     |      Returns a new RDD by applying a function to each partition of the wrapped RDD,\n",
      "     |      where tasks are launched together in a barrier stage.\n",
      "     |      The interface is the same as :meth:`RDD.mapPartitions`.\n",
      "     |      Please see the API doc there.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |         a function to run on each partition of the RDD\n",
      "     |      preservesPartitioning : bool, optional, default False\n",
      "     |          indicates whether the input function preserves the partitioner,\n",
      "     |          which should be False unless this is a pair RDD and the input\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a new :class:`RDD` by applying a function to each partition\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.mapPartitions`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is experimental\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      "     |      >>> def f(iterator): yield sum(iterator)\n",
      "     |      ...\n",
      "     |      >>> barrier = rdd.barrier()\n",
      "     |      >>> barrier\n",
      "     |      <pyspark.rdd.RDDBarrier ...>\n",
      "     |      >>> barrier.mapPartitions(f).collect()\n",
      "     |      [3, 7]\n",
      "     |  \n",
      "     |  mapPartitionsWithIndex(self, f: Callable[[int, Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> pyspark.rdd.RDD[~U]\n",
      "     |      Returns a new RDD by applying a function to each partition of the wrapped RDD, while\n",
      "     |      tracking the index of the original partition. And all tasks are launched together\n",
      "     |      in a barrier stage.\n",
      "     |      The interface is the same as :meth:`RDD.mapPartitionsWithIndex`.\n",
      "     |      Please see the API doc there.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |         a function to run on each partition of the RDD\n",
      "     |      preservesPartitioning : bool, optional, default False\n",
      "     |          indicates whether the input function preserves the partitioner,\n",
      "     |          which should be False unless this is a pair RDD and the input\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          a new :class:`RDD` by applying a function to each partition\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.mapPartitionsWithIndex`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is experimental\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      "     |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      "     |      ...\n",
      "     |      >>> barrier = rdd.barrier()\n",
      "     |      >>> barrier\n",
      "     |      <pyspark.rdd.RDDBarrier ...>\n",
      "     |      >>> barrier.mapPartitionsWithIndex(f).sum()\n",
      "     |      6\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[~T],)\n",
      "     |  \n",
      "     |  __parameters__ = (~T,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |      Parameterizes a generic class.\n",
      "     |      \n",
      "     |      At least, parameterizing a generic class is the *main* thing this method\n",
      "     |      does. For example, for some generic class `Foo`, this is called when we\n",
      "     |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      "     |      \n",
      "     |      However, note that this method is also called when defining generic\n",
      "     |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class SparkConf(builtins.object)\n",
      "     |  SparkConf(loadDefaults: bool = True, _jvm: Optional[py4j.java_gateway.JVMView] = None, _jconf: Optional[py4j.java_gateway.JavaObject] = None)\n",
      "     |  \n",
      "     |  Configuration for a Spark application. Used to set various Spark\n",
      "     |  parameters as key-value pairs.\n",
      "     |  \n",
      "     |  Most of the time, you would create a SparkConf object with\n",
      "     |  ``SparkConf()``, which will load values from `spark.*` Java system\n",
      "     |  properties as well. In this case, any parameters you set directly on\n",
      "     |  the :class:`SparkConf` object take priority over system properties.\n",
      "     |  \n",
      "     |  For unit tests, you can also call ``SparkConf(false)`` to skip\n",
      "     |  loading external settings and get the same configuration no matter\n",
      "     |  what the system properties are.\n",
      "     |  \n",
      "     |  All setter methods in this class support chaining. For example,\n",
      "     |  you can write ``conf.setMaster(\"local\").setAppName(\"My app\")``.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loadDefaults : bool\n",
      "     |      whether to load values from Java system properties (True by default)\n",
      "     |  _jvm : class:`py4j.java_gateway.JVMView`\n",
      "     |      internal parameter used to pass a handle to the\n",
      "     |      Java VM; does not need to be set by users\n",
      "     |  _jconf : class:`py4j.java_gateway.JavaObject`\n",
      "     |      Optionally pass in an existing SparkConf handle\n",
      "     |      to use its parameters\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Once a SparkConf object is passed to Spark, it is cloned\n",
      "     |  and can no longer be modified by the user.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from pyspark.conf import SparkConf\n",
      "     |  >>> from pyspark.context import SparkContext\n",
      "     |  >>> conf = SparkConf()\n",
      "     |  >>> conf.setMaster(\"local\").setAppName(\"My app\")\n",
      "     |  <pyspark.conf.SparkConf object at ...>\n",
      "     |  >>> conf.get(\"spark.master\")\n",
      "     |  'local'\n",
      "     |  >>> conf.get(\"spark.app.name\")\n",
      "     |  'My app'\n",
      "     |  >>> sc = SparkContext(conf=conf)\n",
      "     |  >>> sc.master\n",
      "     |  'local'\n",
      "     |  >>> sc.appName\n",
      "     |  'My app'\n",
      "     |  >>> sc.sparkHome is None\n",
      "     |  True\n",
      "     |  \n",
      "     |  >>> conf = SparkConf(loadDefaults=False)\n",
      "     |  >>> conf.setSparkHome(\"/path\")\n",
      "     |  <pyspark.conf.SparkConf object at ...>\n",
      "     |  >>> conf.get(\"spark.home\")\n",
      "     |  '/path'\n",
      "     |  >>> conf.setExecutorEnv(\"VAR1\", \"value1\")\n",
      "     |  <pyspark.conf.SparkConf object at ...>\n",
      "     |  >>> conf.setExecutorEnv(pairs = [(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "     |  <pyspark.conf.SparkConf object at ...>\n",
      "     |  >>> conf.get(\"spark.executorEnv.VAR1\")\n",
      "     |  'value1'\n",
      "     |  >>> print(conf.toDebugString())\n",
      "     |  spark.executorEnv.VAR1=value1\n",
      "     |  spark.executorEnv.VAR3=value3\n",
      "     |  spark.executorEnv.VAR4=value4\n",
      "     |  spark.home=/path\n",
      "     |  >>> for p in sorted(conf.getAll(), key=lambda p: p[0]):\n",
      "     |  ...     print(p)\n",
      "     |  ('spark.executorEnv.VAR1', 'value1')\n",
      "     |  ('spark.executorEnv.VAR3', 'value3')\n",
      "     |  ('spark.executorEnv.VAR4', 'value4')\n",
      "     |  ('spark.home', '/path')\n",
      "     |  >>> conf._jconf.setExecutorEnv(\"VAR5\", \"value5\")\n",
      "     |  JavaObject id...\n",
      "     |  >>> print(conf.toDebugString())\n",
      "     |  spark.executorEnv.VAR1=value1\n",
      "     |  spark.executorEnv.VAR3=value3\n",
      "     |  spark.executorEnv.VAR4=value4\n",
      "     |  spark.executorEnv.VAR5=value5\n",
      "     |  spark.home=/path\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loadDefaults: bool = True, _jvm: Optional[py4j.java_gateway.JVMView] = None, _jconf: Optional[py4j.java_gateway.JavaObject] = None)\n",
      "     |      Create a new Spark configuration.\n",
      "     |  \n",
      "     |  contains(self, key: str) -> bool\n",
      "     |      Does this configuration contain a given key?\n",
      "     |  \n",
      "     |  get(self, key: str, defaultValue: Optional[str] = None) -> Optional[str]\n",
      "     |      Get the configured value for some key, or return a default otherwise.\n",
      "     |  \n",
      "     |  getAll(self) -> List[Tuple[str, str]]\n",
      "     |      Get all values as a list of key-value pairs.\n",
      "     |  \n",
      "     |  set(self, key: str, value: str) -> 'SparkConf'\n",
      "     |      Set a configuration property.\n",
      "     |  \n",
      "     |  setAll(self, pairs: List[Tuple[str, str]]) -> 'SparkConf'\n",
      "     |      Set multiple parameters, passed as a list of key-value pairs.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      pairs : iterable of tuples\n",
      "     |          list of key-value pairs to set\n",
      "     |  \n",
      "     |  setAppName(self, value: str) -> 'SparkConf'\n",
      "     |      Set application name.\n",
      "     |  \n",
      "     |  setExecutorEnv(self, key: Optional[str] = None, value: Optional[str] = None, pairs: Optional[List[Tuple[str, str]]] = None) -> 'SparkConf'\n",
      "     |      Set an environment variable to be passed to executors.\n",
      "     |  \n",
      "     |  setIfMissing(self, key: str, value: str) -> 'SparkConf'\n",
      "     |      Set a configuration property, if not already set.\n",
      "     |  \n",
      "     |  setMaster(self, value: str) -> 'SparkConf'\n",
      "     |      Set master URL to connect to.\n",
      "     |  \n",
      "     |  setSparkHome(self, value: str) -> 'SparkConf'\n",
      "     |      Set path where Spark is installed on worker nodes.\n",
      "     |  \n",
      "     |  toDebugString(self) -> str\n",
      "     |      Returns a printable version of the configuration, as a list of\n",
      "     |      key=value pairs, one per line.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'_conf': typing.Optional[typing.Dict[str, str]], '_...\n",
      "    \n",
      "    class SparkContext(builtins.object)\n",
      "     |  SparkContext(master: Optional[str] = None, appName: Optional[str] = None, sparkHome: Optional[str] = None, pyFiles: Optional[List[str]] = None, environment: Optional[Dict[str, Any]] = None, batchSize: int = 0, serializer: 'Serializer' = CloudPickleSerializer(), conf: Optional[pyspark.conf.SparkConf] = None, gateway: Optional[py4j.java_gateway.JavaGateway] = None, jsc: Optional[py4j.java_gateway.JavaObject] = None, profiler_cls: Type[pyspark.profiler.BasicProfiler] = <class 'pyspark.profiler.BasicProfiler'>, udf_profiler_cls: Type[pyspark.profiler.UDFBasicProfiler] = <class 'pyspark.profiler.UDFBasicProfiler'>, memory_profiler_cls: Type[pyspark.profiler.MemoryProfiler] = <class 'pyspark.profiler.MemoryProfiler'>)\n",
      "     |  \n",
      "     |  Main entry point for Spark functionality. A SparkContext represents the\n",
      "     |  connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
      "     |  broadcast variables on that cluster.\n",
      "     |  \n",
      "     |  When you create a new SparkContext, at least the master and app name should\n",
      "     |  be set, either through the named parameters here or through `conf`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  master : str, optional\n",
      "     |      Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).\n",
      "     |  appName : str, optional\n",
      "     |      A name for your job, to display on the cluster web UI.\n",
      "     |  sparkHome : str, optional\n",
      "     |      Location where Spark is installed on cluster nodes.\n",
      "     |  pyFiles : list, optional\n",
      "     |      Collection of .zip or .py files to send to the cluster\n",
      "     |      and add to PYTHONPATH.  These can be paths on the local file\n",
      "     |      system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
      "     |  environment : dict, optional\n",
      "     |      A dictionary of environment variables to set on\n",
      "     |      worker nodes.\n",
      "     |  batchSize : int, optional, default 0\n",
      "     |      The number of Python objects represented as a single\n",
      "     |      Java object. Set 1 to disable batching, 0 to automatically choose\n",
      "     |      the batch size based on object sizes, or -1 to use an unlimited\n",
      "     |      batch size\n",
      "     |  serializer : :class:`Serializer`, optional, default :class:`CPickleSerializer`\n",
      "     |      The serializer for RDDs.\n",
      "     |  conf : :class:`SparkConf`, optional\n",
      "     |      An object setting Spark properties.\n",
      "     |  gateway : class:`py4j.java_gateway.JavaGateway`,  optional\n",
      "     |      Use an existing gateway and JVM, otherwise a new JVM\n",
      "     |      will be instantiated. This is only used internally.\n",
      "     |  jsc : class:`py4j.java_gateway.JavaObject`, optional\n",
      "     |      The JavaSparkContext instance. This is only used internally.\n",
      "     |  profiler_cls : type, optional, default :class:`BasicProfiler`\n",
      "     |      A class of custom Profiler used to do profiling\n",
      "     |  udf_profiler_cls : type, optional, default :class:`UDFBasicProfiler`\n",
      "     |      A class of custom Profiler used to do udf profiling\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n",
      "     |  the active :class:`SparkContext` before creating a new one.\n",
      "     |  \n",
      "     |  :class:`SparkContext` instance is not supported to share across multiple\n",
      "     |  processes out of the box, and PySpark does not guarantee multi-processing execution.\n",
      "     |  Use threads instead for concurrent processing purpose.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from pyspark.context import SparkContext\n",
      "     |  >>> sc = SparkContext('local', 'test')\n",
      "     |  >>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "     |  Traceback (most recent call last):\n",
      "     |      ...\n",
      "     |  ValueError: ...\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __enter__(self) -> 'SparkContext'\n",
      "     |      Enable 'with SparkContext(...) as sc: app(sc)' syntax.\n",
      "     |  \n",
      "     |  __exit__(self, type: Optional[Type[BaseException]], value: Optional[BaseException], trace: Optional[traceback]) -> None\n",
      "     |      Enable 'with SparkContext(...) as sc: app' syntax.\n",
      "     |      \n",
      "     |      Specifically stop the context on exit of the with block.\n",
      "     |  \n",
      "     |  __getnewargs__(self) -> NoReturn\n",
      "     |  \n",
      "     |  __init__(self, master: Optional[str] = None, appName: Optional[str] = None, sparkHome: Optional[str] = None, pyFiles: Optional[List[str]] = None, environment: Optional[Dict[str, Any]] = None, batchSize: int = 0, serializer: 'Serializer' = CloudPickleSerializer(), conf: Optional[pyspark.conf.SparkConf] = None, gateway: Optional[py4j.java_gateway.JavaGateway] = None, jsc: Optional[py4j.java_gateway.JavaObject] = None, profiler_cls: Type[pyspark.profiler.BasicProfiler] = <class 'pyspark.profiler.BasicProfiler'>, udf_profiler_cls: Type[pyspark.profiler.UDFBasicProfiler] = <class 'pyspark.profiler.UDFBasicProfiler'>, memory_profiler_cls: Type[pyspark.profiler.MemoryProfiler] = <class 'pyspark.profiler.MemoryProfiler'>)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  accumulator(self, value: ~T, accum_param: Optional[ForwardRef('AccumulatorParam[T]')] = None) -> 'Accumulator[T]'\n",
      "     |      Create an :class:`Accumulator` with the given initial value, using a given\n",
      "     |      :class:`AccumulatorParam` helper object to define how to add values of the\n",
      "     |      data type if provided. Default AccumulatorParams are used for integers\n",
      "     |      and floating-point numbers if you do not provide one. For other types,\n",
      "     |      a custom AccumulatorParam can be used.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      value : T\n",
      "     |          initialized value\n",
      "     |      accum_param : :class:`pyspark.AccumulatorParam`, optional\n",
      "     |          helper object to define how to add values\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Accumulator`\n",
      "     |          `Accumulator` object, a shared variable that can be accumulated\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> acc = sc.accumulator(9)\n",
      "     |      >>> acc.value\n",
      "     |      9\n",
      "     |      >>> acc += 1\n",
      "     |      >>> acc.value\n",
      "     |      10\n",
      "     |      \n",
      "     |      Accumulator object can be accumulated in RDD operations:\n",
      "     |      \n",
      "     |      >>> rdd = sc.range(5)\n",
      "     |      >>> def f(x):\n",
      "     |      ...     global acc\n",
      "     |      ...     acc += 1\n",
      "     |      ...\n",
      "     |      >>> rdd.foreach(f)\n",
      "     |      >>> acc.value\n",
      "     |      15\n",
      "     |  \n",
      "     |  addArchive(self, path: str) -> None\n",
      "     |      Add an archive to be downloaded with this Spark job on every node.\n",
      "     |      The `path` passed can be either a local file, a file in HDFS\n",
      "     |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n",
      "     |      FTP URI.\n",
      "     |      \n",
      "     |      To access the file in Spark jobs, use :meth:`SparkFiles.get` with the\n",
      "     |      filename to find its download/unpacked location. The given path should\n",
      "     |      be one of .zip, .tar, .tar.gz, .tgz and .jar.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          can be either a local file, a file in HDFS (or other Hadoop-supported\n",
      "     |          filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,\n",
      "     |          use :meth:`SparkFiles.get` to find its download location.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.listArchives`\n",
      "     |      :meth:`SparkFiles.get`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      A path can be added only once. Subsequent additions of the same path are ignored.\n",
      "     |      This API is experimental.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Creates a zipped file that contains a text file written '100'.\n",
      "     |      \n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      >>> import zipfile\n",
      "     |      >>> from pyspark import SparkFiles\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     path = os.path.join(d, \"test.txt\")\n",
      "     |      ...     with open(path, \"w\") as f:\n",
      "     |      ...         _ = f.write(\"100\")\n",
      "     |      ...\n",
      "     |      ...     zip_path1 = os.path.join(d, \"test1.zip\")\n",
      "     |      ...     with zipfile.ZipFile(zip_path1, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
      "     |      ...         z.write(path, os.path.basename(path))\n",
      "     |      ...\n",
      "     |      ...     zip_path2 = os.path.join(d, \"test2.zip\")\n",
      "     |      ...     with zipfile.ZipFile(zip_path2, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
      "     |      ...         z.write(path, os.path.basename(path))\n",
      "     |      ...\n",
      "     |      ...     sc.addArchive(zip_path1)\n",
      "     |      ...     arch_list1 = sorted(sc.listArchives)\n",
      "     |      ...\n",
      "     |      ...     sc.addArchive(zip_path2)\n",
      "     |      ...     arch_list2 = sorted(sc.listArchives)\n",
      "     |      ...\n",
      "     |      ...     # add zip_path2 twice, this addition will be ignored\n",
      "     |      ...     sc.addArchive(zip_path2)\n",
      "     |      ...     arch_list3 = sorted(sc.listArchives)\n",
      "     |      ...\n",
      "     |      ...     def func(iterator):\n",
      "     |      ...         with open(\"%s/test.txt\" % SparkFiles.get(\"test1.zip\")) as f:\n",
      "     |      ...             mul = int(f.readline())\n",
      "     |      ...             return [x * mul for x in iterator]\n",
      "     |      ...\n",
      "     |      ...     collected = sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n",
      "     |      \n",
      "     |      >>> arch_list1\n",
      "     |      ['file:/.../test1.zip']\n",
      "     |      >>> arch_list2\n",
      "     |      ['file:/.../test1.zip', 'file:/.../test2.zip']\n",
      "     |      >>> arch_list3\n",
      "     |      ['file:/.../test1.zip', 'file:/.../test2.zip']\n",
      "     |      >>> collected\n",
      "     |      [100, 200, 300, 400]\n",
      "     |  \n",
      "     |  addFile(self, path: str, recursive: bool = False) -> None\n",
      "     |      Add a file to be downloaded with this Spark job on every node.\n",
      "     |      The `path` passed can be either a local file, a file in HDFS\n",
      "     |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n",
      "     |      FTP URI.\n",
      "     |      \n",
      "     |      To access the file in Spark jobs, use :meth:`SparkFiles.get` with the\n",
      "     |      filename to find its download location.\n",
      "     |      \n",
      "     |      A directory can be given if the recursive option is set to True.\n",
      "     |      Currently directories are only supported for Hadoop-supported filesystems.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          can be either a local file, a file in HDFS (or other Hadoop-supported\n",
      "     |          filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,\n",
      "     |          use :meth:`SparkFiles.get` to find its download location.\n",
      "     |      recursive : bool, default False\n",
      "     |          whether to recursively add files in the input directory\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.listFiles`\n",
      "     |      :meth:`SparkContext.addPyFile`\n",
      "     |      :meth:`SparkFiles.get`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      A path can be added only once. Subsequent additions of the same path are ignored.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      >>> from pyspark import SparkFiles\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     path1 = os.path.join(d, \"test1.txt\")\n",
      "     |      ...     with open(path1, \"w\") as f:\n",
      "     |      ...         _ = f.write(\"100\")\n",
      "     |      ...\n",
      "     |      ...     path2 = os.path.join(d, \"test2.txt\")\n",
      "     |      ...     with open(path2, \"w\") as f:\n",
      "     |      ...         _ = f.write(\"200\")\n",
      "     |      ...\n",
      "     |      ...     sc.addFile(path1)\n",
      "     |      ...     file_list1 = sorted(sc.listFiles)\n",
      "     |      ...\n",
      "     |      ...     sc.addFile(path2)\n",
      "     |      ...     file_list2 = sorted(sc.listFiles)\n",
      "     |      ...\n",
      "     |      ...     # add path2 twice, this addition will be ignored\n",
      "     |      ...     sc.addFile(path2)\n",
      "     |      ...     file_list3 = sorted(sc.listFiles)\n",
      "     |      ...\n",
      "     |      ...     def func(iterator):\n",
      "     |      ...         with open(SparkFiles.get(\"test1.txt\")) as f:\n",
      "     |      ...             mul = int(f.readline())\n",
      "     |      ...             return [x * mul for x in iterator]\n",
      "     |      ...\n",
      "     |      ...     collected = sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n",
      "     |      \n",
      "     |      >>> file_list1\n",
      "     |      ['file:/.../test1.txt']\n",
      "     |      >>> file_list2\n",
      "     |      ['file:/.../test1.txt', 'file:/.../test2.txt']\n",
      "     |      >>> file_list3\n",
      "     |      ['file:/.../test1.txt', 'file:/.../test2.txt']\n",
      "     |      >>> collected\n",
      "     |      [100, 200, 300, 400]\n",
      "     |  \n",
      "     |  addJobTag(self, tag: str) -> None\n",
      "     |      Add a tag to be assigned to all the jobs started by this thread.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tag : str\n",
      "     |          The tag to be added. Cannot contain ',' (comma) character.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.removeJobTag`\n",
      "     |      :meth:`SparkContext.getJobTags`\n",
      "     |      :meth:`SparkContext.clearJobTags`\n",
      "     |      :meth:`SparkContext.cancelJobsWithTag`\n",
      "     |      :meth:`SparkContext.setInterruptOnCancel`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import threading\n",
      "     |      >>> from time import sleep\n",
      "     |      >>> from pyspark import InheritableThread\n",
      "     |      >>> sc.setInterruptOnCancel(interruptOnCancel=True)\n",
      "     |      >>> result = \"Not Set\"\n",
      "     |      >>> lock = threading.Lock()\n",
      "     |      >>> def map_func(x):\n",
      "     |      ...     sleep(100)\n",
      "     |      ...     raise RuntimeError(\"Task should have been cancelled\")\n",
      "     |      ...\n",
      "     |      >>> def start_job(x):\n",
      "     |      ...     global result\n",
      "     |      ...     try:\n",
      "     |      ...         sc.addJobTag(\"job_to_cancel\")\n",
      "     |      ...         result = sc.parallelize(range(x)).map(map_func).collect()\n",
      "     |      ...     except Exception as e:\n",
      "     |      ...         result = \"Cancelled\"\n",
      "     |      ...     lock.release()\n",
      "     |      ...\n",
      "     |      >>> def stop_job():\n",
      "     |      ...     sleep(5)\n",
      "     |      ...     sc.cancelJobsWithTag(\"job_to_cancel\")\n",
      "     |      ...\n",
      "     |      >>> suppress = lock.acquire()\n",
      "     |      >>> suppress = InheritableThread(target=start_job, args=(10,)).start()\n",
      "     |      >>> suppress = InheritableThread(target=stop_job).start()\n",
      "     |      >>> suppress = lock.acquire()\n",
      "     |      >>> print(result)\n",
      "     |      Cancelled\n",
      "     |      >>> sc.clearJobTags()\n",
      "     |  \n",
      "     |  addPyFile(self, path: str) -> None\n",
      "     |      Add a .py or .zip dependency for all tasks to be executed on this\n",
      "     |      SparkContext in the future.  The `path` passed can be either a local\n",
      "     |      file, a file in HDFS (or other Hadoop-supported filesystems), or an\n",
      "     |      HTTP, HTTPS or FTP URI.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          can be either a .py file or .zip dependency.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.addFile`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      A path can be added only once. Subsequent additions of the same path are ignored.\n",
      "     |  \n",
      "     |  binaryFiles(self, path: str, minPartitions: Optional[int] = None) -> pyspark.rdd.RDD[typing.Tuple[str, bytes]]\n",
      "     |      Read a directory of binary files from HDFS, a local file system\n",
      "     |      (available on all nodes), or any Hadoop-supported file system URI\n",
      "     |      as a byte array. Each file is read as a single record and returned\n",
      "     |      in a key-value pair, where the key is the path of each file, the\n",
      "     |      value is the content of each file.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          directory to the input data files, the path can be comma separated\n",
      "     |          paths as a list of inputs\n",
      "     |      minPartitions : int, optional\n",
      "     |          suggested minimum number of partitions for the resulting RDD\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          RDD representing path-content pairs from the file(s).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Small files are preferred, large file is also allowable, but may cause bad performance.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.binaryRecords`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a temporary binary file\n",
      "     |      ...     with open(os.path.join(d, \"1.bin\"), \"wb\") as f1:\n",
      "     |      ...         _ = f1.write(b\"binary data I\")\n",
      "     |      ...\n",
      "     |      ...     # Write another temporary binary file\n",
      "     |      ...     with open(os.path.join(d, \"2.bin\"), \"wb\") as f2:\n",
      "     |      ...         _ = f2.write(b\"binary data II\")\n",
      "     |      ...\n",
      "     |      ...     collected = sorted(sc.binaryFiles(d).collect())\n",
      "     |      \n",
      "     |      >>> collected\n",
      "     |      [('.../1.bin', b'binary data I'), ('.../2.bin', b'binary data II')]\n",
      "     |  \n",
      "     |  binaryRecords(self, path: str, recordLength: int) -> pyspark.rdd.RDD[bytes]\n",
      "     |      Load data from a flat binary file, assuming each record is a set of numbers\n",
      "     |      with the specified numerical format (see ByteBuffer), and the number of\n",
      "     |      bytes per record is constant.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          Directory to the input data files\n",
      "     |      recordLength : int\n",
      "     |          The length at which to split the records\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          RDD of data with values, represented as byte arrays\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.binaryFiles`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a temporary file\n",
      "     |      ...     with open(os.path.join(d, \"1.bin\"), \"w\") as f:\n",
      "     |      ...         for i in range(3):\n",
      "     |      ...             _ = f.write(\"%04d\" % i)\n",
      "     |      ...\n",
      "     |      ...     # Write another file\n",
      "     |      ...     with open(os.path.join(d, \"2.bin\"), \"w\") as f:\n",
      "     |      ...         for i in [-1, -2, -10]:\n",
      "     |      ...             _ = f.write(\"%04d\" % i)\n",
      "     |      ...\n",
      "     |      ...     collected = sorted(sc.binaryRecords(d, 4).collect())\n",
      "     |      \n",
      "     |      >>> collected\n",
      "     |      [b'-001', b'-002', b'-010', b'0000', b'0001', b'0002']\n",
      "     |  \n",
      "     |  broadcast(self, value: ~T) -> 'Broadcast[T]'\n",
      "     |      Broadcast a read-only variable to the cluster, returning a :class:`Broadcast`\n",
      "     |      object for reading it in distributed functions. The variable will\n",
      "     |      be sent to each cluster only once.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      value : T\n",
      "     |          value to broadcast to the Spark nodes\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Broadcast`\n",
      "     |          :class:`Broadcast` object, a read-only variable cached on each machine\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> mapping = {1: 10001, 2: 10002}\n",
      "     |      >>> bc = sc.broadcast(mapping)\n",
      "     |      \n",
      "     |      >>> rdd = sc.range(5)\n",
      "     |      >>> rdd2 = rdd.map(lambda i: bc.value[i] if i in bc.value else -1)\n",
      "     |      >>> rdd2.collect()\n",
      "     |      [-1, 10001, 10002, -1, -1]\n",
      "     |      \n",
      "     |      >>> bc.destroy()\n",
      "     |  \n",
      "     |  cancelAllJobs(self) -> None\n",
      "     |      Cancel all jobs that have been scheduled or are running.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.cancelJobGroup`\n",
      "     |      :meth:`SparkContext.cancelJobsWithTag`\n",
      "     |      :meth:`SparkContext.runJob`\n",
      "     |  \n",
      "     |  cancelJobGroup(self, groupId: str) -> None\n",
      "     |      Cancel active jobs for the specified group. See :meth:`SparkContext.setJobGroup`.\n",
      "     |      for more information.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      groupId : str\n",
      "     |          The group ID to cancel the job.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.setJobGroup`\n",
      "     |  \n",
      "     |  cancelJobsWithTag(self, tag: str) -> None\n",
      "     |      Cancel active jobs that have the specified tag. See\n",
      "     |      :meth:`SparkContext.addJobTag`.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tag : str\n",
      "     |          The tag to be cancelled. Cannot contain ',' (comma) character.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.addJobTag`\n",
      "     |      :meth:`SparkContext.removeJobTag`\n",
      "     |      :meth:`SparkContext.getJobTags`\n",
      "     |      :meth:`SparkContext.clearJobTags`\n",
      "     |      :meth:`SparkContext.setInterruptOnCancel`\n",
      "     |  \n",
      "     |  clearJobTags(self) -> None\n",
      "     |      Clear the current thread's job tags.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.addJobTag`\n",
      "     |      :meth:`SparkContext.removeJobTag`\n",
      "     |      :meth:`SparkContext.getJobTags`\n",
      "     |      :meth:`SparkContext.cancelJobsWithTag`\n",
      "     |      :meth:`SparkContext.setInterruptOnCancel`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.addJobTag(\"job_to_cancel\")\n",
      "     |      >>> sc.clearJobTags()\n",
      "     |      >>> sc.getJobTags()\n",
      "     |      set()\n",
      "     |  \n",
      "     |  dump_profiles(self, path: str) -> None\n",
      "     |      Dump the profile stats into directory `path`\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2.0\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.show_profiles`\n",
      "     |  \n",
      "     |  emptyRDD(self) -> pyspark.rdd.RDD[typing.Any]\n",
      "     |      Create an :class:`RDD` that has no partitions or elements.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          An empty RDD\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.emptyRDD()\n",
      "     |      EmptyRDD...\n",
      "     |      >>> sc.emptyRDD().count()\n",
      "     |      0\n",
      "     |  \n",
      "     |  getCheckpointDir(self) -> Optional[str]\n",
      "     |      Return the directory where RDDs are checkpointed. Returns None if no\n",
      "     |      checkpoint directory has been set.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.setCheckpointDir`\n",
      "     |      :meth:`RDD.checkpoint`\n",
      "     |      :meth:`RDD.getCheckpointFile`\n",
      "     |  \n",
      "     |  getConf(self) -> pyspark.conf.SparkConf\n",
      "     |      Return a copy of this SparkContext's configuration :class:`SparkConf`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |  \n",
      "     |  getJobTags(self) -> Set[str]\n",
      "     |      Get the tags that are currently set to be assigned to all the jobs started by this thread.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      set of str\n",
      "     |          the tags that are currently set to be assigned to all the jobs started by this thread.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.addJobTag`\n",
      "     |      :meth:`SparkContext.removeJobTag`\n",
      "     |      :meth:`SparkContext.clearJobTags`\n",
      "     |      :meth:`SparkContext.cancelJobsWithTag`\n",
      "     |      :meth:`SparkContext.setInterruptOnCancel`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.addJobTag(\"job_to_cancel\")\n",
      "     |      >>> sc.getJobTags()\n",
      "     |      {'job_to_cancel'}\n",
      "     |      >>> sc.clearJobTags()\n",
      "     |  \n",
      "     |  getLocalProperty(self, key: str) -> Optional[str]\n",
      "     |      Get a local property set in this thread, or null if it is missing. See\n",
      "     |      :meth:`setLocalProperty`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.setLocalProperty`\n",
      "     |  \n",
      "     |  hadoopFile(self, path: str, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n",
      "     |      Read an 'old' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
      "     |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      "     |      The mechanism is the same as for meth:`SparkContext.sequenceFile`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
      "     |      Configuration in Java.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          path to Hadoop file\n",
      "     |      inputFormatClass : str\n",
      "     |          fully qualified classname of Hadoop InputFormat\n",
      "     |          (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      "     |      keyClass : str\n",
      "     |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n",
      "     |      valueClass : str\n",
      "     |          fully qualified classname of value Writable class\n",
      "     |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      "     |      keyConverter : str, optional\n",
      "     |          fully qualified name of a function returning key WritableConverter\n",
      "     |      valueConverter : str, optional\n",
      "     |          fully qualified name of a function returning value WritableConverter\n",
      "     |      conf : dict, optional\n",
      "     |          Hadoop configuration, passed in as a dict\n",
      "     |      batchSize : int, optional, default 0\n",
      "     |          The number of Python objects represented as a single\n",
      "     |          Java object. (default 0, choose batchSize automatically)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          RDD of tuples of key and corresponding value\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.saveAsSequenceFile`\n",
      "     |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
      "     |      :meth:`RDD.saveAsHadoopFile`\n",
      "     |      :meth:`SparkContext.newAPIHadoopFile`\n",
      "     |      :meth:`SparkContext.hadoopRDD`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      \n",
      "     |      Set the related classes\n",
      "     |      \n",
      "     |      >>> output_format_class = \"org.apache.hadoop.mapred.TextOutputFormat\"\n",
      "     |      >>> input_format_class = \"org.apache.hadoop.mapred.TextInputFormat\"\n",
      "     |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
      "     |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     path = os.path.join(d, \"old_hadoop_file\")\n",
      "     |      ...\n",
      "     |      ...     # Write a temporary Hadoop file\n",
      "     |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      "     |      ...     rdd.saveAsHadoopFile(path, output_format_class, key_class, value_class)\n",
      "     |      ...\n",
      "     |      ...     loaded = sc.hadoopFile(path, input_format_class, key_class, value_class)\n",
      "     |      ...     collected = sorted(loaded.collect())\n",
      "     |      \n",
      "     |      >>> collected\n",
      "     |      [(0, '1\\t'), (0, '1\\ta'), (0, '3\\tx')]\n",
      "     |  \n",
      "     |  hadoopRDD(self, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n",
      "     |      Read an 'old' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
      "     |      Hadoop configuration, which is passed in as a Python dict.\n",
      "     |      This will be converted into a Configuration in Java.\n",
      "     |      The mechanism is the same as for meth:`SparkContext.sequenceFile`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      inputFormatClass : str\n",
      "     |          fully qualified classname of Hadoop InputFormat\n",
      "     |          (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      "     |      keyClass : str\n",
      "     |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n",
      "     |      valueClass : str\n",
      "     |          fully qualified classname of value Writable class\n",
      "     |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      "     |      keyConverter : str, optional\n",
      "     |          fully qualified name of a function returning key WritableConverter\n",
      "     |      valueConverter : str, optional\n",
      "     |          fully qualified name of a function returning value WritableConverter\n",
      "     |      conf : dict, optional\n",
      "     |          Hadoop configuration, passed in as a dict\n",
      "     |      batchSize : int, optional, default 0\n",
      "     |          The number of Python objects represented as a single\n",
      "     |          Java object. (default 0, choose batchSize automatically)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          RDD of tuples of key and corresponding value\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n",
      "     |      :meth:`RDD.saveAsHadoopDataset`\n",
      "     |      :meth:`SparkContext.newAPIHadoopRDD`\n",
      "     |      :meth:`SparkContext.hadoopFile`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      \n",
      "     |      Set the related classes\n",
      "     |      \n",
      "     |      >>> output_format_class = \"org.apache.hadoop.mapred.TextOutputFormat\"\n",
      "     |      >>> input_format_class = \"org.apache.hadoop.mapred.TextInputFormat\"\n",
      "     |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
      "     |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     path = os.path.join(d, \"old_hadoop_file\")\n",
      "     |      ...\n",
      "     |      ...     # Create the conf for writing\n",
      "     |      ...     write_conf = {\n",
      "     |      ...         \"mapred.output.format.class\": output_format_class,\n",
      "     |      ...         \"mapreduce.job.output.key.class\": key_class,\n",
      "     |      ...         \"mapreduce.job.output.value.class\": value_class,\n",
      "     |      ...         \"mapreduce.output.fileoutputformat.outputdir\": path,\n",
      "     |      ...     }\n",
      "     |      ...\n",
      "     |      ...     # Write a temporary Hadoop file\n",
      "     |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      "     |      ...     rdd.saveAsHadoopDataset(conf=write_conf)\n",
      "     |      ...\n",
      "     |      ...     # Create the conf for reading\n",
      "     |      ...     read_conf = {\"mapreduce.input.fileinputformat.inputdir\": path}\n",
      "     |      ...\n",
      "     |      ...     loaded = sc.hadoopRDD(input_format_class, key_class, value_class, conf=read_conf)\n",
      "     |      ...     collected = sorted(loaded.collect())\n",
      "     |      \n",
      "     |      >>> collected\n",
      "     |      [(0, '1\\t'), (0, '1\\ta'), (0, '3\\tx')]\n",
      "     |  \n",
      "     |  newAPIHadoopFile(self, path: str, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n",
      "     |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
      "     |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      "     |      The mechanism is the same as for meth:`SparkContext.sequenceFile`.\n",
      "     |      \n",
      "     |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
      "     |      Configuration in Java\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          path to Hadoop file\n",
      "     |      inputFormatClass : str\n",
      "     |          fully qualified classname of Hadoop InputFormat\n",
      "     |          (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      "     |      keyClass : str\n",
      "     |          fully qualified classname of key Writable class\n",
      "     |          (e.g. \"org.apache.hadoop.io.Text\")\n",
      "     |      valueClass : str\n",
      "     |          fully qualified classname of value Writable class\n",
      "     |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      "     |      keyConverter : str, optional\n",
      "     |          fully qualified name of a function returning key WritableConverter\n",
      "     |          None by default\n",
      "     |      valueConverter : str, optional\n",
      "     |          fully qualified name of a function returning value WritableConverter\n",
      "     |          None by default\n",
      "     |      conf : dict, optional\n",
      "     |          Hadoop configuration, passed in as a dict\n",
      "     |          None by default\n",
      "     |      batchSize : int, optional, default 0\n",
      "     |          The number of Python objects represented as a single\n",
      "     |          Java object. (default 0, choose batchSize automatically)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          RDD of tuples of key and corresponding value\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.saveAsSequenceFile`\n",
      "     |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
      "     |      :meth:`RDD.saveAsHadoopFile`\n",
      "     |      :meth:`SparkContext.sequenceFile`\n",
      "     |      :meth:`SparkContext.hadoopFile`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      \n",
      "     |      Set the related classes\n",
      "     |      \n",
      "     |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n",
      "     |      >>> input_format_class = \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\"\n",
      "     |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
      "     |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     path = os.path.join(d, \"new_hadoop_file\")\n",
      "     |      ...\n",
      "     |      ...     # Write a temporary Hadoop file\n",
      "     |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      "     |      ...     rdd.saveAsNewAPIHadoopFile(path, output_format_class, key_class, value_class)\n",
      "     |      ...\n",
      "     |      ...     loaded = sc.newAPIHadoopFile(path, input_format_class, key_class, value_class)\n",
      "     |      ...     collected = sorted(loaded.collect())\n",
      "     |      \n",
      "     |      >>> collected\n",
      "     |      [(1, ''), (1, 'a'), (3, 'x')]\n",
      "     |  \n",
      "     |  newAPIHadoopRDD(self, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n",
      "     |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
      "     |      Hadoop configuration, which is passed in as a Python dict.\n",
      "     |      This will be converted into a Configuration in Java.\n",
      "     |      The mechanism is the same as for meth:`SparkContext.sequenceFile`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      inputFormatClass : str\n",
      "     |          fully qualified classname of Hadoop InputFormat\n",
      "     |          (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      "     |      keyClass : str\n",
      "     |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n",
      "     |      valueClass : str\n",
      "     |          fully qualified classname of value Writable class\n",
      "     |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      "     |      keyConverter : str, optional\n",
      "     |          fully qualified name of a function returning key WritableConverter\n",
      "     |          (None by default)\n",
      "     |      valueConverter : str, optional\n",
      "     |          fully qualified name of a function returning value WritableConverter\n",
      "     |          (None by default)\n",
      "     |      conf : dict, optional\n",
      "     |          Hadoop configuration, passed in as a dict (None by default)\n",
      "     |      batchSize : int, optional, default 0\n",
      "     |          The number of Python objects represented as a single\n",
      "     |          Java object. (default 0, choose batchSize automatically)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          RDD of tuples of key and corresponding value\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n",
      "     |      :meth:`RDD.saveAsHadoopDataset`\n",
      "     |      :meth:`SparkContext.hadoopRDD`\n",
      "     |      :meth:`SparkContext.hadoopFile`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      \n",
      "     |      Set the related classes\n",
      "     |      \n",
      "     |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n",
      "     |      >>> input_format_class = \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\"\n",
      "     |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
      "     |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     path = os.path.join(d, \"new_hadoop_file\")\n",
      "     |      ...\n",
      "     |      ...     # Create the conf for writing\n",
      "     |      ...     write_conf = {\n",
      "     |      ...         \"mapreduce.job.outputformat.class\": (output_format_class),\n",
      "     |      ...         \"mapreduce.job.output.key.class\": key_class,\n",
      "     |      ...         \"mapreduce.job.output.value.class\": value_class,\n",
      "     |      ...         \"mapreduce.output.fileoutputformat.outputdir\": path,\n",
      "     |      ...     }\n",
      "     |      ...\n",
      "     |      ...     # Write a temporary Hadoop file\n",
      "     |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      "     |      ...     rdd.saveAsNewAPIHadoopDataset(conf=write_conf)\n",
      "     |      ...\n",
      "     |      ...     # Create the conf for reading\n",
      "     |      ...     read_conf = {\"mapreduce.input.fileinputformat.inputdir\": path}\n",
      "     |      ...\n",
      "     |      ...     loaded = sc.newAPIHadoopRDD(input_format_class,\n",
      "     |      ...         key_class, value_class, conf=read_conf)\n",
      "     |      ...     collected = sorted(loaded.collect())\n",
      "     |      \n",
      "     |      >>> collected\n",
      "     |      [(1, ''), (1, 'a'), (3, 'x')]\n",
      "     |  \n",
      "     |  parallelize(self, c: Iterable[~T], numSlices: Optional[int] = None) -> pyspark.rdd.RDD[~T]\n",
      "     |      Distribute a local Python collection to form an RDD. Using range\n",
      "     |      is recommended if the input represents a range for performance.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      c : :class:`collections.abc.Iterable`\n",
      "     |          iterable collection to distribute\n",
      "     |      numSlices : int, optional\n",
      "     |          the number of partitions of the new RDD\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          RDD representing distributed collection.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
      "     |      [[0], [2], [3], [4], [6]]\n",
      "     |      >>> sc.parallelize(range(0, 6, 2), 5).glom().collect()\n",
      "     |      [[], [0], [], [2], [4]]\n",
      "     |      \n",
      "     |      Deal with a list of strings.\n",
      "     |      \n",
      "     |      >>> strings = [\"a\", \"b\", \"c\"]\n",
      "     |      >>> sc.parallelize(strings, 2).glom().collect()\n",
      "     |      [['a'], ['b', 'c']]\n",
      "     |  \n",
      "     |  pickleFile(self, name: str, minPartitions: Optional[int] = None) -> pyspark.rdd.RDD[typing.Any]\n",
      "     |      Load an RDD previously saved using :meth:`RDD.saveAsPickleFile` method.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          directory to the input data files, the path can be comma separated\n",
      "     |          paths as a list of inputs\n",
      "     |      minPartitions : int, optional\n",
      "     |          suggested minimum number of partitions for the resulting RDD\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          RDD representing unpickled data from the file(s).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.saveAsPickleFile`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a temporary pickled file\n",
      "     |      ...     path1 = os.path.join(d, \"pickled1\")\n",
      "     |      ...     sc.parallelize(range(10)).saveAsPickleFile(path1, 3)\n",
      "     |      ...\n",
      "     |      ...     # Write another temporary pickled file\n",
      "     |      ...     path2 = os.path.join(d, \"pickled2\")\n",
      "     |      ...     sc.parallelize(range(-10, -5)).saveAsPickleFile(path2, 3)\n",
      "     |      ...\n",
      "     |      ...     # Load picked file\n",
      "     |      ...     collected1 = sorted(sc.pickleFile(path1, 3).collect())\n",
      "     |      ...     collected2 = sorted(sc.pickleFile(path2, 4).collect())\n",
      "     |      ...\n",
      "     |      ...     # Load two picked files together\n",
      "     |      ...     collected3 = sorted(sc.pickleFile('{},{}'.format(path1, path2), 5).collect())\n",
      "     |      \n",
      "     |      >>> collected1\n",
      "     |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "     |      >>> collected2\n",
      "     |      [-10, -9, -8, -7, -6]\n",
      "     |      >>> collected3\n",
      "     |      [-10, -9, -8, -7, -6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "     |  \n",
      "     |  range(self, start: int, end: Optional[int] = None, step: int = 1, numSlices: Optional[int] = None) -> pyspark.rdd.RDD[int]\n",
      "     |      Create a new RDD of int containing elements from `start` to `end`\n",
      "     |      (exclusive), increased by `step` every element. Can be called the same\n",
      "     |      way as python's built-in range() function. If called with a single argument,\n",
      "     |      the argument is interpreted as `end`, and `start` is set to 0.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      start : int\n",
      "     |          the start value\n",
      "     |      end : int, optional\n",
      "     |          the end value (exclusive)\n",
      "     |      step : int, optional, default 1\n",
      "     |          the incremental step\n",
      "     |      numSlices : int, optional\n",
      "     |          the number of partitions of the new RDD\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          An RDD of int\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`pyspark.sql.SparkSession.range`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.range(5).collect()\n",
      "     |      [0, 1, 2, 3, 4]\n",
      "     |      >>> sc.range(2, 4).collect()\n",
      "     |      [2, 3]\n",
      "     |      >>> sc.range(1, 7, 2).collect()\n",
      "     |      [1, 3, 5]\n",
      "     |      \n",
      "     |      Generate RDD with a negative step\n",
      "     |      \n",
      "     |      >>> sc.range(5, 0, -1).collect()\n",
      "     |      [5, 4, 3, 2, 1]\n",
      "     |      >>> sc.range(0, 5, -1).collect()\n",
      "     |      []\n",
      "     |      \n",
      "     |      Control the number of partitions\n",
      "     |      \n",
      "     |      >>> sc.range(5, numSlices=1).getNumPartitions()\n",
      "     |      1\n",
      "     |      >>> sc.range(5, numSlices=10).getNumPartitions()\n",
      "     |      10\n",
      "     |  \n",
      "     |  removeJobTag(self, tag: str) -> None\n",
      "     |      Remove a tag previously added to be assigned to all the jobs started by this thread.\n",
      "     |      Noop if such a tag was not added earlier.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tag : str\n",
      "     |          The tag to be removed. Cannot contain ',' (comma) character.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.addJobTag`\n",
      "     |      :meth:`SparkContext.getJobTags`\n",
      "     |      :meth:`SparkContext.clearJobTags`\n",
      "     |      :meth:`SparkContext.cancelJobsWithTag`\n",
      "     |      :meth:`SparkContext.setInterruptOnCancel`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.addJobTag(\"job_to_cancel1\")\n",
      "     |      >>> sc.addJobTag(\"job_to_cancel2\")\n",
      "     |      >>> sc.getJobTags()\n",
      "     |      {'job_to_cancel1', 'job_to_cancel2'}\n",
      "     |      >>> sc.removeJobTag(\"job_to_cancel1\")\n",
      "     |      >>> sc.getJobTags()\n",
      "     |      {'job_to_cancel2'}\n",
      "     |      >>> sc.clearJobTags()\n",
      "     |  \n",
      "     |  runJob(self, rdd: pyspark.rdd.RDD[~T], partitionFunc: Callable[[Iterable[~T]], Iterable[~U]], partitions: Optional[Sequence[int]] = None, allowLocal: bool = False) -> List[~U]\n",
      "     |      Executes the given partitionFunc on the specified set of partitions,\n",
      "     |      returning the result as an array of elements.\n",
      "     |      \n",
      "     |      If 'partitions' is not specified, this will run over all partitions.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      rdd : :class:`RDD`\n",
      "     |          target RDD to run tasks on\n",
      "     |      partitionFunc : function\n",
      "     |          a function to run on each partition of the RDD\n",
      "     |      partitions : list, optional\n",
      "     |          set of partitions to run on; some jobs may not want to compute on all\n",
      "     |          partitions of the target RDD, e.g. for operations like `first`\n",
      "     |      allowLocal : bool, default False\n",
      "     |          this parameter takes no effect\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          results of specified partitions\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.cancelAllJobs`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> myRDD = sc.parallelize(range(6), 3)\n",
      "     |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part])\n",
      "     |      [0, 1, 4, 9, 16, 25]\n",
      "     |      \n",
      "     |      >>> myRDD = sc.parallelize(range(6), 3)\n",
      "     |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)\n",
      "     |      [0, 1, 16, 25]\n",
      "     |  \n",
      "     |  sequenceFile(self, path: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, minSplits: Optional[int] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n",
      "     |      Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,\n",
      "     |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      "     |      The mechanism is as follows:\n",
      "     |      \n",
      "     |          1. A Java RDD is created from the SequenceFile or other InputFormat, and the key\n",
      "     |             and value Writable classes\n",
      "     |          2. Serialization is attempted via Pickle pickling\n",
      "     |          3. If this fails, the fallback is to call 'toString' on each key and value\n",
      "     |          4. :class:`CPickleSerializer` is used to deserialize pickled objects on the Python side\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          path to sequencefile\n",
      "     |      keyClass: str, optional\n",
      "     |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n",
      "     |      valueClass : str, optional\n",
      "     |          fully qualified classname of value Writable class\n",
      "     |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      "     |      keyConverter : str, optional\n",
      "     |          fully qualified name of a function returning key WritableConverter\n",
      "     |      valueConverter : str, optional\n",
      "     |          fully qualifiedname of a function returning value WritableConverter\n",
      "     |      minSplits : int, optional\n",
      "     |          minimum splits in dataset (default min(2, sc.defaultParallelism))\n",
      "     |      batchSize : int, optional, default 0\n",
      "     |          The number of Python objects represented as a single\n",
      "     |          Java object. (default 0, choose batchSize automatically)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          RDD of tuples of key and corresponding value\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.saveAsSequenceFile`\n",
      "     |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
      "     |      :meth:`RDD.saveAsHadoopFile`\n",
      "     |      :meth:`SparkContext.newAPIHadoopFile`\n",
      "     |      :meth:`SparkContext.hadoopFile`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      \n",
      "     |      Set the class of output format\n",
      "     |      \n",
      "     |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     path = os.path.join(d, \"hadoop_file\")\n",
      "     |      ...\n",
      "     |      ...     # Write a temporary Hadoop file\n",
      "     |      ...     rdd = sc.parallelize([(1, {3.0: \"bb\"}), (2, {1.0: \"aa\"}), (3, {2.0: \"dd\"})])\n",
      "     |      ...     rdd.saveAsNewAPIHadoopFile(path, output_format_class)\n",
      "     |      ...\n",
      "     |      ...     collected = sorted(sc.sequenceFile(path).collect())\n",
      "     |      \n",
      "     |      >>> collected\n",
      "     |      [(1, {3.0: 'bb'}), (2, {1.0: 'aa'}), (3, {2.0: 'dd'})]\n",
      "     |  \n",
      "     |  setCheckpointDir(self, dirName: str) -> None\n",
      "     |      Set the directory under which RDDs are going to be checkpointed. The\n",
      "     |      directory must be an HDFS path if running on a cluster.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dirName : str\n",
      "     |          path to the directory where checkpoint files will be stored\n",
      "     |          (must be HDFS path if running in cluster)\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.getCheckpointDir`\n",
      "     |      :meth:`RDD.checkpoint`\n",
      "     |      :meth:`RDD.getCheckpointFile`\n",
      "     |  \n",
      "     |  setInterruptOnCancel(self, interruptOnCancel: bool) -> None\n",
      "     |      Set the behavior of job cancellation from jobs started in this thread.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      interruptOnCancel : bool\n",
      "     |          If true, then job cancellation will result in ``Thread.interrupt()``\n",
      "     |          being called on the job's executor threads. This is useful to help ensure that\n",
      "     |          the tasks are actually stopped in a timely manner, but is off by default due to\n",
      "     |          HDFS-1208, where HDFS may respond to ``Thread.interrupt()`` by marking nodes as dead.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.addJobTag`\n",
      "     |      :meth:`SparkContext.removeJobTag`\n",
      "     |      :meth:`SparkContext.cancelAllJobs`\n",
      "     |      :meth:`SparkContext.cancelJobGroup`\n",
      "     |      :meth:`SparkContext.cancelJobsWithTag`\n",
      "     |  \n",
      "     |  setJobDescription(self, value: str) -> None\n",
      "     |      Set a human readable description of the current job.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      value : str\n",
      "     |          The job description to set.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If you run jobs in parallel, use :class:`pyspark.InheritableThread` for thread\n",
      "     |      local inheritance.\n",
      "     |  \n",
      "     |  setJobGroup(self, groupId: str, description: str, interruptOnCancel: bool = False) -> None\n",
      "     |      Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n",
      "     |      different value or cleared.\n",
      "     |      \n",
      "     |      Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n",
      "     |      Application programmers can use this method to group all those jobs together and give a\n",
      "     |      group description. Once set, the Spark web UI will associate such jobs with this group.\n",
      "     |      \n",
      "     |      The application can use :meth:`SparkContext.cancelJobGroup` to cancel all\n",
      "     |      running jobs in this group.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      groupId : str\n",
      "     |          The group ID to assign.\n",
      "     |      description : str\n",
      "     |          The description to set for the job group.\n",
      "     |      interruptOnCancel : bool, optional, default False\n",
      "     |          whether to interrupt jobs on job cancellation.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If interruptOnCancel is set to true for the job group, then job cancellation will result\n",
      "     |      in Thread.interrupt() being called on the job's executor threads. This is useful to help\n",
      "     |      ensure that the tasks are actually stopped in a timely manner, but is off by default due\n",
      "     |      to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n",
      "     |      \n",
      "     |      If you run jobs in parallel, use :class:`pyspark.InheritableThread` for thread\n",
      "     |      local inheritance.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.cancelJobGroup`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import threading\n",
      "     |      >>> from time import sleep\n",
      "     |      >>> from pyspark import InheritableThread\n",
      "     |      >>> result = \"Not Set\"\n",
      "     |      >>> lock = threading.Lock()\n",
      "     |      >>> def map_func(x):\n",
      "     |      ...     sleep(100)\n",
      "     |      ...     raise RuntimeError(\"Task should have been cancelled\")\n",
      "     |      ...\n",
      "     |      >>> def start_job(x):\n",
      "     |      ...     global result\n",
      "     |      ...     try:\n",
      "     |      ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n",
      "     |      ...         result = sc.parallelize(range(x)).map(map_func).collect()\n",
      "     |      ...     except Exception as e:\n",
      "     |      ...         result = \"Cancelled\"\n",
      "     |      ...     lock.release()\n",
      "     |      ...\n",
      "     |      >>> def stop_job():\n",
      "     |      ...     sleep(5)\n",
      "     |      ...     sc.cancelJobGroup(\"job_to_cancel\")\n",
      "     |      ...\n",
      "     |      >>> suppress = lock.acquire()\n",
      "     |      >>> suppress = InheritableThread(target=start_job, args=(10,)).start()\n",
      "     |      >>> suppress = InheritableThread(target=stop_job).start()\n",
      "     |      >>> suppress = lock.acquire()\n",
      "     |      >>> print(result)\n",
      "     |      Cancelled\n",
      "     |  \n",
      "     |  setLocalProperty(self, key: str, value: str) -> None\n",
      "     |      Set a local property that affects jobs submitted from this thread, such as the\n",
      "     |      Spark fair scheduler pool.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key : str\n",
      "     |          The key of the local property to set.\n",
      "     |      value : str\n",
      "     |          The value of the local property to set.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.getLocalProperty`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If you run jobs in parallel, use :class:`pyspark.InheritableThread` for thread\n",
      "     |      local inheritance.\n",
      "     |  \n",
      "     |  setLogLevel(self, logLevel: str) -> None\n",
      "     |      Control our logLevel. This overrides any user-defined log settings.\n",
      "     |      Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      logLevel : str\n",
      "     |          The desired log level as a string.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.setLogLevel(\"WARN\")  # doctest :+SKIP\n",
      "     |  \n",
      "     |  show_profiles(self) -> None\n",
      "     |      Print the profile stats to stdout\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2.0\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.dump_profiles`\n",
      "     |  \n",
      "     |  sparkUser(self) -> str\n",
      "     |      Get SPARK_USER for user who is running SparkContext.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |  \n",
      "     |  statusTracker(self) -> pyspark.status.StatusTracker\n",
      "     |      Return :class:`StatusTracker` object\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  stop(self) -> None\n",
      "     |      Shut down the :class:`SparkContext`.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |  \n",
      "     |  textFile(self, name: str, minPartitions: Optional[int] = None, use_unicode: bool = True) -> pyspark.rdd.RDD[str]\n",
      "     |      Read a text file from HDFS, a local file system (available on all\n",
      "     |      nodes), or any Hadoop-supported file system URI, and return it as an\n",
      "     |      RDD of Strings. The text files must be encoded as UTF-8.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          directory to the input data files, the path can be comma separated\n",
      "     |          paths as a list of inputs\n",
      "     |      minPartitions : int, optional\n",
      "     |          suggested minimum number of partitions for the resulting RDD\n",
      "     |      use_unicode : bool, default True\n",
      "     |          If `use_unicode` is False, the strings will be kept as `str` (encoding\n",
      "     |          as `utf-8`), which is faster and smaller than unicode.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.2.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          RDD representing text data from the file(s).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.saveAsTextFile`\n",
      "     |      :meth:`SparkContext.wholeTextFiles`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     path1 = os.path.join(d, \"text1\")\n",
      "     |      ...     path2 = os.path.join(d, \"text2\")\n",
      "     |      ...\n",
      "     |      ...     # Write a temporary text file\n",
      "     |      ...     sc.parallelize([\"x\", \"y\", \"z\"]).saveAsTextFile(path1)\n",
      "     |      ...\n",
      "     |      ...     # Write another temporary text file\n",
      "     |      ...     sc.parallelize([\"aa\", \"bb\", \"cc\"]).saveAsTextFile(path2)\n",
      "     |      ...\n",
      "     |      ...     # Load text file\n",
      "     |      ...     collected1 = sorted(sc.textFile(path1, 3).collect())\n",
      "     |      ...     collected2 = sorted(sc.textFile(path2, 4).collect())\n",
      "     |      ...\n",
      "     |      ...     # Load two text files together\n",
      "     |      ...     collected3 = sorted(sc.textFile('{},{}'.format(path1, path2), 5).collect())\n",
      "     |      \n",
      "     |      >>> collected1\n",
      "     |      ['x', 'y', 'z']\n",
      "     |      >>> collected2\n",
      "     |      ['aa', 'bb', 'cc']\n",
      "     |      >>> collected3\n",
      "     |      ['aa', 'bb', 'cc', 'x', 'y', 'z']\n",
      "     |  \n",
      "     |  union(self, rdds: List[pyspark.rdd.RDD[~T]]) -> pyspark.rdd.RDD[~T]\n",
      "     |      Build the union of a list of RDDs.\n",
      "     |      \n",
      "     |      This supports unions() of RDDs with different serialized formats,\n",
      "     |      although this forces them to be reserialized using the default\n",
      "     |      serializer:\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.union`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # generate a text RDD\n",
      "     |      ...     with open(os.path.join(d, \"union-text.txt\"), \"w\") as f:\n",
      "     |      ...         _ = f.write(\"Hello\")\n",
      "     |      ...     text_rdd = sc.textFile(d)\n",
      "     |      ...\n",
      "     |      ...     # generate another RDD\n",
      "     |      ...     parallelized = sc.parallelize([\"World!\"])\n",
      "     |      ...\n",
      "     |      ...     unioned = sorted(sc.union([text_rdd, parallelized]).collect())\n",
      "     |      \n",
      "     |      >>> unioned\n",
      "     |      ['Hello', 'World!']\n",
      "     |  \n",
      "     |  wholeTextFiles(self, path: str, minPartitions: Optional[int] = None, use_unicode: bool = True) -> pyspark.rdd.RDD[typing.Tuple[str, str]]\n",
      "     |      Read a directory of text files from HDFS, a local file system\n",
      "     |      (available on all nodes), or any  Hadoop-supported file system\n",
      "     |      URI. Each file is read as a single record and returned in a\n",
      "     |      key-value pair, where the key is the path of each file, the\n",
      "     |      value is the content of each file.\n",
      "     |      The text files must be encoded as UTF-8.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      For example, if you have the following files:\n",
      "     |      \n",
      "     |      .. code-block:: text\n",
      "     |      \n",
      "     |          hdfs://a-hdfs-path/part-00000\n",
      "     |          hdfs://a-hdfs-path/part-00001\n",
      "     |          ...\n",
      "     |          hdfs://a-hdfs-path/part-nnnnn\n",
      "     |      \n",
      "     |      Do ``rdd = sparkContext.wholeTextFiles(\"hdfs://a-hdfs-path\")``,\n",
      "     |      then ``rdd`` contains:\n",
      "     |      \n",
      "     |      .. code-block:: text\n",
      "     |      \n",
      "     |          (a-hdfs-path/part-00000, its content)\n",
      "     |          (a-hdfs-path/part-00001, its content)\n",
      "     |          ...\n",
      "     |          (a-hdfs-path/part-nnnnn, its content)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          directory to the input data files, the path can be comma separated\n",
      "     |          paths as a list of inputs\n",
      "     |      minPartitions : int, optional\n",
      "     |          suggested minimum number of partitions for the resulting RDD\n",
      "     |      use_unicode : bool, default True\n",
      "     |          If `use_unicode` is False, the strings will be kept as `str` (encoding\n",
      "     |          as `utf-8`), which is faster and smaller than unicode.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.2.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |          RDD representing path-content pairs from the file(s).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Small files are preferred, as each file will be loaded fully in memory.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`RDD.saveAsTextFile`\n",
      "     |      :meth:`SparkContext.textFile`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a temporary text file\n",
      "     |      ...     with open(os.path.join(d, \"1.txt\"), \"w\") as f:\n",
      "     |      ...         _ = f.write(\"123\")\n",
      "     |      ...\n",
      "     |      ...     # Write another temporary text file\n",
      "     |      ...     with open(os.path.join(d, \"2.txt\"), \"w\") as f:\n",
      "     |      ...         _ = f.write(\"xyz\")\n",
      "     |      ...\n",
      "     |      ...     collected = sorted(sc.wholeTextFiles(d).collect())\n",
      "     |      >>> collected\n",
      "     |      [('.../1.txt', '123'), ('.../2.txt', 'xyz')]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  getOrCreate(conf: Optional[pyspark.conf.SparkConf] = None) -> 'SparkContext' from builtins.type\n",
      "     |      Get or instantiate a :class:`SparkContext` and register it as a singleton object.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      conf : :class:`SparkConf`, optional\n",
      "     |          :class:`SparkConf` that will be used for initialization of the :class:`SparkContext`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`SparkContext`\n",
      "     |          current :class:`SparkContext`, or a new one if it wasn't created before the function\n",
      "     |          call.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> SparkContext.getOrCreate()\n",
      "     |      <SparkContext ...>\n",
      "     |  \n",
      "     |  setSystemProperty(key: str, value: str) -> None from builtins.type\n",
      "     |      Set a Java system property, such as `spark.executor.memory`. This must\n",
      "     |      be invoked before instantiating :class:`SparkContext`.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.9.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key : str\n",
      "     |          The key of a new Java system property.\n",
      "     |      value : str\n",
      "     |          The value of a new Java system property.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  applicationId\n",
      "     |      A unique identifier for the Spark application.\n",
      "     |      Its format depends on the scheduler implementation.\n",
      "     |      \n",
      "     |      * in case of local spark app something like 'local-1433865536131'\n",
      "     |      * in case of YARN something like 'application_1433865536131_34483'\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.applicationId  # doctest: +ELLIPSIS\n",
      "     |      'local-...'\n",
      "     |  \n",
      "     |  defaultMinPartitions\n",
      "     |      Default min number of partitions for Hadoop RDDs when not given by user\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.defaultMinPartitions > 0\n",
      "     |      True\n",
      "     |  \n",
      "     |  defaultParallelism\n",
      "     |      Default level of parallelism to use when not given by user (e.g. for reduce tasks)\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.defaultParallelism > 0\n",
      "     |      True\n",
      "     |  \n",
      "     |  listArchives\n",
      "     |      Returns a list of archive paths that are added to resources.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.addArchive`\n",
      "     |  \n",
      "     |  listFiles\n",
      "     |      Returns a list of file paths that are added to resources.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkContext.addFile`\n",
      "     |  \n",
      "     |  resources\n",
      "     |      Return the resource information of this :class:`SparkContext`.\n",
      "     |      A resource could be a GPU, FPGA, etc.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |  \n",
      "     |  startTime\n",
      "     |      Return the epoch time when the :class:`SparkContext` was started.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = sc.startTime\n",
      "     |  \n",
      "     |  uiWebUrl\n",
      "     |      Return the URL of the SparkUI instance started by this :class:`SparkContext`\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      When the web ui is disabled, e.g., by ``spark.ui.enabled`` set to ``False``,\n",
      "     |      it returns ``None``.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sc.uiWebUrl\n",
      "     |      'http://...'\n",
      "     |  \n",
      "     |  version\n",
      "     |      The version of Spark on which this application is running.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = sc.version\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  PACKAGE_EXTENSIONS = ('.zip', '.egg', '.jar')\n",
      "     |  \n",
      "     |  __annotations__ = {'PACKAGE_EXTENSIONS': typing.Iterable[str], '_activ...\n",
      "    \n",
      "    class SparkFiles(builtins.object)\n",
      "     |  SparkFiles() -> None\n",
      "     |  \n",
      "     |  Resolves paths to files added through :meth:`SparkContext.addFile`.\n",
      "     |  \n",
      "     |  SparkFiles contains only classmethods; users should not create SparkFiles\n",
      "     |  instances.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  get(filename: str) -> str from builtins.type\n",
      "     |      Get the absolute path of a file added through\n",
      "     |      :meth:`SparkContext.addFile` or :meth:`SparkContext.addPyFile`.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      filename : str\n",
      "     |          file that are added to resources\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      str\n",
      "     |          the absolute path of the file\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkFiles.getRootDirectory`\n",
      "     |      :meth:`SparkContext.addFile`\n",
      "     |      :meth:`SparkContext.addPyFile`\n",
      "     |      :meth:`SparkContext.listFiles`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import os\n",
      "     |      >>> import tempfile\n",
      "     |      >>> from pyspark import SparkFiles\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     path1 = os.path.join(d, \"test.txt\")\n",
      "     |      ...     with open(path1, \"w\") as f:\n",
      "     |      ...         _ = f.write(\"100\")\n",
      "     |      ...\n",
      "     |      ...     sc.addFile(path1)\n",
      "     |      ...     file_list1 = sorted(sc.listFiles)\n",
      "     |      ...\n",
      "     |      ...     def func1(iterator):\n",
      "     |      ...         path = SparkFiles.get(\"test.txt\")\n",
      "     |      ...         assert path.startswith(SparkFiles.getRootDirectory())\n",
      "     |      ...         return [path]\n",
      "     |      ...\n",
      "     |      ...     path_list1 = sc.parallelize([1, 2, 3, 4]).mapPartitions(func1).collect()\n",
      "     |      ...\n",
      "     |      ...     path2 = os.path.join(d, \"test.py\")\n",
      "     |      ...     with open(path2, \"w\") as f:\n",
      "     |      ...         _ = f.write(\"import pyspark\")\n",
      "     |      ...\n",
      "     |      ...     # py files\n",
      "     |      ...     sc.addPyFile(path2)\n",
      "     |      ...     file_list2 = sorted(sc.listFiles)\n",
      "     |      ...\n",
      "     |      ...     def func2(iterator):\n",
      "     |      ...         path = SparkFiles.get(\"test.py\")\n",
      "     |      ...         assert path.startswith(SparkFiles.getRootDirectory())\n",
      "     |      ...         return [path]\n",
      "     |      ...\n",
      "     |      ...     path_list2 = sc.parallelize([1, 2, 3, 4]).mapPartitions(func2).collect()\n",
      "     |      >>> file_list1\n",
      "     |      ['file:/.../test.txt']\n",
      "     |      >>> set(path_list1)\n",
      "     |      {'.../test.txt'}\n",
      "     |      >>> file_list2\n",
      "     |      ['file:/.../test.py', 'file:/.../test.txt']\n",
      "     |      >>> set(path_list2)\n",
      "     |      {'.../test.py'}\n",
      "     |  \n",
      "     |  getRootDirectory() -> str from builtins.type\n",
      "     |      Get the root directory that contains files added through\n",
      "     |      :meth:`SparkContext.addFile` or :meth:`SparkContext.addPyFile`.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.7.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      str\n",
      "     |          the root directory that contains files added to resources\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`SparkFiles.get`\n",
      "     |      :meth:`SparkContext.addFile`\n",
      "     |      :meth:`SparkContext.addPyFile`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.files import SparkFiles\n",
      "     |      >>> SparkFiles.getRootDirectory()  # doctest: +SKIP\n",
      "     |      '.../spark-a904728e-08d3-400c-a872-cfd82fd6dcd2/userFiles-648cf6d6-bb2c-4f53-82bd-e658aba0c5de'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'_is_running_on_worker': typing.ClassVar[bool], '_r...\n",
      "    \n",
      "    class SparkJobInfo(builtins.tuple)\n",
      "     |  SparkJobInfo(jobId: int, stageIds: py4j.java_collections.JavaArray, status: str)\n",
      "     |  \n",
      "     |  Exposes information about Spark Jobs.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SparkJobInfo\n",
      "     |      builtins.tuple\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getnewargs__(self)\n",
      "     |      Return self as a plain tuple.  Used by copy and pickle.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return a nicely formatted representation string\n",
      "     |  \n",
      "     |  _asdict(self)\n",
      "     |      Return a new dict which maps field names to their values.\n",
      "     |  \n",
      "     |  _replace(self, /, **kwds)\n",
      "     |      Return a new SparkJobInfo object replacing specified fields with new values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  _make(iterable) from builtins.type\n",
      "     |      Make a new SparkJobInfo object from a sequence or iterable\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(_cls, jobId: int, stageIds: py4j.java_collections.JavaArray, status: str)\n",
      "     |      Create new instance of SparkJobInfo(jobId, stageIds, status)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  jobId\n",
      "     |      Alias for field number 0\n",
      "     |  \n",
      "     |  stageIds\n",
      "     |      Alias for field number 1\n",
      "     |  \n",
      "     |  status\n",
      "     |      Alias for field number 2\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'jobId': <class 'int'>, 'stageIds': <class 'py4j.ja...\n",
      "     |  \n",
      "     |  __match_args__ = ('jobId', 'stageIds', 'status')\n",
      "     |  \n",
      "     |  __orig_bases__ = (<function NamedTuple>,)\n",
      "     |  \n",
      "     |  _field_defaults = {}\n",
      "     |  \n",
      "     |  _fields = ('jobId', 'stageIds', 'status')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.tuple:\n",
      "     |  \n",
      "     |  __add__(self, value, /)\n",
      "     |      Return self+value.\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      Return key in self.\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __hash__(self, /)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __mul__(self, value, /)\n",
      "     |      Return self*value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __rmul__(self, value, /)\n",
      "     |      Return value*self.\n",
      "     |  \n",
      "     |  count(self, value, /)\n",
      "     |      Return number of occurrences of value.\n",
      "     |  \n",
      "     |  index(self, value, start=0, stop=9223372036854775807, /)\n",
      "     |      Return first index of value.\n",
      "     |      \n",
      "     |      Raises ValueError if the value is not present.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.tuple:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "    \n",
      "    class SparkStageInfo(builtins.tuple)\n",
      "     |  SparkStageInfo(stageId: int, currentAttemptId: int, name: str, numTasks: int, numActiveTasks: int, numCompletedTasks: int, numFailedTasks: int)\n",
      "     |  \n",
      "     |  Exposes information about Spark Stages.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SparkStageInfo\n",
      "     |      builtins.tuple\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getnewargs__(self)\n",
      "     |      Return self as a plain tuple.  Used by copy and pickle.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return a nicely formatted representation string\n",
      "     |  \n",
      "     |  _asdict(self)\n",
      "     |      Return a new dict which maps field names to their values.\n",
      "     |  \n",
      "     |  _replace(self, /, **kwds)\n",
      "     |      Return a new SparkStageInfo object replacing specified fields with new values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  _make(iterable) from builtins.type\n",
      "     |      Make a new SparkStageInfo object from a sequence or iterable\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(_cls, stageId: int, currentAttemptId: int, name: str, numTasks: int, numActiveTasks: int, numCompletedTasks: int, numFailedTasks: int)\n",
      "     |      Create new instance of SparkStageInfo(stageId, currentAttemptId, name, numTasks, numActiveTasks, numCompletedTasks, numFailedTasks)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  stageId\n",
      "     |      Alias for field number 0\n",
      "     |  \n",
      "     |  currentAttemptId\n",
      "     |      Alias for field number 1\n",
      "     |  \n",
      "     |  name\n",
      "     |      Alias for field number 2\n",
      "     |  \n",
      "     |  numTasks\n",
      "     |      Alias for field number 3\n",
      "     |  \n",
      "     |  numActiveTasks\n",
      "     |      Alias for field number 4\n",
      "     |  \n",
      "     |  numCompletedTasks\n",
      "     |      Alias for field number 5\n",
      "     |  \n",
      "     |  numFailedTasks\n",
      "     |      Alias for field number 6\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'currentAttemptId': <class 'int'>, 'name': <class '...\n",
      "     |  \n",
      "     |  __match_args__ = ('stageId', 'currentAttemptId', 'name', 'numTasks', '...\n",
      "     |  \n",
      "     |  __orig_bases__ = (<function NamedTuple>,)\n",
      "     |  \n",
      "     |  _field_defaults = {}\n",
      "     |  \n",
      "     |  _fields = ('stageId', 'currentAttemptId', 'name', 'numTasks', 'numActi...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.tuple:\n",
      "     |  \n",
      "     |  __add__(self, value, /)\n",
      "     |      Return self+value.\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      Return key in self.\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __hash__(self, /)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __mul__(self, value, /)\n",
      "     |      Return self*value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __rmul__(self, value, /)\n",
      "     |      Return value*self.\n",
      "     |  \n",
      "     |  count(self, value, /)\n",
      "     |      Return number of occurrences of value.\n",
      "     |  \n",
      "     |  index(self, value, start=0, stop=9223372036854775807, /)\n",
      "     |      Return first index of value.\n",
      "     |      \n",
      "     |      Raises ValueError if the value is not present.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.tuple:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "    \n",
      "    class StatusTracker(builtins.object)\n",
      "     |  StatusTracker(jtracker: py4j.java_gateway.JavaObject)\n",
      "     |  \n",
      "     |  Low-level status reporting APIs for monitoring job and stage progress.\n",
      "     |  \n",
      "     |  These APIs intentionally provide very weak consistency semantics;\n",
      "     |  consumers of these APIs should be prepared to handle empty / missing\n",
      "     |  information. For example, a job's stage ids may be known but the status\n",
      "     |  API may not have any information about the details of those stages, so\n",
      "     |  `getStageInfo` could potentially return `None` for a valid stage id.\n",
      "     |  \n",
      "     |  To limit memory usage, these APIs only provide information on recent\n",
      "     |  jobs / stages.  These APIs will provide information for the last\n",
      "     |  `spark.ui.retainedStages` stages and `spark.ui.retainedJobs` jobs.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, jtracker: py4j.java_gateway.JavaObject)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  getActiveJobsIds(self) -> List[int]\n",
      "     |      Returns an array containing the ids of all active jobs.\n",
      "     |  \n",
      "     |  getActiveStageIds(self) -> List[int]\n",
      "     |      Returns an array containing the ids of all active stages.\n",
      "     |  \n",
      "     |  getJobIdsForGroup(self, jobGroup: Optional[str] = None) -> List[int]\n",
      "     |      Return a list of all known jobs in a particular job group.  If\n",
      "     |      `jobGroup` is None, then returns all known jobs that are not\n",
      "     |      associated with a job group.\n",
      "     |      \n",
      "     |      The returned list may contain running, failed, and completed jobs,\n",
      "     |      and may vary across invocations of this method. This method does\n",
      "     |      not guarantee the order of the elements in its result.\n",
      "     |  \n",
      "     |  getJobInfo(self, jobId: int) -> Optional[pyspark.status.SparkJobInfo]\n",
      "     |      Returns a :class:`SparkJobInfo` object, or None if the job info\n",
      "     |      could not be found or was garbage collected.\n",
      "     |  \n",
      "     |  getStageInfo(self, stageId: int) -> Optional[pyspark.status.SparkStageInfo]\n",
      "     |      Returns a :class:`SparkStageInfo` object, or None if the stage\n",
      "     |      info could not be found or was garbage collected.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class StorageLevel(builtins.object)\n",
      "     |  StorageLevel(useDisk: bool, useMemory: bool, useOffHeap: bool, deserialized: bool, replication: int = 1)\n",
      "     |  \n",
      "     |  Flags for controlling the storage of an RDD. Each StorageLevel records whether to use memory,\n",
      "     |  whether to drop the RDD to disk if it falls out of memory, whether to keep the data in memory\n",
      "     |  in a JAVA-specific serialized format, and whether to replicate the RDD partitions on multiple\n",
      "     |  nodes. Also contains static constants for some commonly used storage levels, MEMORY_ONLY.\n",
      "     |  Since the data is always serialized on the Python side, all the constants use the serialized\n",
      "     |  formats.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, useDisk: bool, useMemory: bool, useOffHeap: bool, deserialized: bool, replication: int = 1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __str__(self) -> str\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  DISK_ONLY = StorageLevel(True, False, False, False, 1)\n",
      "     |  \n",
      "     |  DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)\n",
      "     |  \n",
      "     |  DISK_ONLY_3 = StorageLevel(True, False, False, False, 3)\n",
      "     |  \n",
      "     |  MEMORY_AND_DISK = StorageLevel(True, True, False, False, 1)\n",
      "     |  \n",
      "     |  MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)\n",
      "     |  \n",
      "     |  MEMORY_AND_DISK_DESER = StorageLevel(True, True, False, True, 1)\n",
      "     |  \n",
      "     |  MEMORY_ONLY = StorageLevel(False, True, False, False, 1)\n",
      "     |  \n",
      "     |  MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)\n",
      "     |  \n",
      "     |  NONE = StorageLevel(False, False, False, False, 1)\n",
      "     |  \n",
      "     |  OFF_HEAP = StorageLevel(True, True, True, False, 1)\n",
      "     |  \n",
      "     |  __annotations__ = {'DISK_ONLY': typing.ClassVar[ForwardRef('StorageLev...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class TaskContext(builtins.object)\n",
      "     |  TaskContext() -> 'TaskContext'\n",
      "     |  \n",
      "     |  Contextual information about a task which can be read or mutated during\n",
      "     |  execution. To access the TaskContext for a running task, use:\n",
      "     |  :meth:`TaskContext.get`.\n",
      "     |  \n",
      "     |  .. versionadded:: 2.2.0\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from pyspark import TaskContext\n",
      "     |  \n",
      "     |  Get a task context instance from :class:`RDD`.\n",
      "     |  \n",
      "     |  >>> spark.sparkContext.setLocalProperty(\"key1\", \"value\")\n",
      "     |  >>> taskcontext = spark.sparkContext.parallelize([1]).map(lambda _: TaskContext.get()).first()\n",
      "     |  >>> isinstance(taskcontext.attemptNumber(), int)\n",
      "     |  True\n",
      "     |  >>> isinstance(taskcontext.partitionId(), int)\n",
      "     |  True\n",
      "     |  >>> isinstance(taskcontext.stageId(), int)\n",
      "     |  True\n",
      "     |  >>> isinstance(taskcontext.taskAttemptId(), int)\n",
      "     |  True\n",
      "     |  >>> taskcontext.getLocalProperty(\"key1\")\n",
      "     |  'value'\n",
      "     |  >>> isinstance(taskcontext.cpus(), int)\n",
      "     |  True\n",
      "     |  \n",
      "     |  Get a task context instance from a dataframe via Python UDF.\n",
      "     |  \n",
      "     |  >>> from pyspark.sql import Row\n",
      "     |  >>> from pyspark.sql.functions import udf\n",
      "     |  >>> @udf(\"STRUCT<anum: INT, partid: INT, stageid: INT, taskaid: INT, prop: STRING, cpus: INT>\")\n",
      "     |  ... def taskcontext_as_row():\n",
      "     |  ...    taskcontext = TaskContext.get()\n",
      "     |  ...    return Row(\n",
      "     |  ...        anum=taskcontext.attemptNumber(),\n",
      "     |  ...        partid=taskcontext.partitionId(),\n",
      "     |  ...        stageid=taskcontext.stageId(),\n",
      "     |  ...        taskaid=taskcontext.taskAttemptId(),\n",
      "     |  ...        prop=taskcontext.getLocalProperty(\"key2\"),\n",
      "     |  ...        cpus=taskcontext.cpus())\n",
      "     |  ...\n",
      "     |  >>> spark.sparkContext.setLocalProperty(\"key2\", \"value\")\n",
      "     |  >>> [(anum, partid, stageid, taskaid, prop, cpus)] = (\n",
      "     |  ...     spark.range(1).select(taskcontext_as_row()).first()\n",
      "     |  ... )\n",
      "     |  >>> isinstance(anum, int)\n",
      "     |  True\n",
      "     |  >>> isinstance(partid, int)\n",
      "     |  True\n",
      "     |  >>> isinstance(stageid, int)\n",
      "     |  True\n",
      "     |  >>> isinstance(taskaid, int)\n",
      "     |  True\n",
      "     |  >>> prop\n",
      "     |  'value'\n",
      "     |  >>> isinstance(cpus, int)\n",
      "     |  True\n",
      "     |  \n",
      "     |  Get a task context instance from a dataframe via Pandas UDF.\n",
      "     |  \n",
      "     |  >>> import pandas as pd  # doctest: +SKIP\n",
      "     |  >>> from pyspark.sql.functions import pandas_udf\n",
      "     |  >>> @pandas_udf(\"STRUCT<\"\n",
      "     |  ...     \"anum: INT, partid: INT, stageid: INT, taskaid: INT, prop: STRING, cpus: INT>\")\n",
      "     |  ... def taskcontext_as_row(_):\n",
      "     |  ...    taskcontext = TaskContext.get()\n",
      "     |  ...    return pd.DataFrame({\n",
      "     |  ...        \"anum\": [taskcontext.attemptNumber()],\n",
      "     |  ...        \"partid\": [taskcontext.partitionId()],\n",
      "     |  ...        \"stageid\": [taskcontext.stageId()],\n",
      "     |  ...        \"taskaid\": [taskcontext.taskAttemptId()],\n",
      "     |  ...        \"prop\": [taskcontext.getLocalProperty(\"key3\")],\n",
      "     |  ...        \"cpus\": [taskcontext.cpus()]\n",
      "     |  ...    })  # doctest: +SKIP\n",
      "     |  ...\n",
      "     |  >>> spark.sparkContext.setLocalProperty(\"key3\", \"value\")  # doctest: +SKIP\n",
      "     |  >>> [(anum, partid, stageid, taskaid, prop, cpus)] = (\n",
      "     |  ...     spark.range(1).select(taskcontext_as_row(\"id\")).first()\n",
      "     |  ... )  # doctest: +SKIP\n",
      "     |  >>> isinstance(anum, int)\n",
      "     |  True\n",
      "     |  >>> isinstance(partid, int)\n",
      "     |  True\n",
      "     |  >>> isinstance(stageid, int)\n",
      "     |  True\n",
      "     |  >>> isinstance(taskaid, int)\n",
      "     |  True\n",
      "     |  >>> prop\n",
      "     |  'value'\n",
      "     |  >>> isinstance(cpus, int)\n",
      "     |  True\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  attemptNumber(self) -> int\n",
      "     |      How many times this task has been attempted.  The first task attempt will be assigned\n",
      "     |      attemptNumber = 0, and subsequent attempts will have increasing attempt numbers.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          current attempt number.\n",
      "     |  \n",
      "     |  cpus(self) -> int\n",
      "     |      CPUs allocated to the task.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          the number of CPUs.\n",
      "     |  \n",
      "     |  getLocalProperty(self, key: str) -> Optional[str]\n",
      "     |      Get a local property set upstream in the driver, or None if it is missing.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key : str\n",
      "     |          the key of the local property to get.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          the value of the local property.\n",
      "     |  \n",
      "     |  partitionId(self) -> int\n",
      "     |      The ID of the RDD partition that is computed by this task.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          current partition id.\n",
      "     |  \n",
      "     |  resources(self) -> Dict[str, pyspark.resource.information.ResourceInformation]\n",
      "     |      Resources allocated to the task. The key is the resource name and the value is information\n",
      "     |      about the resource.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dict\n",
      "     |          a dictionary of a string resource name, and :class:`ResourceInformation`.\n",
      "     |  \n",
      "     |  stageId(self) -> int\n",
      "     |      The ID of the stage that this task belong to.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          current stage id.\n",
      "     |  \n",
      "     |  taskAttemptId(self) -> int\n",
      "     |      An ID that is unique to this task attempt (within the same :class:`SparkContext`,\n",
      "     |      no two task attempts will share the same attempt ID).  This is roughly equivalent\n",
      "     |      to Hadoop's `TaskAttemptID`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          current task attempt id.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  get() -> Optional[ForwardRef('TaskContext')] from builtins.type\n",
      "     |      Return the currently active :class:`TaskContext`. This can be called inside of\n",
      "     |      user functions to access contextual information about running tasks.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`TaskContext`, optional\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Must be called on the worker, not the driver. Returns ``None`` if not initialized.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(cls: Type[ForwardRef('TaskContext')]) -> 'TaskContext'\n",
      "     |      Even if users construct :class:`TaskContext` instead of using get, give them the singleton.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'_attemptNumber': typing.Optional[int], '_cpus': ty...\n",
      "\n",
      "FUNCTIONS\n",
      "    inheritable_thread_target(f: Callable) -> Callable\n",
      "        Return thread target wrapper which is recommended to be used in PySpark when the\n",
      "        pinned thread mode is enabled. The wrapper function, before calling original\n",
      "        thread target, it inherits the inheritable properties specific\n",
      "        to JVM thread such as ``InheritableThreadLocal``.\n",
      "        \n",
      "        Also, note that pinned thread mode does not close the connection from Python\n",
      "        to JVM when the thread is finished in the Python side. With this wrapper, Python\n",
      "        garbage-collects the Python thread instance and also closes the connection\n",
      "        which finishes JVM thread correctly.\n",
      "        \n",
      "        When the pinned thread mode is off, it return the original ``f``.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            the original thread target.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This API is experimental.\n",
      "        \n",
      "        It is important to know that it captures the local properties when you decorate it\n",
      "        whereas :class:`InheritableThread` captures when the thread is started.\n",
      "        Therefore, it is encouraged to decorate it when you want to capture the local\n",
      "        properties.\n",
      "        \n",
      "        For example, the local properties from the current Spark context is captured\n",
      "        when you define a function here instead of the invocation:\n",
      "        \n",
      "        >>> @inheritable_thread_target\n",
      "        ... def target_func():\n",
      "        ...     pass  # your codes.\n",
      "        \n",
      "        If you have any updates on local properties afterwards, it would not be reflected to\n",
      "        the Spark context in ``target_func()``.\n",
      "        \n",
      "        The example below mimics the behavior of JVM threads as close as possible:\n",
      "        \n",
      "        >>> Thread(target=inheritable_thread_target(target_func)).start()  # doctest: +SKIP\n",
      "\n",
      "DATA\n",
      "    __all__ = ['SparkConf', 'SparkContext', 'SparkFiles', 'RDD', 'StorageL...\n",
      "\n",
      "VERSION\n",
      "    3.5.0\n",
      "\n",
      "FILE\n",
      "    /usr/local/spark/python/pyspark/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "help(pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bbcc756-84f0-44f0-9901-153e2f620fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package pyspark.sql in pyspark:\n",
      "\n",
      "NAME\n",
      "    pyspark.sql - Important classes of Spark SQL and DataFrames:\n",
      "\n",
      "DESCRIPTION\n",
      "        - :class:`pyspark.sql.SparkSession`\n",
      "          Main entry point for :class:`DataFrame` and SQL functionality.\n",
      "        - :class:`pyspark.sql.DataFrame`\n",
      "          A distributed collection of data grouped into named columns.\n",
      "        - :class:`pyspark.sql.Column`\n",
      "          A column expression in a :class:`DataFrame`.\n",
      "        - :class:`pyspark.sql.Row`\n",
      "          A row of data in a :class:`DataFrame`.\n",
      "        - :class:`pyspark.sql.GroupedData`\n",
      "          Aggregation methods, returned by :func:`DataFrame.groupBy`.\n",
      "        - :class:`pyspark.sql.DataFrameNaFunctions`\n",
      "          Methods for handling missing data (null values).\n",
      "        - :class:`pyspark.sql.DataFrameStatFunctions`\n",
      "          Methods for statistics functionality.\n",
      "        - :class:`pyspark.sql.functions`\n",
      "          List of built-in functions available for :class:`DataFrame`.\n",
      "        - :class:`pyspark.sql.types`\n",
      "          List of data types available.\n",
      "        - :class:`pyspark.sql.Window`\n",
      "          For working with window functions.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    avro (package)\n",
      "    catalog\n",
      "    column\n",
      "    conf\n",
      "    connect (package)\n",
      "    context\n",
      "    dataframe\n",
      "    functions\n",
      "    group\n",
      "    observation\n",
      "    pandas (package)\n",
      "    protobuf (package)\n",
      "    readwriter\n",
      "    session\n",
      "    sql_formatter\n",
      "    streaming (package)\n",
      "    tests (package)\n",
      "    types\n",
      "    udf\n",
      "    udtf\n",
      "    utils\n",
      "    window\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        pyspark.sql.catalog.Catalog\n",
      "        pyspark.sql.column.Column\n",
      "        pyspark.sql.context.SQLContext\n",
      "            pyspark.sql.context.HiveContext\n",
      "        pyspark.sql.dataframe.DataFrameNaFunctions\n",
      "        pyspark.sql.dataframe.DataFrameStatFunctions\n",
      "        pyspark.sql.observation.Observation\n",
      "        pyspark.sql.pandas.group_ops.PandasCogroupedOps\n",
      "        pyspark.sql.readwriter.DataFrameWriterV2\n",
      "        pyspark.sql.udf.UDFRegistration\n",
      "        pyspark.sql.udtf.UDTFRegistration\n",
      "        pyspark.sql.window.Window\n",
      "        pyspark.sql.window.WindowSpec\n",
      "    builtins.tuple(builtins.object)\n",
      "        pyspark.sql.types.Row\n",
      "    pyspark.sql.pandas.conversion.PandasConversionMixin(builtins.object)\n",
      "        pyspark.sql.dataframe.DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
      "    pyspark.sql.pandas.conversion.SparkConversionMixin(builtins.object)\n",
      "        pyspark.sql.session.SparkSession\n",
      "    pyspark.sql.pandas.group_ops.PandasGroupedOpsMixin(builtins.object)\n",
      "        pyspark.sql.group.GroupedData\n",
      "    pyspark.sql.pandas.map_ops.PandasMapOpsMixin(builtins.object)\n",
      "        pyspark.sql.dataframe.DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
      "    pyspark.sql.readwriter.OptionUtils(builtins.object)\n",
      "        pyspark.sql.readwriter.DataFrameReader\n",
      "        pyspark.sql.readwriter.DataFrameWriter\n",
      "    \n",
      "    class Catalog(builtins.object)\n",
      "     |  Catalog(sparkSession: pyspark.sql.session.SparkSession) -> None\n",
      "     |  \n",
      "     |  User-facing catalog API, accessible through `SparkSession.catalog`.\n",
      "     |  \n",
      "     |  This is a thin wrapper around its Scala implementation org.apache.spark.sql.catalog.Catalog.\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sparkSession: pyspark.sql.session.SparkSession) -> None\n",
      "     |      Create a new Catalog that wraps the underlying JVM object.\n",
      "     |  \n",
      "     |  cacheTable(self, tableName: str, storageLevel: Optional[pyspark.storagelevel.StorageLevel] = None) -> None\n",
      "     |      Caches the specified table in-memory or with given storage level.\n",
      "     |      Default MEMORY_AND_DISK.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          name of the table to get.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |              Allow ``tableName`` to be qualified with catalog name.\n",
      "     |      \n",
      "     |      storageLevel : :class:`StorageLevel`\n",
      "     |          storage level to set for persistence.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.5.0\n",
      "     |              Allow to specify storage level.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      >>> _ = spark.sql(\"CREATE TABLE tbl1 (name STRING, age INT) USING parquet\")\n",
      "     |      >>> spark.catalog.cacheTable(\"tbl1\")\n",
      "     |      \n",
      "     |      or\n",
      "     |      \n",
      "     |      >>> spark.catalog.cacheTable(\"tbl1\", StorageLevel.OFF_HEAP)\n",
      "     |      \n",
      "     |      Throw an analysis exception when the table does not exist.\n",
      "     |      \n",
      "     |      >>> spark.catalog.cacheTable(\"not_existing_table\")\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      AnalysisException: ...\n",
      "     |      \n",
      "     |      Using the fully qualified name for the table.\n",
      "     |      \n",
      "     |      >>> spark.catalog.cacheTable(\"spark_catalog.default.tbl1\")\n",
      "     |      >>> spark.catalog.uncacheTable(\"tbl1\")\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |  \n",
      "     |  clearCache(self) -> None\n",
      "     |      Removes all cached tables from the in-memory cache.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      >>> _ = spark.sql(\"CREATE TABLE tbl1 (name STRING, age INT) USING parquet\")\n",
      "     |      >>> spark.catalog.clearCache()\n",
      "     |      >>> spark.catalog.isCached(\"tbl1\")\n",
      "     |      False\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |  \n",
      "     |  createExternalTable(self, tableName: str, path: Optional[str] = None, source: Optional[str] = None, schema: Optional[pyspark.sql.types.StructType] = None, **options: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Creates a table based on the dataset in a data source.\n",
      "     |      \n",
      "     |      It returns the DataFrame associated with the external table.\n",
      "     |      \n",
      "     |      The data source is specified by the ``source`` and a set of ``options``.\n",
      "     |      If ``source`` is not specified, the default data source configured by\n",
      "     |      ``spark.sql.sources.default`` will be used.\n",
      "     |      \n",
      "     |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
      "     |      created external table.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |  \n",
      "     |  createTable(self, tableName: str, path: Optional[str] = None, source: Optional[str] = None, schema: Optional[pyspark.sql.types.StructType] = None, description: Optional[str] = None, **options: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Creates a table based on the dataset in a data source.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.2.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          name of the table to create.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |             Allow ``tableName`` to be qualified with catalog name.\n",
      "     |      \n",
      "     |      path : str, optional\n",
      "     |          the path in which the data for this table exists.\n",
      "     |          When ``path`` is specified, an external table is\n",
      "     |          created from the data at the given path. Otherwise a managed table is created.\n",
      "     |      source : str, optional\n",
      "     |          the source of this table such as 'parquet, 'orc', etc.\n",
      "     |          If ``source`` is not specified, the default data source configured by\n",
      "     |          ``spark.sql.sources.default`` will be used.\n",
      "     |      schema : class:`StructType`, optional\n",
      "     |          the schema for this table.\n",
      "     |      description : str, optional\n",
      "     |          the description of this table.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.1.0\n",
      "     |              Added the ``description`` parameter.\n",
      "     |      \n",
      "     |      **options : dict, optional\n",
      "     |          extra options to specify in the table.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          The DataFrame associated with the table.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Creating a managed table.\n",
      "     |      \n",
      "     |      >>> _ = spark.catalog.createTable(\"tbl1\", schema=spark.range(1).schema, source='parquet')\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |      \n",
      "     |      Creating an external table\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     _ = spark.catalog.createTable(\n",
      "     |      ...         \"tbl2\", schema=spark.range(1).schema, path=d, source='parquet')\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl2\")\n",
      "     |  \n",
      "     |  currentCatalog(self) -> str\n",
      "     |      Returns the current default catalog in this session.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog.currentCatalog()\n",
      "     |      'spark_catalog'\n",
      "     |  \n",
      "     |  currentDatabase(self) -> str\n",
      "     |      Returns the current default database in this session.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      str\n",
      "     |          The current default database name.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog.currentDatabase()\n",
      "     |      'default'\n",
      "     |  \n",
      "     |  databaseExists(self, dbName: str) -> bool\n",
      "     |      Check if the database with the specified name exists.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dbName : str\n",
      "     |          name of the database to check existence\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |             Allow ``dbName`` to be qualified with catalog name.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          Indicating whether the database exists\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Check if 'test_new_database' database exists\n",
      "     |      \n",
      "     |      >>> spark.catalog.databaseExists(\"test_new_database\")\n",
      "     |      False\n",
      "     |      >>> _ = spark.sql(\"CREATE DATABASE test_new_database\")\n",
      "     |      >>> spark.catalog.databaseExists(\"test_new_database\")\n",
      "     |      True\n",
      "     |      \n",
      "     |      Using the fully qualified name with the catalog name.\n",
      "     |      \n",
      "     |      >>> spark.catalog.databaseExists(\"spark_catalog.test_new_database\")\n",
      "     |      True\n",
      "     |      >>> _ = spark.sql(\"DROP DATABASE test_new_database\")\n",
      "     |  \n",
      "     |  dropGlobalTempView(self, viewName: str) -> bool\n",
      "     |      Drops the global temporary view with the given view name in the catalog.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      viewName : str\n",
      "     |          name of the global view to drop.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          If the global view was successfully dropped or not.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If the view has been cached before, then it will also be uncached.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.createDataFrame([(1, 1)]).createGlobalTempView(\"my_table\")\n",
      "     |      \n",
      "     |      Dropping the global view.\n",
      "     |      \n",
      "     |      >>> spark.catalog.dropGlobalTempView(\"my_table\")\n",
      "     |      True\n",
      "     |      \n",
      "     |      Throw an exception if the global view does not exists.\n",
      "     |      \n",
      "     |      >>> spark.table(\"global_temp.my_table\")\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      AnalysisException: ...\n",
      "     |  \n",
      "     |  dropTempView(self, viewName: str) -> bool\n",
      "     |      Drops the local temporary view with the given view name in the catalog.\n",
      "     |      If the view has been cached before, then it will also be uncached.\n",
      "     |      Returns true if this view is dropped successfully, false otherwise.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      viewName : str\n",
      "     |          name of the temporary view to drop.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          If the temporary view was successfully dropped or not.\n",
      "     |      \n",
      "     |          .. versionadded:: 2.1.0\n",
      "     |              The return type of this method was ``None`` in Spark 2.0, but changed to ``bool``\n",
      "     |              in Spark 2.1.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.createDataFrame([(1, 1)]).createTempView(\"my_table\")\n",
      "     |      \n",
      "     |      Dropping the temporary view.\n",
      "     |      \n",
      "     |      >>> spark.catalog.dropTempView(\"my_table\")\n",
      "     |      True\n",
      "     |      \n",
      "     |      Throw an exception if the temporary view does not exists.\n",
      "     |      \n",
      "     |      >>> spark.table(\"my_table\")\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      AnalysisException: ...\n",
      "     |  \n",
      "     |  functionExists(self, functionName: str, dbName: Optional[str] = None) -> bool\n",
      "     |      Check if the function with the specified name exists.\n",
      "     |      This can either be a temporary function or a function.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      functionName : str\n",
      "     |          name of the function to check existence\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |             Allow ``functionName`` to be qualified with catalog name\n",
      "     |      \n",
      "     |      dbName : str, optional\n",
      "     |          name of the database to check function existence in.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          Indicating whether the function exists\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If no database is specified, the current database and catalog\n",
      "     |      are used. This API includes all temporary functions.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog.functionExists(\"count\")\n",
      "     |      True\n",
      "     |      \n",
      "     |      Using the fully qualified name for function name.\n",
      "     |      \n",
      "     |      >>> spark.catalog.functionExists(\"default.unexisting_function\")\n",
      "     |      False\n",
      "     |      >>> spark.catalog.functionExists(\"spark_catalog.default.unexisting_function\")\n",
      "     |      False\n",
      "     |  \n",
      "     |  getDatabase(self, dbName: str) -> pyspark.sql.catalog.Database\n",
      "     |      Get the database with the specified name.\n",
      "     |      This throws an :class:`AnalysisException` when the database cannot be found.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dbName : str\n",
      "     |           name of the database to get.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Database`\n",
      "     |          The database found by the name.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog.getDatabase(\"default\")\n",
      "     |      Database(name='default', catalog='spark_catalog', description='default database', ...\n",
      "     |      \n",
      "     |      Using the fully qualified name with the catalog name.\n",
      "     |      \n",
      "     |      >>> spark.catalog.getDatabase(\"spark_catalog.default\")\n",
      "     |      Database(name='default', catalog='spark_catalog', description='default database', ...\n",
      "     |  \n",
      "     |  getFunction(self, functionName: str) -> pyspark.sql.catalog.Function\n",
      "     |      Get the function with the specified name. This function can be a temporary function or a\n",
      "     |      function. This throws an :class:`AnalysisException` when the function cannot be found.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      functionName : str\n",
      "     |          name of the function to check existence.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Function`\n",
      "     |          The function found by the name.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = spark.sql(\n",
      "     |      ...     \"CREATE FUNCTION my_func1 AS 'test.org.apache.spark.sql.MyDoubleAvg'\")\n",
      "     |      >>> spark.catalog.getFunction(\"my_func1\")\n",
      "     |      Function(name='my_func1', catalog='spark_catalog', namespace=['default'], ...\n",
      "     |      \n",
      "     |      Using the fully qualified name for function name.\n",
      "     |      \n",
      "     |      >>> spark.catalog.getFunction(\"default.my_func1\")\n",
      "     |      Function(name='my_func1', catalog='spark_catalog', namespace=['default'], ...\n",
      "     |      >>> spark.catalog.getFunction(\"spark_catalog.default.my_func1\")\n",
      "     |      Function(name='my_func1', catalog='spark_catalog', namespace=['default'], ...\n",
      "     |      \n",
      "     |      Throw an analysis exception when the function does not exists.\n",
      "     |      \n",
      "     |      >>> spark.catalog.getFunction(\"my_func2\")\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      AnalysisException: ...\n",
      "     |  \n",
      "     |  getTable(self, tableName: str) -> pyspark.sql.catalog.Table\n",
      "     |      Get the table or view with the specified name. This table can be a temporary view or a\n",
      "     |      table/view. This throws an :class:`AnalysisException` when no Table can be found.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          name of the table to get.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |             Allow `tableName` to be qualified with catalog name.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Table`\n",
      "     |          The table found by the name.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      >>> _ = spark.sql(\"CREATE TABLE tbl1 (name STRING, age INT) USING parquet\")\n",
      "     |      >>> spark.catalog.getTable(\"tbl1\")\n",
      "     |      Table(name='tbl1', catalog='spark_catalog', namespace=['default'], ...\n",
      "     |      \n",
      "     |      Using the fully qualified name with the catalog name.\n",
      "     |      \n",
      "     |      >>> spark.catalog.getTable(\"default.tbl1\")\n",
      "     |      Table(name='tbl1', catalog='spark_catalog', namespace=['default'], ...\n",
      "     |      >>> spark.catalog.getTable(\"spark_catalog.default.tbl1\")\n",
      "     |      Table(name='tbl1', catalog='spark_catalog', namespace=['default'], ...\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |      \n",
      "     |      Throw an analysis exception when the table does not exist.\n",
      "     |      \n",
      "     |      >>> spark.catalog.getTable(\"tbl1\")\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      AnalysisException: ...\n",
      "     |  \n",
      "     |  isCached(self, tableName: str) -> bool\n",
      "     |      Returns true if the table is currently cached in-memory.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          name of the table to get.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |              Allow ``tableName`` to be qualified with catalog name.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      >>> _ = spark.sql(\"CREATE TABLE tbl1 (name STRING, age INT) USING parquet\")\n",
      "     |      >>> spark.catalog.cacheTable(\"tbl1\")\n",
      "     |      >>> spark.catalog.isCached(\"tbl1\")\n",
      "     |      True\n",
      "     |      \n",
      "     |      Throw an analysis exception when the table does not exist.\n",
      "     |      \n",
      "     |      >>> spark.catalog.isCached(\"not_existing_table\")\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      AnalysisException: ...\n",
      "     |      \n",
      "     |      Using the fully qualified name for the table.\n",
      "     |      \n",
      "     |      >>> spark.catalog.isCached(\"spark_catalog.default.tbl1\")\n",
      "     |      True\n",
      "     |      >>> spark.catalog.uncacheTable(\"tbl1\")\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |  \n",
      "     |  listCatalogs(self, pattern: Optional[str] = None) -> List[pyspark.sql.catalog.CatalogMetadata]\n",
      "     |      Returns a list of catalogs in this session.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      pattern : str\n",
      "     |          The pattern that the catalog name needs to match.\n",
      "     |      \n",
      "     |          .. versionchanged: 3.5.0\n",
      "     |              Added ``pattern`` argument.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          A list of :class:`CatalogMetadata`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog.listCatalogs()\n",
      "     |      [CatalogMetadata(name='spark_catalog', description=None)]\n",
      "     |      \n",
      "     |      >>> spark.catalog.listCatalogs(\"spark*\")\n",
      "     |      [CatalogMetadata(name='spark_catalog', description=None)]\n",
      "     |      \n",
      "     |      >>> spark.catalog.listCatalogs(\"hive*\")\n",
      "     |      []\n",
      "     |  \n",
      "     |  listColumns(self, tableName: str, dbName: Optional[str] = None) -> List[pyspark.sql.catalog.Column]\n",
      "     |      Returns a list of columns for the given table/view in the specified database.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          name of the table to list columns.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |             Allow ``tableName`` to be qualified with catalog name when ``dbName`` is None.\n",
      "     |      \n",
      "     |      dbName : str, optional\n",
      "     |          name of the database to find the table to list columns.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          A list of :class:`Column`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The order of arguments here is different from that of its JVM counterpart\n",
      "     |      because Python does not support method overloading.\n",
      "     |      \n",
      "     |      If no database is specified, the current database and catalog\n",
      "     |      are used. This API includes all temporary views.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      >>> _ = spark.sql(\"CREATE TABLE tblA (name STRING, age INT) USING parquet\")\n",
      "     |      >>> spark.catalog.listColumns(\"tblA\")\n",
      "     |      [Column(name='name', description=None, dataType='string', nullable=True, ...\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n",
      "     |  \n",
      "     |  listDatabases(self, pattern: Optional[str] = None) -> List[pyspark.sql.catalog.Database]\n",
      "     |      Returns a list of databases available across all sessions.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      pattern : str\n",
      "     |          The pattern that the database name needs to match.\n",
      "     |      \n",
      "     |          .. versionchanged: 3.5.0\n",
      "     |              Adds ``pattern`` argument.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          A list of :class:`Database`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog.listDatabases()\n",
      "     |      [Database(name='default', catalog='spark_catalog', description='default database', ...\n",
      "     |      \n",
      "     |      >>> spark.catalog.listDatabases(\"def*\")\n",
      "     |      [Database(name='default', catalog='spark_catalog', description='default database', ...\n",
      "     |      \n",
      "     |      >>> spark.catalog.listDatabases(\"def2*\")\n",
      "     |      []\n",
      "     |  \n",
      "     |  listFunctions(self, dbName: Optional[str] = None, pattern: Optional[str] = None) -> List[pyspark.sql.catalog.Function]\n",
      "     |      Returns a list of functions registered in the specified database.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dbName : str\n",
      "     |          name of the database to list the functions.\n",
      "     |          ``dbName`` can be qualified with catalog name.\n",
      "     |      pattern : str\n",
      "     |          The pattern that the function name needs to match.\n",
      "     |      \n",
      "     |          .. versionchanged: 3.5.0\n",
      "     |              Adds ``pattern`` argument.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          A list of :class:`Function`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If no database is specified, the current database and catalog\n",
      "     |      are used. This API includes all temporary functions.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog.listFunctions()\n",
      "     |      [Function(name=...\n",
      "     |      \n",
      "     |      >>> spark.catalog.listFunctions(pattern=\"to_*\")\n",
      "     |      [Function(name=...\n",
      "     |      \n",
      "     |      >>> spark.catalog.listFunctions(pattern=\"*not_existing_func*\")\n",
      "     |      []\n",
      "     |  \n",
      "     |  listTables(self, dbName: Optional[str] = None, pattern: Optional[str] = None) -> List[pyspark.sql.catalog.Table]\n",
      "     |      Returns a list of tables/views in the specified database.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dbName : str\n",
      "     |          name of the database to list the tables.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |             Allow ``dbName`` to be qualified with catalog name.\n",
      "     |      \n",
      "     |      pattern : str\n",
      "     |          The pattern that the database name needs to match.\n",
      "     |      \n",
      "     |          .. versionchanged: 3.5.0\n",
      "     |              Adds ``pattern`` argument.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          A list of :class:`Table`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If no database is specified, the current database and catalog\n",
      "     |      are used. This API includes all temporary views.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.range(1).createTempView(\"test_view\")\n",
      "     |      >>> spark.catalog.listTables()\n",
      "     |      [Table(name='test_view', catalog=None, namespace=[], description=None, ...\n",
      "     |      \n",
      "     |      >>> spark.catalog.listTables(pattern=\"test*\")\n",
      "     |      [Table(name='test_view', catalog=None, namespace=[], description=None, ...\n",
      "     |      \n",
      "     |      >>> spark.catalog.listTables(pattern=\"table*\")\n",
      "     |      []\n",
      "     |      \n",
      "     |      >>> _ = spark.catalog.dropTempView(\"test_view\")\n",
      "     |      >>> spark.catalog.listTables()\n",
      "     |      []\n",
      "     |  \n",
      "     |  recoverPartitions(self, tableName: str) -> None\n",
      "     |      Recovers all the partitions of the given table and updates the catalog.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.1\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          name of the table to get.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Only works with a partitioned table, and not a view.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      The example below creates a partitioned table against the existing directory of\n",
      "     |      the partitioned table. After that, it recovers the partitions.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      ...     spark.range(1).selectExpr(\n",
      "     |      ...         \"id as key\", \"id as value\").write.partitionBy(\"key\").mode(\"overwrite\").save(d)\n",
      "     |      ...     _ = spark.sql(\n",
      "     |      ...          \"CREATE TABLE tbl1 (key LONG, value LONG)\"\n",
      "     |      ...          \"USING parquet OPTIONS (path '{}') PARTITIONED BY (key)\".format(d))\n",
      "     |      ...     spark.table(\"tbl1\").show()\n",
      "     |      ...     spark.catalog.recoverPartitions(\"tbl1\")\n",
      "     |      ...     spark.table(\"tbl1\").show()\n",
      "     |      +-----+---+\n",
      "     |      |value|key|\n",
      "     |      +-----+---+\n",
      "     |      +-----+---+\n",
      "     |      +-----+---+\n",
      "     |      |value|key|\n",
      "     |      +-----+---+\n",
      "     |      |    0|  0|\n",
      "     |      +-----+---+\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |  \n",
      "     |  refreshByPath(self, path: str) -> None\n",
      "     |      Invalidates and refreshes all the cached data (and the associated metadata) for any\n",
      "     |      DataFrame that contains the given data source path.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.2.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          the path to refresh the cache.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      The example below caches a table, and then removes the data.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      ...     _ = spark.sql(\n",
      "     |      ...         \"CREATE TABLE tbl1 (col STRING) USING TEXT LOCATION '{}'\".format(d))\n",
      "     |      ...     _ = spark.sql(\"INSERT INTO tbl1 SELECT 'abc'\")\n",
      "     |      ...     spark.catalog.cacheTable(\"tbl1\")\n",
      "     |      ...     spark.table(\"tbl1\").show()\n",
      "     |      +---+\n",
      "     |      |col|\n",
      "     |      +---+\n",
      "     |      |abc|\n",
      "     |      +---+\n",
      "     |      \n",
      "     |      Because the table is cached, it computes from the cached data as below.\n",
      "     |      \n",
      "     |      >>> spark.table(\"tbl1\").count()\n",
      "     |      1\n",
      "     |      \n",
      "     |      After refreshing the table by path, it shows 0 because the data does not exist anymore.\n",
      "     |      \n",
      "     |      >>> spark.catalog.refreshByPath(d)\n",
      "     |      >>> spark.table(\"tbl1\").count()\n",
      "     |      0\n",
      "     |      \n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |  \n",
      "     |  refreshTable(self, tableName: str) -> None\n",
      "     |      Invalidates and refreshes all the cached data and metadata of the given table.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          name of the table to get.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |              Allow ``tableName`` to be qualified with catalog name.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      The example below caches a table, and then removes the data.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      ...     _ = spark.sql(\n",
      "     |      ...         \"CREATE TABLE tbl1 (col STRING) USING TEXT LOCATION '{}'\".format(d))\n",
      "     |      ...     _ = spark.sql(\"INSERT INTO tbl1 SELECT 'abc'\")\n",
      "     |      ...     spark.catalog.cacheTable(\"tbl1\")\n",
      "     |      ...     spark.table(\"tbl1\").show()\n",
      "     |      +---+\n",
      "     |      |col|\n",
      "     |      +---+\n",
      "     |      |abc|\n",
      "     |      +---+\n",
      "     |      \n",
      "     |      Because the table is cached, it computes from the cached data as below.\n",
      "     |      \n",
      "     |      >>> spark.table(\"tbl1\").count()\n",
      "     |      1\n",
      "     |      \n",
      "     |      After refreshing the table, it shows 0 because the data does not exist anymore.\n",
      "     |      \n",
      "     |      >>> spark.catalog.refreshTable(\"tbl1\")\n",
      "     |      >>> spark.table(\"tbl1\").count()\n",
      "     |      0\n",
      "     |      \n",
      "     |      Using the fully qualified name for the table.\n",
      "     |      \n",
      "     |      >>> spark.catalog.refreshTable(\"spark_catalog.default.tbl1\")\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |  \n",
      "     |  registerFunction(self, name: str, f: Callable[..., Any], returnType: Optional[ForwardRef('DataType')] = None) -> 'UserDefinedFunctionLike'\n",
      "     |      An alias for :func:`spark.udf.register`.\n",
      "     |      See :meth:`pyspark.sql.UDFRegistration.register`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. deprecated:: 2.3.0\n",
      "     |          Use :func:`spark.udf.register` instead.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |  \n",
      "     |  setCurrentCatalog(self, catalogName: str) -> None\n",
      "     |      Sets the current default catalog in this session.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      catalogName : str\n",
      "     |          name of the catalog to set\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog.setCurrentCatalog(\"spark_catalog\")\n",
      "     |  \n",
      "     |  setCurrentDatabase(self, dbName: str) -> None\n",
      "     |      Sets the current default database in this session.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog.setCurrentDatabase(\"default\")\n",
      "     |  \n",
      "     |  tableExists(self, tableName: str, dbName: Optional[str] = None) -> bool\n",
      "     |      Check if the table or view with the specified name exists.\n",
      "     |      This can either be a temporary view or a table/view.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          name of the table to check existence.\n",
      "     |          If no database is specified, first try to treat ``tableName`` as a\n",
      "     |          multi-layer-namespace identifier, then try ``tableName`` as a normal table\n",
      "     |          name in the current database if necessary.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |             Allow ``tableName`` to be qualified with catalog name when ``dbName`` is None.\n",
      "     |      \n",
      "     |      dbName : str, optional\n",
      "     |          name of the database to check table existence in.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          Indicating whether the table/view exists\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      This function can check if a table is defined or not:\n",
      "     |      \n",
      "     |      >>> spark.catalog.tableExists(\"unexisting_table\")\n",
      "     |      False\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      >>> _ = spark.sql(\"CREATE TABLE tbl1 (name STRING, age INT) USING parquet\")\n",
      "     |      >>> spark.catalog.tableExists(\"tbl1\")\n",
      "     |      True\n",
      "     |      \n",
      "     |      Using the fully qualified names for tables.\n",
      "     |      \n",
      "     |      >>> spark.catalog.tableExists(\"default.tbl1\")\n",
      "     |      True\n",
      "     |      >>> spark.catalog.tableExists(\"spark_catalog.default.tbl1\")\n",
      "     |      True\n",
      "     |      >>> spark.catalog.tableExists(\"tbl1\", \"default\")\n",
      "     |      True\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |      \n",
      "     |      Check if views exist:\n",
      "     |      \n",
      "     |      >>> spark.catalog.tableExists(\"view1\")\n",
      "     |      False\n",
      "     |      >>> _ = spark.sql(\"CREATE VIEW view1 AS SELECT 1\")\n",
      "     |      >>> spark.catalog.tableExists(\"view1\")\n",
      "     |      True\n",
      "     |      \n",
      "     |      Using the fully qualified names for views.\n",
      "     |      \n",
      "     |      >>> spark.catalog.tableExists(\"default.view1\")\n",
      "     |      True\n",
      "     |      >>> spark.catalog.tableExists(\"spark_catalog.default.view1\")\n",
      "     |      True\n",
      "     |      >>> spark.catalog.tableExists(\"view1\", \"default\")\n",
      "     |      True\n",
      "     |      >>> _ = spark.sql(\"DROP VIEW view1\")\n",
      "     |      \n",
      "     |      Check if temporary views exist:\n",
      "     |      \n",
      "     |      >>> _ = spark.sql(\"CREATE TEMPORARY VIEW view1 AS SELECT 1\")\n",
      "     |      >>> spark.catalog.tableExists(\"view1\")\n",
      "     |      True\n",
      "     |      >>> df = spark.sql(\"DROP VIEW view1\")\n",
      "     |      >>> spark.catalog.tableExists(\"view1\")\n",
      "     |      False\n",
      "     |  \n",
      "     |  uncacheTable(self, tableName: str) -> None\n",
      "     |      Removes the specified table from the in-memory cache.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          name of the table to get.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |              Allow ``tableName`` to be qualified with catalog name.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      >>> _ = spark.sql(\"CREATE TABLE tbl1 (name STRING, age INT) USING parquet\")\n",
      "     |      >>> spark.catalog.cacheTable(\"tbl1\")\n",
      "     |      >>> spark.catalog.uncacheTable(\"tbl1\")\n",
      "     |      >>> spark.catalog.isCached(\"tbl1\")\n",
      "     |      False\n",
      "     |      \n",
      "     |      Throw an analysis exception when the table does not exist.\n",
      "     |      \n",
      "     |      >>> spark.catalog.uncacheTable(\"not_existing_table\")\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      AnalysisException: ...\n",
      "     |      \n",
      "     |      Using the fully qualified name for the table.\n",
      "     |      \n",
      "     |      >>> spark.catalog.uncacheTable(\"spark_catalog.default.tbl1\")\n",
      "     |      >>> spark.catalog.isCached(\"tbl1\")\n",
      "     |      False\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Column(builtins.object)\n",
      "     |  Column(jc: py4j.java_gateway.JavaObject) -> None\n",
      "     |  \n",
      "     |  A column in a DataFrame.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.3.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Column instances can be created by\n",
      "     |  \n",
      "     |  >>> df = spark.createDataFrame(\n",
      "     |  ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |  \n",
      "     |  Select a column out of a DataFrame\n",
      "     |  >>> df.name\n",
      "     |  Column<'name'>\n",
      "     |  >>> df[\"name\"]\n",
      "     |  Column<'name'>\n",
      "     |  \n",
      "     |  Create from an expression\n",
      "     |  \n",
      "     |  >>> df.age + 1\n",
      "     |  Column<...>\n",
      "     |  >>> 1 / df.age\n",
      "     |  Column<...>\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __add__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __and__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __bool__ = __nonzero__(self) -> None\n",
      "     |  \n",
      "     |  __contains__(self, item: Any) -> None\n",
      "     |      # container operators\n",
      "     |  \n",
      "     |  __div__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __eq__(self, other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary function\n",
      "     |  \n",
      "     |  __ge__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __getattr__(self, item: Any) -> 'Column'\n",
      "     |      An expression that gets an item at position ``ordinal`` out of a list,\n",
      "     |      or gets an item by key out of a dict.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      item\n",
      "     |          a literal value.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing the item got by key out of a dict.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([('abcedfg', {\"key\": \"value\"})], [\"l\", \"d\"])\n",
      "     |      >>> df.select(df.d.key).show()\n",
      "     |      +------+\n",
      "     |      |d[key]|\n",
      "     |      +------+\n",
      "     |      | value|\n",
      "     |      +------+\n",
      "     |  \n",
      "     |  __getitem__(self, k: Any) -> 'Column'\n",
      "     |      An expression that gets an item at position ``ordinal`` out of a list,\n",
      "     |      or gets an item by key out of a dict.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      k\n",
      "     |          a literal value, or a slice object without step.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing the item got by key out of a dict, or substrings sliced by\n",
      "     |          the given slice object.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([('abcedfg', {\"key\": \"value\"})], [\"l\", \"d\"])\n",
      "     |      >>> df.select(df.l[slice(1, 3)], df.d['key']).show()\n",
      "     |      +------------------+------+\n",
      "     |      |substring(l, 1, 3)|d[key]|\n",
      "     |      +------------------+------+\n",
      "     |      |               abc| value|\n",
      "     |      +------------------+------+\n",
      "     |  \n",
      "     |  __gt__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __init__(self, jc: py4j.java_gateway.JavaObject) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __invert__ = _(self: 'Column') -> 'Column'\n",
      "     |  \n",
      "     |  __iter__(self) -> None\n",
      "     |  \n",
      "     |  __le__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __lt__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __mod__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __mul__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __ne__(self, other: Any) -> 'Column'\n",
      "     |      binary function\n",
      "     |  \n",
      "     |  __neg__ = _(self: 'Column') -> 'Column'\n",
      "     |  \n",
      "     |  __nonzero__(self) -> None\n",
      "     |  \n",
      "     |  __or__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __pow__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      "     |      binary function\n",
      "     |  \n",
      "     |  __radd__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __rand__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __rdiv__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __rmod__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __rmul__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __ror__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __rpow__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      "     |      binary function\n",
      "     |  \n",
      "     |  __rsub__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __rtruediv__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __sub__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __truediv__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  alias(self, *alias: str, **kwargs: Any) -> 'Column'\n",
      "     |      Returns this column aliased with a new name or names (in the case of expressions that\n",
      "     |      return more than one column, such as explode).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      alias : str\n",
      "     |          desired column names (collects all positional arguments passed)\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      metadata: dict\n",
      "     |          a dict of information to be stored in ``metadata`` attribute of the\n",
      "     |          corresponding :class:`StructField <pyspark.sql.types.StructField>` (optional, keyword\n",
      "     |          only argument)\n",
      "     |      \n",
      "     |          .. versionchanged:: 2.2.0\n",
      "     |             Added optional ``metadata`` argument.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing whether each element of Column is aliased with new name or names.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.select(df.age.alias(\"age2\")).collect()\n",
      "     |      [Row(age2=2), Row(age2=5)]\n",
      "     |      >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n",
      "     |      99\n",
      "     |  \n",
      "     |  asc = _(self: 'Column') -> 'Column'\n",
      "     |      Returns a sort expression based on the ascending order of the column.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
      "     |      >>> df.select(df.name).orderBy(df.name.asc()).collect()\n",
      "     |      [Row(name='Alice'), Row(name='Tom')]\n",
      "     |  \n",
      "     |  asc_nulls_first = _(self: 'Column') -> 'Column'\n",
      "     |      Returns a sort expression based on ascending order of the column, and null values\n",
      "     |      return before non-null values.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      "     |      >>> df.select(df.name).orderBy(df.name.asc_nulls_first()).collect()\n",
      "     |      [Row(name=None), Row(name='Alice'), Row(name='Tom')]\n",
      "     |  \n",
      "     |  asc_nulls_last = _(self: 'Column') -> 'Column'\n",
      "     |      Returns a sort expression based on ascending order of the column, and null values\n",
      "     |      appear after non-null values.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      "     |      >>> df.select(df.name).orderBy(df.name.asc_nulls_last()).collect()\n",
      "     |      [Row(name='Alice'), Row(name='Tom'), Row(name=None)]\n",
      "     |  \n",
      "     |  astype = cast(self, dataType)\n",
      "     |      :func:`astype` is an alias for :func:`cast`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  between(self, lowerBound: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DateTimeLiteral'), ForwardRef('DecimalLiteral')], upperBound: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DateTimeLiteral'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      "     |      True if the current column is between the lower bound and upper bound, inclusive.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      lowerBound : :class:`Column`, int, float, string, bool, datetime, date or Decimal\n",
      "     |          a boolean expression that boundary start, inclusive.\n",
      "     |      upperBound : :class:`Column`, int, float, string, bool, datetime, date or Decimal\n",
      "     |          a boolean expression that boundary end, inclusive.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column of booleans showing whether each element of Column\n",
      "     |          is between left and right (inclusive).\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.select(df.name, df.age.between(2, 4)).show()\n",
      "     |      +-----+---------------------------+\n",
      "     |      | name|((age >= 2) AND (age <= 4))|\n",
      "     |      +-----+---------------------------+\n",
      "     |      |Alice|                       true|\n",
      "     |      |  Bob|                      false|\n",
      "     |      +-----+---------------------------+\n",
      "     |  \n",
      "     |  bitwiseAND = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      Compute bitwise AND of this expression with another expression.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other\n",
      "     |          a value or :class:`Column` to calculate bitwise and(&) with\n",
      "     |          this :class:`Column`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      "     |      >>> df.select(df.a.bitwiseAND(df.b)).collect()\n",
      "     |      [Row((a & b)=10)]\n",
      "     |  \n",
      "     |  bitwiseOR = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      Compute bitwise OR of this expression with another expression.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other\n",
      "     |          a value or :class:`Column` to calculate bitwise or(|) with\n",
      "     |          this :class:`Column`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      "     |      >>> df.select(df.a.bitwiseOR(df.b)).collect()\n",
      "     |      [Row((a | b)=235)]\n",
      "     |  \n",
      "     |  bitwiseXOR = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      Compute bitwise XOR of this expression with another expression.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other\n",
      "     |          a value or :class:`Column` to calculate bitwise xor(^) with\n",
      "     |          this :class:`Column`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      "     |      >>> df.select(df.a.bitwiseXOR(df.b)).collect()\n",
      "     |      [Row((a ^ b)=225)]\n",
      "     |  \n",
      "     |  cast(self, dataType: Union[pyspark.sql.types.DataType, str]) -> 'Column'\n",
      "     |      Casts the column into type ``dataType``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dataType : :class:`DataType` or str\n",
      "     |          a DataType or Python string literal with a DDL-formatted string\n",
      "     |          to use when parsing the column to the same type.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing whether each element of Column is cast into new type.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.types import StringType\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n",
      "     |      [Row(ages='2'), Row(ages='5')]\n",
      "     |      >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n",
      "     |      [Row(ages='2'), Row(ages='5')]\n",
      "     |  \n",
      "     |  contains = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      Contains the other element. Returns a boolean :class:`Column` based on a string match.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other\n",
      "     |          string in line. A value as a literal or a :class:`Column`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.filter(df.name.contains('o')).collect()\n",
      "     |      [Row(age=5, name='Bob')]\n",
      "     |  \n",
      "     |  desc = _(self: 'Column') -> 'Column'\n",
      "     |      Returns a sort expression based on the descending order of the column.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
      "     |      >>> df.select(df.name).orderBy(df.name.desc()).collect()\n",
      "     |      [Row(name='Tom'), Row(name='Alice')]\n",
      "     |  \n",
      "     |  desc_nulls_first = _(self: 'Column') -> 'Column'\n",
      "     |      Returns a sort expression based on the descending order of the column, and null values\n",
      "     |      appear before non-null values.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      "     |      >>> df.select(df.name).orderBy(df.name.desc_nulls_first()).collect()\n",
      "     |      [Row(name=None), Row(name='Tom'), Row(name='Alice')]\n",
      "     |  \n",
      "     |  desc_nulls_last = _(self: 'Column') -> 'Column'\n",
      "     |      Returns a sort expression based on the descending order of the column, and null values\n",
      "     |      appear after non-null values.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      "     |      >>> df.select(df.name).orderBy(df.name.desc_nulls_last()).collect()\n",
      "     |      [Row(name='Tom'), Row(name='Alice'), Row(name=None)]\n",
      "     |  \n",
      "     |  dropFields(self, *fieldNames: str) -> 'Column'\n",
      "     |      An expression that drops fields in :class:`StructType` by name.\n",
      "     |      This is a no-op if the schema doesn't contain field name(s).\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fieldNames : str\n",
      "     |          Desired field names (collects all positional arguments passed)\n",
      "     |          The result will drop at a location if any field matches in the Column.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing whether each element of Column with field dropped by fieldName.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> from pyspark.sql.functions import col, lit\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n",
      "     |      >>> df.withColumn('a', df['a'].dropFields('b')).show()\n",
      "     |      +-----------------+\n",
      "     |      |                a|\n",
      "     |      +-----------------+\n",
      "     |      |{2, 3, {4, 5, 6}}|\n",
      "     |      +-----------------+\n",
      "     |      \n",
      "     |      >>> df.withColumn('a', df['a'].dropFields('b', 'c')).show()\n",
      "     |      +--------------+\n",
      "     |      |             a|\n",
      "     |      +--------------+\n",
      "     |      |{3, {4, 5, 6}}|\n",
      "     |      +--------------+\n",
      "     |      \n",
      "     |      This method supports dropping multiple nested fields directly e.g.\n",
      "     |      \n",
      "     |      >>> df.withColumn(\"a\", col(\"a\").dropFields(\"e.g\", \"e.h\")).show()\n",
      "     |      +--------------+\n",
      "     |      |             a|\n",
      "     |      +--------------+\n",
      "     |      |{1, 2, 3, {4}}|\n",
      "     |      +--------------+\n",
      "     |      \n",
      "     |      However, if you are going to add/replace multiple nested fields,\n",
      "     |      it is preferred to extract out the nested struct before\n",
      "     |      adding/replacing multiple fields e.g.\n",
      "     |      \n",
      "     |      >>> df.select(col(\"a\").withField(\n",
      "     |      ...     \"e\", col(\"a.e\").dropFields(\"g\", \"h\")).alias(\"a\")\n",
      "     |      ... ).show()\n",
      "     |      +--------------+\n",
      "     |      |             a|\n",
      "     |      +--------------+\n",
      "     |      |{1, 2, 3, {4}}|\n",
      "     |      +--------------+\n",
      "     |  \n",
      "     |  endswith = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      String ends with. Returns a boolean :class:`Column` based on a string match.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`Column` or str\n",
      "     |          string at end of line (do not use a regex `$`)\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.filter(df.name.endswith('ice')).collect()\n",
      "     |      [Row(age=2, name='Alice')]\n",
      "     |      >>> df.filter(df.name.endswith('ice$')).collect()\n",
      "     |      []\n",
      "     |  \n",
      "     |  eqNullSafe = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      Equality test that is safe for null values.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other\n",
      "     |          a value or :class:`Column`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df1 = spark.createDataFrame([\n",
      "     |      ...     Row(id=1, value='foo'),\n",
      "     |      ...     Row(id=2, value=None)\n",
      "     |      ... ])\n",
      "     |      >>> df1.select(\n",
      "     |      ...     df1['value'] == 'foo',\n",
      "     |      ...     df1['value'].eqNullSafe('foo'),\n",
      "     |      ...     df1['value'].eqNullSafe(None)\n",
      "     |      ... ).show()\n",
      "     |      +-------------+---------------+----------------+\n",
      "     |      |(value = foo)|(value <=> foo)|(value <=> NULL)|\n",
      "     |      +-------------+---------------+----------------+\n",
      "     |      |         true|           true|           false|\n",
      "     |      |         NULL|          false|            true|\n",
      "     |      +-------------+---------------+----------------+\n",
      "     |      >>> df2 = spark.createDataFrame([\n",
      "     |      ...     Row(value = 'bar'),\n",
      "     |      ...     Row(value = None)\n",
      "     |      ... ])\n",
      "     |      >>> df1.join(df2, df1[\"value\"] == df2[\"value\"]).count()\n",
      "     |      0\n",
      "     |      >>> df1.join(df2, df1[\"value\"].eqNullSafe(df2[\"value\"])).count()\n",
      "     |      1\n",
      "     |      >>> df2 = spark.createDataFrame([\n",
      "     |      ...     Row(id=1, value=float('NaN')),\n",
      "     |      ...     Row(id=2, value=42.0),\n",
      "     |      ...     Row(id=3, value=None)\n",
      "     |      ... ])\n",
      "     |      >>> df2.select(\n",
      "     |      ...     df2['value'].eqNullSafe(None),\n",
      "     |      ...     df2['value'].eqNullSafe(float('NaN')),\n",
      "     |      ...     df2['value'].eqNullSafe(42.0)\n",
      "     |      ... ).show()\n",
      "     |      +----------------+---------------+----------------+\n",
      "     |      |(value <=> NULL)|(value <=> NaN)|(value <=> 42.0)|\n",
      "     |      +----------------+---------------+----------------+\n",
      "     |      |           false|           true|           false|\n",
      "     |      |           false|          false|            true|\n",
      "     |      |            true|          false|           false|\n",
      "     |      +----------------+---------------+----------------+\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Unlike Pandas, PySpark doesn't consider NaN values to be NULL. See the\n",
      "     |      `NaN Semantics <https://spark.apache.org/docs/latest/sql-ref-datatypes.html#nan-semantics>`_\n",
      "     |      for details.\n",
      "     |  \n",
      "     |  getField(self, name: Any) -> 'Column'\n",
      "     |      An expression that gets a field by name in a :class:`StructType`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name\n",
      "     |          a literal value, or a :class:`Column` expression.\n",
      "     |          The result will only be true at a location if the field matches in the Column.\n",
      "     |      \n",
      "     |           .. deprecated:: 3.0.0\n",
      "     |               :class:`Column` as a parameter is deprecated.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing whether each element of Column got by name.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n",
      "     |      >>> df.select(df.r.getField(\"b\")).show()\n",
      "     |      +---+\n",
      "     |      |r.b|\n",
      "     |      +---+\n",
      "     |      |  b|\n",
      "     |      +---+\n",
      "     |      >>> df.select(df.r.a).show()\n",
      "     |      +---+\n",
      "     |      |r.a|\n",
      "     |      +---+\n",
      "     |      |  1|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  getItem(self, key: Any) -> 'Column'\n",
      "     |      An expression that gets an item at position ``ordinal`` out of a list,\n",
      "     |      or gets an item by key out of a dict.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key\n",
      "     |          a literal value, or a :class:`Column` expression.\n",
      "     |          The result will only be true at a location if the item matches in the column.\n",
      "     |      \n",
      "     |           .. deprecated:: 3.0.0\n",
      "     |               :class:`Column` as a parameter is deprecated.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing the item(s) got at position out of a list or by key out of a dict.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n",
      "     |      >>> df.select(df.l.getItem(0), df.d.getItem(\"key\")).show()\n",
      "     |      +----+------+\n",
      "     |      |l[0]|d[key]|\n",
      "     |      +----+------+\n",
      "     |      |   1| value|\n",
      "     |      +----+------+\n",
      "     |  \n",
      "     |  ilike(self: 'Column', other: str) -> 'Column'\n",
      "     |      SQL ILIKE expression (case insensitive LIKE). Returns a boolean :class:`Column`\n",
      "     |      based on a case insensitive match.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : str\n",
      "     |          a SQL LIKE pattern\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.Column.rlike\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column of booleans showing whether each element\n",
      "     |          in the Column is matched by SQL LIKE pattern.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.filter(df.name.ilike('%Ice')).collect()\n",
      "     |      [Row(age=2, name='Alice')]\n",
      "     |  \n",
      "     |  isNotNull = _(self: 'Column') -> 'Column'\n",
      "     |      True if the current expression is NOT null.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      "     |      >>> df.filter(df.height.isNotNull()).collect()\n",
      "     |      [Row(name='Tom', height=80)]\n",
      "     |  \n",
      "     |  isNull = _(self: 'Column') -> 'Column'\n",
      "     |      True if the current expression is null.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      "     |      >>> df.filter(df.height.isNull()).collect()\n",
      "     |      [Row(name='Alice', height=None)]\n",
      "     |  \n",
      "     |  isin(self, *cols: Any) -> 'Column'\n",
      "     |      A boolean expression that is evaluated to true if the value of this\n",
      "     |      expression is contained by the evaluated values of the arguments.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols\n",
      "     |          The result will only be true at a location if any value matches in the Column.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column of booleans showing whether each element in the Column is contained in cols.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n",
      "     |      [Row(age=5, name='Bob')]\n",
      "     |      >>> df[df.age.isin([1, 2, 3])].collect()\n",
      "     |      [Row(age=2, name='Alice')]\n",
      "     |  \n",
      "     |  like(self: 'Column', other: str) -> 'Column'\n",
      "     |      SQL like expression. Returns a boolean :class:`Column` based on a SQL LIKE match.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : str\n",
      "     |          a SQL LIKE pattern\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.Column.rlike\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column of booleans showing whether each element\n",
      "     |          in the Column is matched by SQL LIKE pattern.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.filter(df.name.like('Al%')).collect()\n",
      "     |      [Row(age=2, name='Alice')]\n",
      "     |  \n",
      "     |  name = alias(self, *alias, **kwargs)\n",
      "     |      :func:`name` is an alias for :func:`alias`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0\n",
      "     |  \n",
      "     |  otherwise(self, value: Any) -> 'Column'\n",
      "     |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "     |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      value\n",
      "     |          a literal value, or a :class:`Column` expression.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing whether each element of Column is unmatched conditions.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import functions as sf\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.select(df.name, sf.when(df.age > 3, 1).otherwise(0)).show()\n",
      "     |      +-----+-------------------------------------+\n",
      "     |      | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n",
      "     |      +-----+-------------------------------------+\n",
      "     |      |Alice|                                    0|\n",
      "     |      |  Bob|                                    1|\n",
      "     |      +-----+-------------------------------------+\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.functions.when\n",
      "     |  \n",
      "     |  over(self, window: 'WindowSpec') -> 'Column'\n",
      "     |      Define a windowing column.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      window : :class:`WindowSpec`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Window\n",
      "     |      >>> window = (\n",
      "     |      ...     Window.partitionBy(\"name\")\n",
      "     |      ...     .orderBy(\"age\")\n",
      "     |      ...     .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
      "     |      ... )\n",
      "     |      >>> from pyspark.sql.functions import rank, min, desc\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.withColumn(\n",
      "     |      ...      \"rank\", rank().over(window)\n",
      "     |      ... ).withColumn(\n",
      "     |      ...      \"min\", min('age').over(window)\n",
      "     |      ... ).sort(desc(\"age\")).show()\n",
      "     |      +---+-----+----+---+\n",
      "     |      |age| name|rank|min|\n",
      "     |      +---+-----+----+---+\n",
      "     |      |  5|  Bob|   1|  5|\n",
      "     |      |  2|Alice|   1|  2|\n",
      "     |      +---+-----+----+---+\n",
      "     |  \n",
      "     |  rlike(self: 'Column', other: str) -> 'Column'\n",
      "     |      SQL RLIKE expression (LIKE with Regex). Returns a boolean :class:`Column` based on a regex\n",
      "     |      match.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : str\n",
      "     |          an extended regex expression\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column of booleans showing whether each element\n",
      "     |          in the Column is matched by extended regex expression.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.filter(df.name.rlike('ice$')).collect()\n",
      "     |      [Row(age=2, name='Alice')]\n",
      "     |  \n",
      "     |  startswith = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      String starts with. Returns a boolean :class:`Column` based on a string match.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`Column` or str\n",
      "     |          string at start of line (do not use a regex `^`)\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.filter(df.name.startswith('Al')).collect()\n",
      "     |      [Row(age=2, name='Alice')]\n",
      "     |      >>> df.filter(df.name.startswith('^Al')).collect()\n",
      "     |      []\n",
      "     |  \n",
      "     |  substr(self, startPos: Union[int, ForwardRef('Column')], length: Union[int, ForwardRef('Column')]) -> 'Column'\n",
      "     |      Return a :class:`Column` which is a substring of the column.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      startPos : :class:`Column` or int\n",
      "     |          start position\n",
      "     |      length : :class:`Column` or int\n",
      "     |          length of the substring\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing whether each element of Column is substr of origin Column.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n",
      "     |      [Row(col='Ali'), Row(col='Bob')]\n",
      "     |  \n",
      "     |  when(self, condition: 'Column', value: Any) -> 'Column'\n",
      "     |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "     |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      condition : :class:`Column`\n",
      "     |          a boolean :class:`Column` expression.\n",
      "     |      value\n",
      "     |          a literal value, or a :class:`Column` expression.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing whether each element of Column is in conditions.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import functions as sf\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.select(df.name, sf.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()\n",
      "     |      +-----+------------------------------------------------------------+\n",
      "     |      | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\n",
      "     |      +-----+------------------------------------------------------------+\n",
      "     |      |Alice|                                                          -1|\n",
      "     |      |  Bob|                                                           1|\n",
      "     |      +-----+------------------------------------------------------------+\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.functions.when\n",
      "     |  \n",
      "     |  withField(self, fieldName: str, col: 'Column') -> 'Column'\n",
      "     |      An expression that adds/replaces a field in :class:`StructType` by name.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fieldName : str\n",
      "     |          a literal value.\n",
      "     |          The result will only be true at a location if any field matches in the Column.\n",
      "     |      col : :class:`Column`\n",
      "     |          A :class:`Column` expression for the column with `fieldName`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing whether each element of Column\n",
      "     |          which field was added/replaced by fieldName.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> from pyspark.sql.functions import lit\n",
      "     |      >>> df = spark.createDataFrame([Row(a=Row(b=1, c=2))])\n",
      "     |      >>> df.withColumn('a', df['a'].withField('b', lit(3))).select('a.b').show()\n",
      "     |      +---+\n",
      "     |      |  b|\n",
      "     |      +---+\n",
      "     |      |  3|\n",
      "     |      +---+\n",
      "     |      >>> df.withColumn('a', df['a'].withField('d', lit(4))).select('a.d').show()\n",
      "     |      +---+\n",
      "     |      |  d|\n",
      "     |      +---+\n",
      "     |      |  4|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
      "     |  DataFrame(jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[ForwardRef('SQLContext'), ForwardRef('SparkSession')])\n",
      "     |  \n",
      "     |  A distributed collection of data grouped into named columns.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.3.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
      "     |  and can be created using various functions in :class:`SparkSession`:\n",
      "     |  \n",
      "     |  >>> people = spark.createDataFrame([\n",
      "     |  ...     {\"deptId\": 1, \"age\": 40, \"name\": \"Hyukjin Kwon\", \"gender\": \"M\", \"salary\": 50},\n",
      "     |  ...     {\"deptId\": 1, \"age\": 50, \"name\": \"Takuya Ueshin\", \"gender\": \"M\", \"salary\": 100},\n",
      "     |  ...     {\"deptId\": 2, \"age\": 60, \"name\": \"Xinrong Meng\", \"gender\": \"F\", \"salary\": 150},\n",
      "     |  ...     {\"deptId\": 3, \"age\": 20, \"name\": \"Haejoon Lee\", \"gender\": \"M\", \"salary\": 200}\n",
      "     |  ... ])\n",
      "     |  \n",
      "     |  Once created, it can be manipulated using the various domain-specific-language\n",
      "     |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
      "     |  \n",
      "     |  To select a column from the :class:`DataFrame`, use the apply method:\n",
      "     |  \n",
      "     |  >>> age_col = people.age\n",
      "     |  \n",
      "     |  A more concrete example:\n",
      "     |  \n",
      "     |  >>> # To create DataFrame using SparkSession\n",
      "     |  ... department = spark.createDataFrame([\n",
      "     |  ...     {\"id\": 1, \"name\": \"PySpark\"},\n",
      "     |  ...     {\"id\": 2, \"name\": \"ML\"},\n",
      "     |  ...     {\"id\": 3, \"name\": \"Spark SQL\"}\n",
      "     |  ... ])\n",
      "     |  \n",
      "     |  >>> people.filter(people.age > 30).join(\n",
      "     |  ...     department, people.deptId == department.id).groupBy(\n",
      "     |  ...     department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"}).show()\n",
      "     |  +-------+------+-----------+--------+\n",
      "     |  |   name|gender|avg(salary)|max(age)|\n",
      "     |  +-------+------+-----------+--------+\n",
      "     |  |     ML|     F|      150.0|      60|\n",
      "     |  |PySpark|     M|       75.0|      50|\n",
      "     |  +-------+------+-----------+--------+\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  A DataFrame should only be created as described above. It should not be directly\n",
      "     |  created via using the constructor.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DataFrame\n",
      "     |      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n",
      "     |      pyspark.sql.pandas.conversion.PandasConversionMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __dir__(self) -> List[str]\n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import lit\n",
      "     |      \n",
      "     |      Create a dataframe with a column named 'id'.\n",
      "     |      \n",
      "     |      >>> df = spark.range(3)\n",
      "     |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Includes column id\n",
      "     |      ['id', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal', 'isStreaming']\n",
      "     |      \n",
      "     |      Add a column named 'i_like_pancakes'.\n",
      "     |      \n",
      "     |      >>> df = df.withColumn('i_like_pancakes', lit(1))\n",
      "     |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Includes columns i_like_pancakes, id\n",
      "     |      ['i_like_pancakes', 'id', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal']\n",
      "     |      \n",
      "     |      Try to add an existed column 'inputFiles'.\n",
      "     |      \n",
      "     |      >>> df = df.withColumn('inputFiles', lit(2))\n",
      "     |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Doesn't duplicate inputFiles\n",
      "     |      ['i_like_pancakes', 'id', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal']\n",
      "     |      \n",
      "     |      Try to add a column named 'id2'.\n",
      "     |      \n",
      "     |      >>> df = df.withColumn('id2', lit(3))\n",
      "     |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # result includes id2 and sorted\n",
      "     |      ['i_like_pancakes', 'id', 'id2', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty']\n",
      "     |      \n",
      "     |      Don't include columns that are not valid python identifiers.\n",
      "     |      \n",
      "     |      >>> df = df.withColumn('1', lit(4))\n",
      "     |      >>> df = df.withColumn('name 1', lit(5))\n",
      "     |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Doesn't include 1 or name 1\n",
      "     |      ['i_like_pancakes', 'id', 'id2', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty']\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> pyspark.sql.column.Column\n",
      "     |      Returns the :class:`Column` denoted by ``name``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          Column name to return as :class:`Column`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Requested column.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Retrieve a column instance.\n",
      "     |      \n",
      "     |      >>> df.select(df.age).show()\n",
      "     |      +---+\n",
      "     |      |age|\n",
      "     |      +---+\n",
      "     |      |  2|\n",
      "     |      |  5|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  __getitem__(self, item: Union[int, str, pyspark.sql.column.Column, List, Tuple]) -> Union[pyspark.sql.column.Column, ForwardRef('DataFrame')]\n",
      "     |      Returns the column as a :class:`Column`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      item : int, str, :class:`Column`, list or tuple\n",
      "     |          column index, column name, column, or a list or tuple of columns\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column` or :class:`DataFrame`\n",
      "     |          a specified column, or a filtered or projected dataframe.\n",
      "     |      \n",
      "     |          * If the input `item` is an int or str, the output is a :class:`Column`.\n",
      "     |      \n",
      "     |          * If the input `item` is a :class:`Column`, the output is a :class:`DataFrame`\n",
      "     |              filtered by this given :class:`Column`.\n",
      "     |      \n",
      "     |          * If the input `item` is a list or tuple, the output is a :class:`DataFrame`\n",
      "     |              projected by this given list or tuple.\n",
      "     |      \n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Retrieve a column instance.\n",
      "     |      \n",
      "     |      >>> df.select(df['age']).show()\n",
      "     |      +---+\n",
      "     |      |age|\n",
      "     |      +---+\n",
      "     |      |  2|\n",
      "     |      |  5|\n",
      "     |      +---+\n",
      "     |      \n",
      "     |      >>> df.select(df[1]).show()\n",
      "     |      +-----+\n",
      "     |      | name|\n",
      "     |      +-----+\n",
      "     |      |Alice|\n",
      "     |      |  Bob|\n",
      "     |      +-----+\n",
      "     |      \n",
      "     |      Select multiple string columns as index.\n",
      "     |      \n",
      "     |      >>> df[[\"name\", \"age\"]].show()\n",
      "     |      +-----+---+\n",
      "     |      | name|age|\n",
      "     |      +-----+---+\n",
      "     |      |Alice|  2|\n",
      "     |      |  Bob|  5|\n",
      "     |      +-----+---+\n",
      "     |      >>> df[df.age > 3].show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |  5| Bob|\n",
      "     |      +---+----+\n",
      "     |      >>> df[df[0] > 3].show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |  5| Bob|\n",
      "     |      +---+----+\n",
      "     |  \n",
      "     |  __init__(self, jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[ForwardRef('SQLContext'), ForwardRef('SparkSession')])\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  agg(self, *exprs: Union[pyspark.sql.column.Column, Dict[str, str]]) -> 'DataFrame'\n",
      "     |      Aggregate on the entire :class:`DataFrame` without groups\n",
      "     |      (shorthand for ``df.groupBy().agg()``).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      exprs : :class:`Column` or dict of key and value strings\n",
      "     |          Columns or expressions to aggregate DataFrame by.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Aggregated DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import functions as sf\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.agg({\"age\": \"max\"}).show()\n",
      "     |      +--------+\n",
      "     |      |max(age)|\n",
      "     |      +--------+\n",
      "     |      |       5|\n",
      "     |      +--------+\n",
      "     |      >>> df.agg(sf.min(df.age)).show()\n",
      "     |      +--------+\n",
      "     |      |min(age)|\n",
      "     |      +--------+\n",
      "     |      |       2|\n",
      "     |      +--------+\n",
      "     |  \n",
      "     |  alias(self, alias: str) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` with an alias set.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      alias : str\n",
      "     |          an alias name to be set for the :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Aliased DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import col, desc\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df_as1 = df.alias(\"df_as1\")\n",
      "     |      >>> df_as2 = df.alias(\"df_as2\")\n",
      "     |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
      "     |      >>> joined_df.select(\n",
      "     |      ...     \"df_as1.name\", \"df_as2.name\", \"df_as2.age\").sort(desc(\"df_as1.name\")).show()\n",
      "     |      +-----+-----+---+\n",
      "     |      | name| name|age|\n",
      "     |      +-----+-----+---+\n",
      "     |      |  Tom|  Tom| 14|\n",
      "     |      |  Bob|  Bob| 16|\n",
      "     |      |Alice|Alice| 23|\n",
      "     |      +-----+-----+---+\n",
      "     |  \n",
      "     |  approxQuantile(self, col: Union[str, List[str], Tuple[str]], probabilities: Union[List[float], Tuple[float]], relativeError: float) -> Union[List[float], List[List[float]]]\n",
      "     |      Calculates the approximate quantiles of numerical columns of a\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The result of this algorithm has the following deterministic bound:\n",
      "     |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
      "     |      probability `p` up to error `err`, then the algorithm will return\n",
      "     |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
      "     |      close to (p * N). More precisely,\n",
      "     |      \n",
      "     |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
      "     |      \n",
      "     |      This method implements a variation of the Greenwald-Khanna\n",
      "     |      algorithm (with some speed optimizations). The algorithm was first\n",
      "     |      present in [[https://doi.org/10.1145/375663.375670\n",
      "     |      Space-efficient Online Computation of Quantile Summaries]]\n",
      "     |      by Greenwald and Khanna.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col: str, tuple or list\n",
      "     |          Can be a single column name, or a list of names for multiple columns.\n",
      "     |      \n",
      "     |          .. versionchanged:: 2.2.0\n",
      "     |             Added support for multiple columns.\n",
      "     |      probabilities : list or tuple\n",
      "     |          a list of quantile probabilities\n",
      "     |          Each number must belong to [0, 1].\n",
      "     |          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
      "     |      relativeError : float\n",
      "     |          The relative target precision to achieve\n",
      "     |          (>= 0). If set to zero, the exact quantiles are computed, which\n",
      "     |          could be very expensive. Note that values greater than 1 are\n",
      "     |          accepted but gives the same result as 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          the approximate quantiles at the given probabilities.\n",
      "     |      \n",
      "     |          * If the input `col` is a string, the output is a list of floats.\n",
      "     |      \n",
      "     |          * If the input `col` is a list or tuple of strings, the output is also a\n",
      "     |              list, but each element in it is a list of floats, i.e., the output\n",
      "     |              is a list of list of floats.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Null values will be ignored in numerical columns before calculation.\n",
      "     |      For columns only containing null values, an empty list is returned.\n",
      "     |  \n",
      "     |  cache(self) -> 'DataFrame'\n",
      "     |      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Cached DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.range(1)\n",
      "     |      >>> df.cache()\n",
      "     |      DataFrame[id: bigint]\n",
      "     |      \n",
      "     |      >>> df.explain()\n",
      "     |      == Physical Plan ==\n",
      "     |      AdaptiveSparkPlan isFinalPlan=false\n",
      "     |      +- InMemoryTableScan ...\n",
      "     |  \n",
      "     |  checkpoint(self, eager: bool = True) -> 'DataFrame'\n",
      "     |      Returns a checkpointed version of this :class:`DataFrame`. Checkpointing can be used to\n",
      "     |      truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      "     |      iterative algorithms where the plan may grow exponentially. It will be saved to files\n",
      "     |      inside the checkpoint directory set with :meth:`SparkContext.setCheckpointDir`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      eager : bool, optional, default True\n",
      "     |          Whether to checkpoint this :class:`DataFrame` immediately.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Checkpointed DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is experimental.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import tempfile\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     spark.sparkContext.setCheckpointDir(\"/tmp/bb\")\n",
      "     |      ...     df.checkpoint(False)\n",
      "     |      DataFrame[age: bigint, name: string]\n",
      "     |  \n",
      "     |  coalesce(self, numPartitions: int) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
      "     |      \n",
      "     |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n",
      "     |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n",
      "     |      there will not be a shuffle, instead each of the 100 new partitions will\n",
      "     |      claim 10 of the current partitions. If a larger number of partitions is requested,\n",
      "     |      it will stay at the current number of partitions.\n",
      "     |      \n",
      "     |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n",
      "     |      this may result in your computation taking place on fewer nodes than\n",
      "     |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n",
      "     |      you can call repartition(). This will add a shuffle step, but means the\n",
      "     |      current upstream partitions will be executed in parallel (per whatever\n",
      "     |      the current partitioning is).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      numPartitions : int\n",
      "     |          specify the target number of partitions\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.range(10)\n",
      "     |      >>> df.coalesce(1).rdd.getNumPartitions()\n",
      "     |      1\n",
      "     |  \n",
      "     |  colRegex(self, colName: str) -> pyspark.sql.column.Column\n",
      "     |      Selects column based on the column name specified as a regex and returns it\n",
      "     |      as :class:`Column`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      colName : str\n",
      "     |          string, column name specified as a regex.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
      "     |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n",
      "     |      +----+\n",
      "     |      |Col2|\n",
      "     |      +----+\n",
      "     |      |   1|\n",
      "     |      |   2|\n",
      "     |      |   3|\n",
      "     |      +----+\n",
      "     |  \n",
      "     |  collect(self) -> List[pyspark.sql.types.Row]\n",
      "     |      Returns all the records as a list of :class:`Row`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          List of rows.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.collect()\n",
      "     |      [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
      "     |  \n",
      "     |  corr(self, col1: str, col2: str, method: Optional[str] = None) -> float\n",
      "     |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
      "     |      Currently only supports the Pearson Correlation Coefficient.\n",
      "     |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col1 : str\n",
      "     |          The name of the first column\n",
      "     |      col2 : str\n",
      "     |          The name of the second column\n",
      "     |      method : str, optional\n",
      "     |          The correlation method. Currently only supports \"pearson\"\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float\n",
      "     |          Pearson Correlation Coefficient of two columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
      "     |      >>> df.corr(\"c1\", \"c2\")\n",
      "     |      -0.3592106040535498\n",
      "     |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
      "     |      >>> df.corr(\"small\", \"bigger\")\n",
      "     |      1.0\n",
      "     |  \n",
      "     |  count(self) -> int\n",
      "     |      Returns the number of rows in this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          Number of rows.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Return the number of rows in the :class:`DataFrame`.\n",
      "     |      \n",
      "     |      >>> df.count()\n",
      "     |      3\n",
      "     |  \n",
      "     |  cov(self, col1: str, col2: str) -> float\n",
      "     |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
      "     |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col1 : str\n",
      "     |          The name of the first column\n",
      "     |      col2 : str\n",
      "     |          The name of the second column\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float\n",
      "     |          Covariance of two columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
      "     |      >>> df.cov(\"c1\", \"c2\")\n",
      "     |      -18.0\n",
      "     |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
      "     |      >>> df.cov(\"small\", \"bigger\")\n",
      "     |      1.0\n",
      "     |  \n",
      "     |  createGlobalTempView(self, name: str) -> None\n",
      "     |      Creates a global temporary view with this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The lifetime of this temporary view is tied to this Spark application.\n",
      "     |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      "     |      catalog.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          Name of the view.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Create a global temporary view.\n",
      "     |      \n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.createGlobalTempView(\"people\")\n",
      "     |      >>> df2 = spark.sql(\"SELECT * FROM global_temp.people\")\n",
      "     |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |      \n",
      "     |      Throws an exception if the global temporary view already exists.\n",
      "     |      \n",
      "     |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "     |      Traceback (most recent call last):\n",
      "     |      ...\n",
      "     |      AnalysisException: \"Temporary table 'people' already exists;\"\n",
      "     |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      "     |      True\n",
      "     |  \n",
      "     |  createOrReplaceGlobalTempView(self, name: str) -> None\n",
      "     |      Creates or replaces a global temporary view using the given name.\n",
      "     |      \n",
      "     |      The lifetime of this temporary view is tied to this Spark application.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.2.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          Name of the view.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Create a global temporary view.\n",
      "     |      \n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.createOrReplaceGlobalTempView(\"people\")\n",
      "     |      \n",
      "     |      Replace the global temporary view.\n",
      "     |      \n",
      "     |      >>> df2 = df.filter(df.age > 3)\n",
      "     |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n",
      "     |      >>> df3 = spark.sql(\"SELECT * FROM global_temp.people\")\n",
      "     |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      "     |      True\n",
      "     |  \n",
      "     |  createOrReplaceTempView(self, name: str) -> None\n",
      "     |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      "     |      that was used to create this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          Name of the view.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Create a local temporary view named 'people'.\n",
      "     |      \n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.createOrReplaceTempView(\"people\")\n",
      "     |      \n",
      "     |      Replace the local temporary view.\n",
      "     |      \n",
      "     |      >>> df2 = df.filter(df.age > 3)\n",
      "     |      >>> df2.createOrReplaceTempView(\"people\")\n",
      "     |      >>> df3 = spark.sql(\"SELECT * FROM people\")\n",
      "     |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |      >>> spark.catalog.dropTempView(\"people\")\n",
      "     |      True\n",
      "     |  \n",
      "     |  createTempView(self, name: str) -> None\n",
      "     |      Creates a local temporary view with this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      "     |      that was used to create this :class:`DataFrame`.\n",
      "     |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      "     |      catalog.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          Name of the view.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Create a local temporary view.\n",
      "     |      \n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.createTempView(\"people\")\n",
      "     |      >>> df2 = spark.sql(\"SELECT * FROM people\")\n",
      "     |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |      \n",
      "     |      Throw an exception if the table already exists.\n",
      "     |      \n",
      "     |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "     |      Traceback (most recent call last):\n",
      "     |      ...\n",
      "     |      AnalysisException: \"Temporary table 'people' already exists;\"\n",
      "     |      >>> spark.catalog.dropTempView(\"people\")\n",
      "     |      True\n",
      "     |  \n",
      "     |  crossJoin(self, other: 'DataFrame') -> 'DataFrame'\n",
      "     |      Returns the cartesian product with another :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          Right side of the cartesian product.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Joined DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df2 = spark.createDataFrame(\n",
      "     |      ...     [Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      "     |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").show()\n",
      "     |      +---+-----+------+\n",
      "     |      |age| name|height|\n",
      "     |      +---+-----+------+\n",
      "     |      | 14|  Tom|    80|\n",
      "     |      | 14|  Tom|    85|\n",
      "     |      | 23|Alice|    80|\n",
      "     |      | 23|Alice|    85|\n",
      "     |      | 16|  Bob|    80|\n",
      "     |      | 16|  Bob|    85|\n",
      "     |      +---+-----+------+\n",
      "     |  \n",
      "     |  crosstab(self, col1: str, col2: str) -> 'DataFrame'\n",
      "     |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
      "     |      table.\n",
      "     |      The first column of each row will be the distinct values of `col1` and the column names\n",
      "     |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
      "     |      Pairs that have no occurrences will have zero as their counts.\n",
      "     |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col1 : str\n",
      "     |          The name of the first column. Distinct items will make the first item of\n",
      "     |          each row.\n",
      "     |      col2 : str\n",
      "     |          The name of the second column. Distinct items will make the column names\n",
      "     |          of the :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Frequency matrix of two columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
      "     |      >>> df.crosstab(\"c1\", \"c2\").sort(\"c1_c2\").show()\n",
      "     |      +-----+---+---+---+\n",
      "     |      |c1_c2| 10| 11|  8|\n",
      "     |      +-----+---+---+---+\n",
      "     |      |    1|  0|  2|  0|\n",
      "     |      |    3|  1|  0|  0|\n",
      "     |      |    4|  0|  0|  2|\n",
      "     |      +-----+---+---+---+\n",
      "     |  \n",
      "     |  cube(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
      "     |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
      "     |      the specified columns, so we can run aggregations on them.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : list, str or :class:`Column`\n",
      "     |          columns to create cube by.\n",
      "     |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
      "     |          or list of them.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`GroupedData`\n",
      "     |          Cube of the data by given columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      "     |      +-----+----+-----+\n",
      "     |      | name| age|count|\n",
      "     |      +-----+----+-----+\n",
      "     |      | NULL|NULL|    2|\n",
      "     |      | NULL|   2|    1|\n",
      "     |      | NULL|   5|    1|\n",
      "     |      |Alice|NULL|    1|\n",
      "     |      |Alice|   2|    1|\n",
      "     |      |  Bob|NULL|    1|\n",
      "     |      |  Bob|   5|    1|\n",
      "     |      +-----+----+-----+\n",
      "     |  \n",
      "     |  describe(self, *cols: Union[str, List[str]]) -> 'DataFrame'\n",
      "     |      Computes basic statistics for numeric and string columns.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      This includes count, mean, stddev, min, and max. If no columns are\n",
      "     |      given, this function computes statistics for all numerical or string columns.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This function is meant for exploratory data analysis, as we make no\n",
      "     |      guarantee about the backward compatibility of the schema of the resulting\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Use summary for expanded statistics and control over which statistics to compute.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, list, optional\n",
      "     |           Column name or list of column names to describe by (default All columns).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          A new DataFrame that describes (provides statistics) given DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
      "     |      ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
      "     |      ... )\n",
      "     |      >>> df.describe(['age']).show()\n",
      "     |      +-------+----+\n",
      "     |      |summary| age|\n",
      "     |      +-------+----+\n",
      "     |      |  count|   3|\n",
      "     |      |   mean|12.0|\n",
      "     |      | stddev| 1.0|\n",
      "     |      |    min|  11|\n",
      "     |      |    max|  13|\n",
      "     |      +-------+----+\n",
      "     |      \n",
      "     |      >>> df.describe(['age', 'weight', 'height']).show()\n",
      "     |      +-------+----+------------------+-----------------+\n",
      "     |      |summary| age|            weight|           height|\n",
      "     |      +-------+----+------------------+-----------------+\n",
      "     |      |  count|   3|                 3|                3|\n",
      "     |      |   mean|12.0| 40.73333333333333|            145.0|\n",
      "     |      | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
      "     |      |    min|  11|              37.8|            142.2|\n",
      "     |      |    max|  13|              44.1|            150.5|\n",
      "     |      +-------+----+------------------+-----------------+\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      DataFrame.summary\n",
      "     |  \n",
      "     |  distinct(self) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with distinct records.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (23, \"Alice\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Return the number of distinct rows in the :class:`DataFrame`\n",
      "     |      \n",
      "     |      >>> df.distinct().count()\n",
      "     |      2\n",
      "     |  \n",
      "     |  drop(self, *cols: 'ColumnOrName') -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` without specified columns.\n",
      "     |      This is a no-op if the schema doesn't contain the given column name(s).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols: str or :class:`Column`\n",
      "     |          a name of the column, or the :class:`Column` to drop\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame without given columns.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      When an input is a column name, it is treated literally without further interpretation.\n",
      "     |      Otherwise, will try to match the equivalent expression.\n",
      "     |      So that dropping column by its name `drop(colName)` has different semantic with directly\n",
      "     |      dropping the column `drop(col(colName))`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> from pyspark.sql.functions import col, lit\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      "     |      \n",
      "     |      >>> df.drop('age').show()\n",
      "     |      +-----+\n",
      "     |      | name|\n",
      "     |      +-----+\n",
      "     |      |  Tom|\n",
      "     |      |Alice|\n",
      "     |      |  Bob|\n",
      "     |      +-----+\n",
      "     |      >>> df.drop(df.age).show()\n",
      "     |      +-----+\n",
      "     |      | name|\n",
      "     |      +-----+\n",
      "     |      |  Tom|\n",
      "     |      |Alice|\n",
      "     |      |  Bob|\n",
      "     |      +-----+\n",
      "     |      \n",
      "     |      Drop the column that joined both DataFrames on.\n",
      "     |      \n",
      "     |      >>> df.join(df2, df.name == df2.name, 'inner').drop('name').sort('age').show()\n",
      "     |      +---+------+\n",
      "     |      |age|height|\n",
      "     |      +---+------+\n",
      "     |      | 14|    80|\n",
      "     |      | 16|    85|\n",
      "     |      +---+------+\n",
      "     |      \n",
      "     |      >>> df3 = df.join(df2)\n",
      "     |      >>> df3.show()\n",
      "     |      +---+-----+------+----+\n",
      "     |      |age| name|height|name|\n",
      "     |      +---+-----+------+----+\n",
      "     |      | 14|  Tom|    80| Tom|\n",
      "     |      | 14|  Tom|    85| Bob|\n",
      "     |      | 23|Alice|    80| Tom|\n",
      "     |      | 23|Alice|    85| Bob|\n",
      "     |      | 16|  Bob|    80| Tom|\n",
      "     |      | 16|  Bob|    85| Bob|\n",
      "     |      +---+-----+------+----+\n",
      "     |      \n",
      "     |      Drop two column by the same name.\n",
      "     |      \n",
      "     |      >>> df3.drop(\"name\").show()\n",
      "     |      +---+------+\n",
      "     |      |age|height|\n",
      "     |      +---+------+\n",
      "     |      | 14|    80|\n",
      "     |      | 14|    85|\n",
      "     |      | 23|    80|\n",
      "     |      | 23|    85|\n",
      "     |      | 16|    80|\n",
      "     |      | 16|    85|\n",
      "     |      +---+------+\n",
      "     |      \n",
      "     |      Can not drop col('name') due to ambiguous reference.\n",
      "     |      \n",
      "     |      >>> df3.drop(col(\"name\")).show()\n",
      "     |      Traceback (most recent call last):\n",
      "     |      ...\n",
      "     |      pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference...\n",
      "     |      \n",
      "     |      >>> df4 = df.withColumn(\"a.b.c\", lit(1))\n",
      "     |      >>> df4.show()\n",
      "     |      +---+-----+-----+\n",
      "     |      |age| name|a.b.c|\n",
      "     |      +---+-----+-----+\n",
      "     |      | 14|  Tom|    1|\n",
      "     |      | 23|Alice|    1|\n",
      "     |      | 16|  Bob|    1|\n",
      "     |      +---+-----+-----+\n",
      "     |      \n",
      "     |      >>> df4.drop(\"a.b.c\").show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      | 14|  Tom|\n",
      "     |      | 23|Alice|\n",
      "     |      | 16|  Bob|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Can not find a column matching the expression \"a.b.c\".\n",
      "     |      \n",
      "     |      >>> df4.drop(col(\"a.b.c\")).show()\n",
      "     |      +---+-----+-----+\n",
      "     |      |age| name|a.b.c|\n",
      "     |      +---+-----+-----+\n",
      "     |      | 14|  Tom|    1|\n",
      "     |      | 23|Alice|    1|\n",
      "     |      | 16|  Bob|    1|\n",
      "     |      +---+-----+-----+\n",
      "     |  \n",
      "     |  dropDuplicates(self, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
      "     |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
      "     |      optionally only considering certain columns.\n",
      "     |      \n",
      "     |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      "     |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      "     |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      "     |      be and the system will accordingly limit the state. In addition, data older than\n",
      "     |      watermark will be dropped to avoid any possibility of duplicates.\n",
      "     |      \n",
      "     |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      subset : List of column names, optional\n",
      "     |          List of columns to use for duplicate comparison (default All columns).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame without duplicates.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     Row(name='Alice', age=5, height=80),\n",
      "     |      ...     Row(name='Alice', age=5, height=80),\n",
      "     |      ...     Row(name='Alice', age=10, height=80)\n",
      "     |      ... ])\n",
      "     |      \n",
      "     |      Deduplicate the same rows.\n",
      "     |      \n",
      "     |      >>> df.dropDuplicates().show()\n",
      "     |      +-----+---+------+\n",
      "     |      | name|age|height|\n",
      "     |      +-----+---+------+\n",
      "     |      |Alice|  5|    80|\n",
      "     |      |Alice| 10|    80|\n",
      "     |      +-----+---+------+\n",
      "     |      \n",
      "     |      Deduplicate values on 'name' and 'height' columns.\n",
      "     |      \n",
      "     |      >>> df.dropDuplicates(['name', 'height']).show()\n",
      "     |      +-----+---+------+\n",
      "     |      | name|age|height|\n",
      "     |      +-----+---+------+\n",
      "     |      |Alice|  5|    80|\n",
      "     |      +-----+---+------+\n",
      "     |  \n",
      "     |  dropDuplicatesWithinWatermark(self, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
      "     |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
      "     |       optionally only considering certain columns, within watermark.\n",
      "     |      \n",
      "     |       This only works with streaming :class:`DataFrame`, and watermark for the input\n",
      "     |       :class:`DataFrame` must be set via :func:`withWatermark`.\n",
      "     |      \n",
      "     |      For a streaming :class:`DataFrame`, this will keep all data across triggers as intermediate\n",
      "     |      state to drop duplicated rows. The state will be kept to guarantee the semantic, \"Events\n",
      "     |      are deduplicated as long as the time distance of earliest and latest events are smaller\n",
      "     |      than the delay threshold of watermark.\" Users are encouraged to set the delay threshold of\n",
      "     |      watermark longer than max timestamp differences among duplicated events.\n",
      "     |      \n",
      "     |      Note: too late data older than watermark will be dropped.\n",
      "     |      \n",
      "     |       .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |       Parameters\n",
      "     |       ----------\n",
      "     |       subset : List of column names, optional\n",
      "     |           List of columns to use for duplicate comparison (default All columns).\n",
      "     |      \n",
      "     |       Returns\n",
      "     |       -------\n",
      "     |       :class:`DataFrame`\n",
      "     |           DataFrame without duplicates.\n",
      "     |      \n",
      "     |       Notes\n",
      "     |       -----\n",
      "     |       Supports Spark Connect.\n",
      "     |      \n",
      "     |       Examples\n",
      "     |       --------\n",
      "     |       >>> from pyspark.sql import Row\n",
      "     |       >>> from pyspark.sql.functions import timestamp_seconds\n",
      "     |       >>> df = spark.readStream.format(\"rate\").load().selectExpr(\n",
      "     |       ...     \"value % 5 AS value\", \"timestamp\")\n",
      "     |       >>> df.select(\"value\", df.timestamp.alias(\"time\")).withWatermark(\"time\", '10 minutes')\n",
      "     |       DataFrame[value: bigint, time: timestamp]\n",
      "     |      \n",
      "     |       Deduplicate the same rows.\n",
      "     |      \n",
      "     |       >>> df.dropDuplicatesWithinWatermark() # doctest: +SKIP\n",
      "     |      \n",
      "     |       Deduplicate values on 'value' columns.\n",
      "     |      \n",
      "     |       >>> df.dropDuplicatesWithinWatermark(['value'])  # doctest: +SKIP\n",
      "     |  \n",
      "     |  drop_duplicates = dropDuplicates(self, subset=None)\n",
      "     |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  dropna(self, how: str = 'any', thresh: Optional[int] = None, subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      "     |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      how : str, optional\n",
      "     |          'any' or 'all'.\n",
      "     |          If 'any', drop a row if it contains any nulls.\n",
      "     |          If 'all', drop a row only if all its values are null.\n",
      "     |      thresh: int, optional\n",
      "     |          default None\n",
      "     |          If specified, drop rows that have less than `thresh` non-null values.\n",
      "     |          This overwrites the `how` parameter.\n",
      "     |      subset : str, tuple or list, optional\n",
      "     |          optional list of column names to consider.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with null only rows excluded.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
      "     |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
      "     |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
      "     |      ...     Row(age=None, height=None, name=None),\n",
      "     |      ... ])\n",
      "     |      >>> df.na.drop().show()\n",
      "     |      +---+------+-----+\n",
      "     |      |age|height| name|\n",
      "     |      +---+------+-----+\n",
      "     |      | 10|    80|Alice|\n",
      "     |      +---+------+-----+\n",
      "     |  \n",
      "     |  exceptAll(self, other: 'DataFrame') -> 'DataFrame'\n",
      "     |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
      "     |      not in another :class:`DataFrame` while preserving duplicates.\n",
      "     |      \n",
      "     |      This is equivalent to `EXCEPT ALL` in SQL.\n",
      "     |      As standard in SQL, this function resolves columns by position (not by name).\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          The other :class:`DataFrame` to compare to.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df1 = spark.createDataFrame(\n",
      "     |      ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      "     |      >>> df1.exceptAll(df2).show()\n",
      "     |      +---+---+\n",
      "     |      | C1| C2|\n",
      "     |      +---+---+\n",
      "     |      |  a|  1|\n",
      "     |      |  a|  1|\n",
      "     |      |  a|  2|\n",
      "     |      |  c|  4|\n",
      "     |      +---+---+\n",
      "     |  \n",
      "     |  explain(self, extended: Union[bool, str, NoneType] = None, mode: Optional[str] = None) -> None\n",
      "     |      Prints the (logical and physical) plans to the console for debugging purposes.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      extended : bool, optional\n",
      "     |          default ``False``. If ``False``, prints only the physical plan.\n",
      "     |          When this is a string without specifying the ``mode``, it works as the mode is\n",
      "     |          specified.\n",
      "     |      mode : str, optional\n",
      "     |          specifies the expected output format of plans.\n",
      "     |      \n",
      "     |          * ``simple``: Print only a physical plan.\n",
      "     |          * ``extended``: Print both logical and physical plans.\n",
      "     |          * ``codegen``: Print a physical plan and generated codes if they are available.\n",
      "     |          * ``cost``: Print a logical plan and statistics if they are available.\n",
      "     |          * ``formatted``: Split explain output into two sections: a physical plan outline                 and node details.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.0.0\n",
      "     |             Added optional argument `mode` to specify the expected output format of plans.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Print out the physical plan only (default).\n",
      "     |      \n",
      "     |      >>> df.explain()  # doctest: +SKIP\n",
      "     |      == Physical Plan ==\n",
      "     |      *(1) Scan ExistingRDD[age...,name...]\n",
      "     |      \n",
      "     |      Print out all of the parsed, analyzed, optimized and physical plans.\n",
      "     |      \n",
      "     |      >>> df.explain(True)\n",
      "     |      == Parsed Logical Plan ==\n",
      "     |      ...\n",
      "     |      == Analyzed Logical Plan ==\n",
      "     |      ...\n",
      "     |      == Optimized Logical Plan ==\n",
      "     |      ...\n",
      "     |      == Physical Plan ==\n",
      "     |      ...\n",
      "     |      \n",
      "     |      Print out the plans with two sections: a physical plan outline and node details\n",
      "     |      \n",
      "     |      >>> df.explain(mode=\"formatted\")  # doctest: +SKIP\n",
      "     |      == Physical Plan ==\n",
      "     |      * Scan ExistingRDD (...)\n",
      "     |      (1) Scan ExistingRDD [codegen id : ...]\n",
      "     |      Output [2]: [age..., name...]\n",
      "     |      ...\n",
      "     |      \n",
      "     |      Print a logical plan and statistics if they are available.\n",
      "     |      \n",
      "     |      >>> df.explain(\"cost\")\n",
      "     |      == Optimized Logical Plan ==\n",
      "     |      ...Statistics...\n",
      "     |      ...\n",
      "     |  \n",
      "     |  fillna(self, value: Union[ForwardRef('LiteralType'), Dict[str, ForwardRef('LiteralType')]], subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> 'DataFrame'\n",
      "     |      Replace null values, alias for ``na.fill()``.\n",
      "     |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      value : int, float, string, bool or dict\n",
      "     |          Value to replace null values with.\n",
      "     |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      "     |          from column name (string) to replacement value. The replacement value must be\n",
      "     |          an int, float, boolean, or string.\n",
      "     |      subset : str, tuple or list, optional\n",
      "     |          optional list of column names to consider.\n",
      "     |          Columns specified in subset that do not have matching data types are ignored.\n",
      "     |          For example, if `value` is a string, and subset contains a non-string column,\n",
      "     |          then the non-string column is simply ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with replaced null values.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (10, 80.5, \"Alice\", None),\n",
      "     |      ...     (5, None, \"Bob\", None),\n",
      "     |      ...     (None, None, \"Tom\", None),\n",
      "     |      ...     (None, None, None, True)],\n",
      "     |      ...     schema=[\"age\", \"height\", \"name\", \"bool\"])\n",
      "     |      \n",
      "     |      Fill all null values with 50 for numeric columns.\n",
      "     |      \n",
      "     |      >>> df.na.fill(50).show()\n",
      "     |      +---+------+-----+----+\n",
      "     |      |age|height| name|bool|\n",
      "     |      +---+------+-----+----+\n",
      "     |      | 10|  80.5|Alice|NULL|\n",
      "     |      |  5|  50.0|  Bob|NULL|\n",
      "     |      | 50|  50.0|  Tom|NULL|\n",
      "     |      | 50|  50.0| NULL|true|\n",
      "     |      +---+------+-----+----+\n",
      "     |      \n",
      "     |      Fill all null values with ``False`` for boolean columns.\n",
      "     |      \n",
      "     |      >>> df.na.fill(False).show()\n",
      "     |      +----+------+-----+-----+\n",
      "     |      | age|height| name| bool|\n",
      "     |      +----+------+-----+-----+\n",
      "     |      |  10|  80.5|Alice|false|\n",
      "     |      |   5|  NULL|  Bob|false|\n",
      "     |      |NULL|  NULL|  Tom|false|\n",
      "     |      |NULL|  NULL| NULL| true|\n",
      "     |      +----+------+-----+-----+\n",
      "     |      \n",
      "     |      Fill all null values with to 50 and \"unknown\" for 'age' and 'name' column respectively.\n",
      "     |      \n",
      "     |      >>> df.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      "     |      +---+------+-------+----+\n",
      "     |      |age|height|   name|bool|\n",
      "     |      +---+------+-------+----+\n",
      "     |      | 10|  80.5|  Alice|NULL|\n",
      "     |      |  5|  NULL|    Bob|NULL|\n",
      "     |      | 50|  NULL|    Tom|NULL|\n",
      "     |      | 50|  NULL|unknown|true|\n",
      "     |      +---+------+-------+----+\n",
      "     |  \n",
      "     |  filter(self, condition: 'ColumnOrName') -> 'DataFrame'\n",
      "     |      Filters rows using the given condition.\n",
      "     |      \n",
      "     |      :func:`where` is an alias for :func:`filter`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      condition : :class:`Column` or str\n",
      "     |          a :class:`Column` of :class:`types.BooleanType`\n",
      "     |          or a string of SQL expressions.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Filtered DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Filter by :class:`Column` instances.\n",
      "     |      \n",
      "     |      >>> df.filter(df.age > 3).show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |  5| Bob|\n",
      "     |      +---+----+\n",
      "     |      >>> df.where(df.age == 2).show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  2|Alice|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Filter by SQL expression in a string.\n",
      "     |      \n",
      "     |      >>> df.filter(\"age > 3\").show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |  5| Bob|\n",
      "     |      +---+----+\n",
      "     |      >>> df.where(\"age = 2\").show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  2|Alice|\n",
      "     |      +---+-----+\n",
      "     |  \n",
      "     |  first(self) -> Optional[pyspark.sql.types.Row]\n",
      "     |      Returns the first row as a :class:`Row`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Row`\n",
      "     |          First row if :class:`DataFrame` is not empty, otherwise ``None``.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.first()\n",
      "     |      Row(age=2, name='Alice')\n",
      "     |  \n",
      "     |  foreach(self, f: Callable[[pyspark.sql.types.Row], NoneType]) -> None\n",
      "     |      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      This is a shorthand for ``df.rdd.foreach()``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |          A function that accepts one parameter which will\n",
      "     |          receive each row to process.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> def func(person):\n",
      "     |      ...     print(person.name)\n",
      "     |      ...\n",
      "     |      >>> df.foreach(func)\n",
      "     |  \n",
      "     |  foreachPartition(self, f: Callable[[Iterator[pyspark.sql.types.Row]], NoneType]) -> None\n",
      "     |      Applies the ``f`` function to each partition of this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      This a shorthand for ``df.rdd.foreachPartition()``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |          A function that accepts one parameter which will receive\n",
      "     |          each partition to process.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> def func(itr):\n",
      "     |      ...     for person in itr:\n",
      "     |      ...         print(person.name)\n",
      "     |      ...\n",
      "     |      >>> df.foreachPartition(func)\n",
      "     |  \n",
      "     |  freqItems(self, cols: Union[List[str], Tuple[str]], support: Optional[float] = None) -> 'DataFrame'\n",
      "     |      Finding frequent items for columns, possibly with false positives. Using the\n",
      "     |      frequent element count algorithm described in\n",
      "     |      \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
      "     |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : list or tuple\n",
      "     |          Names of the columns to calculate frequent items for as a list or tuple of\n",
      "     |          strings.\n",
      "     |      support : float, optional\n",
      "     |          The frequency with which to consider an item 'frequent'. Default is 1%.\n",
      "     |          The support must be greater than 1e-4.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with frequent items.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This function is meant for exploratory data analysis, as we make no\n",
      "     |      guarantee about the backward compatibility of the schema of the resulting\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
      "     |      >>> df.freqItems([\"c1\", \"c2\"]).show()  # doctest: +SKIP\n",
      "     |      +------------+------------+\n",
      "     |      |c1_freqItems|c2_freqItems|\n",
      "     |      +------------+------------+\n",
      "     |      |   [4, 1, 3]| [8, 11, 10]|\n",
      "     |      +------------+------------+\n",
      "     |  \n",
      "     |  groupBy(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
      "     |      Groups the :class:`DataFrame` using the specified columns,\n",
      "     |      so we can run aggregation on them. See :class:`GroupedData`\n",
      "     |      for all the available aggregate functions.\n",
      "     |      \n",
      "     |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : list, str or :class:`Column`\n",
      "     |          columns to group by.\n",
      "     |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
      "     |          or list of them.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`GroupedData`\n",
      "     |          Grouped data by given columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (2, \"Bob\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Empty grouping columns triggers a global aggregation.\n",
      "     |      \n",
      "     |      >>> df.groupBy().avg().show()\n",
      "     |      +--------+\n",
      "     |      |avg(age)|\n",
      "     |      +--------+\n",
      "     |      |    2.75|\n",
      "     |      +--------+\n",
      "     |      \n",
      "     |      Group-by 'name', and specify a dictionary to calculate the summation of 'age'.\n",
      "     |      \n",
      "     |      >>> df.groupBy(\"name\").agg({\"age\": \"sum\"}).sort(\"name\").show()\n",
      "     |      +-----+--------+\n",
      "     |      | name|sum(age)|\n",
      "     |      +-----+--------+\n",
      "     |      |Alice|       2|\n",
      "     |      |  Bob|       9|\n",
      "     |      +-----+--------+\n",
      "     |      \n",
      "     |      Group-by 'name', and calculate maximum values.\n",
      "     |      \n",
      "     |      >>> df.groupBy(df.name).max().sort(\"name\").show()\n",
      "     |      +-----+--------+\n",
      "     |      | name|max(age)|\n",
      "     |      +-----+--------+\n",
      "     |      |Alice|       2|\n",
      "     |      |  Bob|       5|\n",
      "     |      +-----+--------+\n",
      "     |      \n",
      "     |      Group-by 'name' and 'age', and calculate the number of rows in each group.\n",
      "     |      \n",
      "     |      >>> df.groupBy([\"name\", df.age]).count().sort(\"name\", \"age\").show()\n",
      "     |      +-----+---+-----+\n",
      "     |      | name|age|count|\n",
      "     |      +-----+---+-----+\n",
      "     |      |Alice|  2|    1|\n",
      "     |      |  Bob|  2|    2|\n",
      "     |      |  Bob|  5|    1|\n",
      "     |      +-----+---+-----+\n",
      "     |  \n",
      "     |  groupby = groupBy(self, *cols)\n",
      "     |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  head(self, n: Optional[int] = None) -> Union[pyspark.sql.types.Row, NoneType, List[pyspark.sql.types.Row]]\n",
      "     |      Returns the first ``n`` rows.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method should only be used if the resulting array is expected\n",
      "     |      to be small, as all the data is loaded into the driver's memory.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int, optional\n",
      "     |          default 1. Number of rows to return.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      If n is greater than 1, return a list of :class:`Row`.\n",
      "     |      If n is 1, return a single Row.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.head()\n",
      "     |      Row(age=2, name='Alice')\n",
      "     |      >>> df.head(1)\n",
      "     |      [Row(age=2, name='Alice')]\n",
      "     |  \n",
      "     |  hint(self, name: str, *parameters: Union[ForwardRef('PrimitiveType'), List[ForwardRef('PrimitiveType')]]) -> 'DataFrame'\n",
      "     |      Specifies some hint on the current :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.2.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          A name of the hint.\n",
      "     |      parameters : str, list, float or int\n",
      "     |          Optional parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Hinted DataFrame\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      "     |      >>> df.join(df2, \"name\").explain()  # doctest: +SKIP\n",
      "     |      == Physical Plan ==\n",
      "     |      ...\n",
      "     |      ... +- SortMergeJoin ...\n",
      "     |      ...\n",
      "     |      \n",
      "     |      Explicitly trigger the broadcast hashjoin by providing the hint in ``df2``.\n",
      "     |      \n",
      "     |      >>> df.join(df2.hint(\"broadcast\"), \"name\").explain()\n",
      "     |      == Physical Plan ==\n",
      "     |      ...\n",
      "     |      ... +- BroadcastHashJoin ...\n",
      "     |      ...\n",
      "     |  \n",
      "     |  inputFiles(self) -> List[str]\n",
      "     |      Returns a best-effort snapshot of the files that compose this :class:`DataFrame`.\n",
      "     |      This method simply asks each constituent BaseRelation for its respective files and\n",
      "     |      takes the union of all results. Depending on the source relations, this may not find\n",
      "     |      all input files. Duplicates are removed.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          List of file paths.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a single-row DataFrame into a JSON file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).repartition(1).write.json(d, mode=\"overwrite\")\n",
      "     |      ...\n",
      "     |      ...     # Read the JSON file as a DataFrame.\n",
      "     |      ...     df = spark.read.format(\"json\").load(d)\n",
      "     |      ...\n",
      "     |      ...     # Returns the number of input files.\n",
      "     |      ...     len(df.inputFiles())\n",
      "     |      1\n",
      "     |  \n",
      "     |  intersect(self, other: 'DataFrame') -> 'DataFrame'\n",
      "     |      Return a new :class:`DataFrame` containing rows only in\n",
      "     |      both this :class:`DataFrame` and another :class:`DataFrame`.\n",
      "     |      Note that any duplicates are removed. To preserve duplicates\n",
      "     |      use :func:`intersectAll`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          Another :class:`DataFrame` that needs to be combined.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Combined DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This is equivalent to `INTERSECT` in SQL.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      "     |      >>> df1.intersect(df2).sort(df1.C1.desc()).show()\n",
      "     |      +---+---+\n",
      "     |      | C1| C2|\n",
      "     |      +---+---+\n",
      "     |      |  b|  3|\n",
      "     |      |  a|  1|\n",
      "     |      +---+---+\n",
      "     |  \n",
      "     |  intersectAll(self, other: 'DataFrame') -> 'DataFrame'\n",
      "     |      Return a new :class:`DataFrame` containing rows in both this :class:`DataFrame`\n",
      "     |      and another :class:`DataFrame` while preserving duplicates.\n",
      "     |      \n",
      "     |      This is equivalent to `INTERSECT ALL` in SQL. As standard in SQL, this function\n",
      "     |      resolves columns by position (not by name).\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          Another :class:`DataFrame` that needs to be combined.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Combined DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      "     |      >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n",
      "     |      +---+---+\n",
      "     |      | C1| C2|\n",
      "     |      +---+---+\n",
      "     |      |  a|  1|\n",
      "     |      |  a|  1|\n",
      "     |      |  b|  3|\n",
      "     |      +---+---+\n",
      "     |  \n",
      "     |  isEmpty(self) -> bool\n",
      "     |      Checks if the :class:`DataFrame` is empty and returns a boolean value.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          Returns ``True`` if the DataFrame is empty, ``False`` otherwise.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      DataFrame.count : Counts the number of rows in DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      - Unlike `count()`, this method does not trigger any computation.\n",
      "     |      - An empty DataFrame has no rows. It may have columns, but no data.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Example 1: Checking if an empty DataFrame is empty\n",
      "     |      \n",
      "     |      >>> df_empty = spark.createDataFrame([], 'a STRING')\n",
      "     |      >>> df_empty.isEmpty()\n",
      "     |      True\n",
      "     |      \n",
      "     |      Example 2: Checking if a non-empty DataFrame is empty\n",
      "     |      \n",
      "     |      >>> df_non_empty = spark.createDataFrame([\"a\"], 'STRING')\n",
      "     |      >>> df_non_empty.isEmpty()\n",
      "     |      False\n",
      "     |      \n",
      "     |      Example 3: Checking if a DataFrame with null values is empty\n",
      "     |      \n",
      "     |      >>> df_nulls = spark.createDataFrame([(None, None)], 'a STRING, b INT')\n",
      "     |      >>> df_nulls.isEmpty()\n",
      "     |      False\n",
      "     |      \n",
      "     |      Example 4: Checking if a DataFrame with no rows but with columns is empty\n",
      "     |      \n",
      "     |      >>> df_no_rows = spark.createDataFrame([], 'id INT, value STRING')\n",
      "     |      >>> df_no_rows.isEmpty()\n",
      "     |      True\n",
      "     |  \n",
      "     |  isLocal(self) -> bool\n",
      "     |      Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally\n",
      "     |      (without any Spark executors).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.sql(\"SHOW TABLES\")\n",
      "     |      >>> df.isLocal()\n",
      "     |      True\n",
      "     |  \n",
      "     |  join(self, other: 'DataFrame', on: Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], NoneType] = None, how: Optional[str] = None) -> 'DataFrame'\n",
      "     |      Joins with another :class:`DataFrame`, using the given join expression.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          Right side of the join\n",
      "     |      on : str, list or :class:`Column`, optional\n",
      "     |          a string for the join column name, a list of column names,\n",
      "     |          a join expression (Column), or a list of Columns.\n",
      "     |          If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      "     |          the column(s) must exist on both sides, and this performs an equi-join.\n",
      "     |      how : str, optional\n",
      "     |          default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      "     |          ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      "     |          ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      "     |          ``anti``, ``leftanti`` and ``left_anti``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Joined DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      The following performs a full outer join between ``df1`` and ``df2``.\n",
      "     |      \n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> from pyspark.sql.functions import desc\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")]).toDF(\"age\", \"name\")\n",
      "     |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      "     |      >>> df3 = spark.createDataFrame([Row(age=2, name=\"Alice\"), Row(age=5, name=\"Bob\")])\n",
      "     |      >>> df4 = spark.createDataFrame([\n",
      "     |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
      "     |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
      "     |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
      "     |      ...     Row(age=None, height=None, name=None),\n",
      "     |      ... ])\n",
      "     |      \n",
      "     |      Inner join on columns (default)\n",
      "     |      \n",
      "     |      >>> df.join(df2, 'name').select(df.name, df2.height).show()\n",
      "     |      +----+------+\n",
      "     |      |name|height|\n",
      "     |      +----+------+\n",
      "     |      | Bob|    85|\n",
      "     |      +----+------+\n",
      "     |      >>> df.join(df4, ['name', 'age']).select(df.name, df.age).show()\n",
      "     |      +----+---+\n",
      "     |      |name|age|\n",
      "     |      +----+---+\n",
      "     |      | Bob|  5|\n",
      "     |      +----+---+\n",
      "     |      \n",
      "     |      Outer join for both DataFrames on the 'name' column.\n",
      "     |      \n",
      "     |      >>> df.join(df2, df.name == df2.name, 'outer').select(\n",
      "     |      ...     df.name, df2.height).sort(desc(\"name\")).show()\n",
      "     |      +-----+------+\n",
      "     |      | name|height|\n",
      "     |      +-----+------+\n",
      "     |      |  Bob|    85|\n",
      "     |      |Alice|  NULL|\n",
      "     |      | NULL|    80|\n",
      "     |      +-----+------+\n",
      "     |      >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).show()\n",
      "     |      +-----+------+\n",
      "     |      | name|height|\n",
      "     |      +-----+------+\n",
      "     |      |  Tom|    80|\n",
      "     |      |  Bob|    85|\n",
      "     |      |Alice|  NULL|\n",
      "     |      +-----+------+\n",
      "     |      \n",
      "     |      Outer join for both DataFrams with multiple columns.\n",
      "     |      \n",
      "     |      >>> df.join(\n",
      "     |      ...     df3,\n",
      "     |      ...     [df.name == df3.name, df.age == df3.age],\n",
      "     |      ...     'outer'\n",
      "     |      ... ).select(df.name, df3.age).show()\n",
      "     |      +-----+---+\n",
      "     |      | name|age|\n",
      "     |      +-----+---+\n",
      "     |      |Alice|  2|\n",
      "     |      |  Bob|  5|\n",
      "     |      +-----+---+\n",
      "     |  \n",
      "     |  limit(self, num: int) -> 'DataFrame'\n",
      "     |      Limits the result count to the number specified.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      num : int\n",
      "     |          Number of records to return. Will return this number of records\n",
      "     |          or all records if the DataFrame contains less than this number of records.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Subset of the records\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.limit(1).show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      | 14| Tom|\n",
      "     |      +---+----+\n",
      "     |      >>> df.limit(0).show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      +---+----+\n",
      "     |  \n",
      "     |  localCheckpoint(self, eager: bool = True) -> 'DataFrame'\n",
      "     |      Returns a locally checkpointed version of this :class:`DataFrame`. Checkpointing can be\n",
      "     |      used to truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      "     |      iterative algorithms where the plan may grow exponentially. Local checkpoints are\n",
      "     |      stored in the executors using the caching subsystem and therefore they are not reliable.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      eager : bool, optional, default True\n",
      "     |          Whether to checkpoint this :class:`DataFrame` immediately.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Checkpointed DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is experimental.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.localCheckpoint(False)\n",
      "     |      DataFrame[age: bigint, name: string]\n",
      "     |  \n",
      "     |  melt(self, ids: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...]], values: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...], NoneType], variableColumnName: str, valueColumnName: str) -> 'DataFrame'\n",
      "     |      Unpivot a DataFrame from wide format to long format, optionally leaving\n",
      "     |      identifier columns set. This is the reverse to `groupBy(...).pivot(...).agg(...)`,\n",
      "     |      except for the aggregation, which cannot be reversed.\n",
      "     |      \n",
      "     |      :func:`melt` is an alias for :func:`unpivot`.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ids : str, Column, tuple, list, optional\n",
      "     |          Column(s) to use as identifiers. Can be a single column or column name,\n",
      "     |          or a list or tuple for multiple columns.\n",
      "     |      values : str, Column, tuple, list, optional\n",
      "     |          Column(s) to unpivot. Can be a single column or column name, or a list or tuple\n",
      "     |          for multiple columns. If not specified or empty, use all columns that\n",
      "     |          are not set as `ids`.\n",
      "     |      variableColumnName : str\n",
      "     |          Name of the variable column.\n",
      "     |      valueColumnName : str\n",
      "     |          Name of the value column.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Unpivoted DataFrame.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      DataFrame.unpivot\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  observe(self, observation: Union[ForwardRef('Observation'), str], *exprs: pyspark.sql.column.Column) -> 'DataFrame'\n",
      "     |      Define (named) metrics to observe on the DataFrame. This method returns an 'observed'\n",
      "     |      DataFrame that returns the same result as the input, with the following guarantees:\n",
      "     |      \n",
      "     |      * It will compute the defined aggregates (metrics) on all the data that is flowing through\n",
      "     |          the Dataset at that point.\n",
      "     |      \n",
      "     |      * It will report the value of the defined aggregate columns as soon as we reach a completion\n",
      "     |          point. A completion point is either the end of a query (batch mode) or the end of a\n",
      "     |          streaming epoch. The value of the aggregates only reflects the data processed since\n",
      "     |          the previous completion point.\n",
      "     |      \n",
      "     |      The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or\n",
      "     |      more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that\n",
      "     |      contain references to the input Dataset's columns must always be wrapped in an aggregate\n",
      "     |      function.\n",
      "     |      \n",
      "     |      A user can observe these metrics by adding\n",
      "     |      Python's :class:`~pyspark.sql.streaming.StreamingQueryListener`,\n",
      "     |      Scala/Java's ``org.apache.spark.sql.streaming.StreamingQueryListener`` or Scala/Java's\n",
      "     |      ``org.apache.spark.sql.util.QueryExecutionListener`` to the spark session.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      observation : :class:`Observation` or str\n",
      "     |          `str` to specify the name, or an :class:`Observation` instance to obtain the metric.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |             Added support for `str` in this parameter.\n",
      "     |      exprs : :class:`Column`\n",
      "     |          column expressions (:class:`Column`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          the observed :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      When ``observation`` is :class:`Observation`, this method only supports batch queries.\n",
      "     |      When ``observation`` is a string, this method works for both batch and streaming queries.\n",
      "     |      Continuous execution is currently not supported yet.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      When ``observation`` is :class:`Observation`, only batch queries work as below.\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.functions import col, count, lit, max\n",
      "     |      >>> from pyspark.sql import Observation\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> observation = Observation(\"my metrics\")\n",
      "     |      >>> observed_df = df.observe(observation, count(lit(1)).alias(\"count\"), max(col(\"age\")))\n",
      "     |      >>> observed_df.count()\n",
      "     |      2\n",
      "     |      >>> observation.get\n",
      "     |      {'count': 2, 'max(age)': 5}\n",
      "     |      \n",
      "     |      When ``observation`` is a string, streaming queries also work as below.\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.streaming import StreamingQueryListener\n",
      "     |      >>> class MyErrorListener(StreamingQueryListener):\n",
      "     |      ...    def onQueryStarted(self, event):\n",
      "     |      ...        pass\n",
      "     |      ...\n",
      "     |      ...    def onQueryProgress(self, event):\n",
      "     |      ...        row = event.progress.observedMetrics.get(\"my_event\")\n",
      "     |      ...        # Trigger if the number of errors exceeds 5 percent\n",
      "     |      ...        num_rows = row.rc\n",
      "     |      ...        num_error_rows = row.erc\n",
      "     |      ...        ratio = num_error_rows / num_rows\n",
      "     |      ...        if ratio > 0.05:\n",
      "     |      ...            # Trigger alert\n",
      "     |      ...            pass\n",
      "     |      ...\n",
      "     |      ...    def onQueryIdle(self, event):\n",
      "     |      ...        pass\n",
      "     |      ...\n",
      "     |      ...    def onQueryTerminated(self, event):\n",
      "     |      ...        pass\n",
      "     |      ...\n",
      "     |      >>> spark.streams.addListener(MyErrorListener())\n",
      "     |      >>> # Observe row count (rc) and error row count (erc) in the streaming Dataset\n",
      "     |      ... observed_ds = df.observe(\n",
      "     |      ...     \"my_event\",\n",
      "     |      ...     count(lit(1)).alias(\"rc\"),\n",
      "     |      ...     count(col(\"error\")).alias(\"erc\"))  # doctest: +SKIP\n",
      "     |      >>> observed_ds.writeStream.format(\"console\").start()  # doctest: +SKIP\n",
      "     |  \n",
      "     |  offset(self, num: int) -> 'DataFrame'\n",
      "     |      Returns a new :class: `DataFrame` by skipping the first `n` rows.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports vanilla PySpark.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      num : int\n",
      "     |          Number of records to skip.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Subset of the records\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.offset(1).show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      | 23|Alice|\n",
      "     |      | 16|  Bob|\n",
      "     |      +---+-----+\n",
      "     |      >>> df.offset(10).show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      +---+----+\n",
      "     |  \n",
      "     |  orderBy = sort(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
      "     |  \n",
      "     |  pandas_api(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
      "     |      Converts the existing DataFrame into a pandas-on-Spark DataFrame.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.2.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      If a pandas-on-Spark DataFrame is converted to a Spark DataFrame and then back\n",
      "     |      to pandas-on-Spark, it will lose the index information and the original index\n",
      "     |      will be turned into a normal column.\n",
      "     |      \n",
      "     |      This is only available if Pandas is installed and available.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      index_col: str or list of str, optional, default: None\n",
      "     |          Index column of table in Spark.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`PandasOnSparkDataFrame`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.pandas.frame.DataFrame.to_spark\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      >>> df.pandas_api()  # doctest: +SKIP\n",
      "     |         age   name\n",
      "     |      0   14    Tom\n",
      "     |      1   23  Alice\n",
      "     |      2   16    Bob\n",
      "     |      \n",
      "     |      We can specify the index columns.\n",
      "     |      \n",
      "     |      >>> df.pandas_api(index_col=\"age\")  # doctest: +SKIP\n",
      "     |            name\n",
      "     |      age\n",
      "     |      14     Tom\n",
      "     |      23   Alice\n",
      "     |      16     Bob\n",
      "     |  \n",
      "     |  persist(self, storageLevel: pyspark.storagelevel.StorageLevel = StorageLevel(True, True, False, True, 1)) -> 'DataFrame'\n",
      "     |      Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
      "     |      operations after the first time it is computed. This can only be used to assign\n",
      "     |      a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
      "     |      If no storage level is specified defaults to (`MEMORY_AND_DISK_DESER`)\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      storageLevel : :class:`StorageLevel`\n",
      "     |          Storage level to set for persistence. Default is MEMORY_AND_DISK_DESER.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Persisted DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.range(1)\n",
      "     |      >>> df.persist()\n",
      "     |      DataFrame[id: bigint]\n",
      "     |      \n",
      "     |      >>> df.explain()\n",
      "     |      == Physical Plan ==\n",
      "     |      AdaptiveSparkPlan isFinalPlan=false\n",
      "     |      +- InMemoryTableScan ...\n",
      "     |      \n",
      "     |      Persists the data in the disk by specifying the storage level.\n",
      "     |      \n",
      "     |      >>> from pyspark.storagelevel import StorageLevel\n",
      "     |      >>> df.persist(StorageLevel.DISK_ONLY)\n",
      "     |      DataFrame[id: bigint]\n",
      "     |  \n",
      "     |  printSchema(self, level: Optional[int] = None) -> None\n",
      "     |      Prints out the schema in the tree format.\n",
      "     |      Optionally allows to specify how many levels to print if schema is nested.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      level : int, optional, default None\n",
      "     |          How many levels to print for nested schemas.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.5.0\n",
      "     |              Added Level parameter.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.printSchema()\n",
      "     |      root\n",
      "     |       |-- age: long (nullable = true)\n",
      "     |       |-- name: string (nullable = true)\n",
      "     |      \n",
      "     |      >>> df = spark.createDataFrame([(1, (2,2))], [\"a\", \"b\"])\n",
      "     |      >>> df.printSchema(1)\n",
      "     |      root\n",
      "     |       |-- a: long (nullable = true)\n",
      "     |       |-- b: struct (nullable = true)\n",
      "     |      \n",
      "     |      >>> df.printSchema(2)\n",
      "     |      root\n",
      "     |       |-- a: long (nullable = true)\n",
      "     |       |-- b: struct (nullable = true)\n",
      "     |       |    |-- _1: long (nullable = true)\n",
      "     |       |    |-- _2: long (nullable = true)\n",
      "     |  \n",
      "     |  randomSplit(self, weights: List[float], seed: Optional[int] = None) -> List[ForwardRef('DataFrame')]\n",
      "     |      Randomly splits this :class:`DataFrame` with the provided weights.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      weights : list\n",
      "     |          list of doubles as weights with which to split the :class:`DataFrame`.\n",
      "     |          Weights will be normalized if they don't sum up to 1.0.\n",
      "     |      seed : int, optional\n",
      "     |          The seed for sampling.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          List of DataFrames.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
      "     |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
      "     |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
      "     |      ...     Row(age=None, height=None, name=None),\n",
      "     |      ... ])\n",
      "     |      \n",
      "     |      >>> splits = df.randomSplit([1.0, 2.0], 24)\n",
      "     |      >>> splits[0].count()\n",
      "     |      2\n",
      "     |      >>> splits[1].count()\n",
      "     |      2\n",
      "     |  \n",
      "     |  registerTempTable(self, name: str) -> None\n",
      "     |      Registers this :class:`DataFrame` as a temporary table using the given name.\n",
      "     |      \n",
      "     |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      "     |      that was used to create this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      .. deprecated:: 2.0.0\n",
      "     |          Use :meth:`DataFrame.createOrReplaceTempView` instead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          Name of the temporary table to register.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.registerTempTable(\"people\")\n",
      "     |      >>> df2 = spark.sql(\"SELECT * FROM people\")\n",
      "     |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |      >>> spark.catalog.dropTempView(\"people\")\n",
      "     |      True\n",
      "     |  \n",
      "     |  repartition(self, numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      "     |      resulting :class:`DataFrame` is hash partitioned.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      numPartitions : int\n",
      "     |          can be an int to specify the target number of partitions or a Column.\n",
      "     |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      "     |          the default number of partitions is used.\n",
      "     |      cols : str or :class:`Column`\n",
      "     |          partitioning columns.\n",
      "     |      \n",
      "     |          .. versionchanged:: 1.6.0\n",
      "     |             Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
      "     |             optional if partitioning columns are specified.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Repartitioned DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Repartition the data into 10 partitions.\n",
      "     |      \n",
      "     |      >>> df.repartition(10).rdd.getNumPartitions()\n",
      "     |      10\n",
      "     |      \n",
      "     |      Repartition the data into 7 partitions by 'age' column.\n",
      "     |      \n",
      "     |      >>> df.repartition(7, \"age\").rdd.getNumPartitions()\n",
      "     |      7\n",
      "     |      \n",
      "     |      Repartition the data into 7 partitions by 'age' and 'name columns.\n",
      "     |      \n",
      "     |      >>> df.repartition(3, \"name\", \"age\").rdd.getNumPartitions()\n",
      "     |      3\n",
      "     |  \n",
      "     |  repartitionByRange(self, numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      "     |      resulting :class:`DataFrame` is range partitioned.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      numPartitions : int\n",
      "     |          can be an int to specify the target number of partitions or a Column.\n",
      "     |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      "     |          the default number of partitions is used.\n",
      "     |      cols : str or :class:`Column`\n",
      "     |          partitioning columns.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Repartitioned DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      At least one partition-by expression must be specified.\n",
      "     |      When no explicit sort order is specified, \"ascending nulls first\" is assumed.\n",
      "     |      \n",
      "     |      Due to performance reasons this method uses sampling to estimate the ranges.\n",
      "     |      Hence, the output may not be consistent, since sampling can return different values.\n",
      "     |      The sample size can be controlled by the config\n",
      "     |      `spark.sql.execution.rangeExchange.sampleSizePerPartition`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Repartition the data into 2 partitions by range in 'age' column.\n",
      "     |      For example, the first partition can have ``(14, \"Tom\")``, and the second\n",
      "     |      partition would have ``(16, \"Bob\")`` and ``(23, \"Alice\")``.\n",
      "     |      \n",
      "     |      >>> df.repartitionByRange(2, \"age\").rdd.getNumPartitions()\n",
      "     |      2\n",
      "     |  \n",
      "     |  replace(self, to_replace: Union[ForwardRef('LiteralType'), List[ForwardRef('LiteralType')], Dict[ForwardRef('LiteralType'), ForwardRef('OptionalPrimitiveType')]], value: Union[ForwardRef('OptionalPrimitiveType'), List[ForwardRef('OptionalPrimitiveType')], pyspark._globals._NoValueType, NoneType] = <no value>, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      "     |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      "     |      aliases of each other.\n",
      "     |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      "     |      or strings. Value can have None. When replacing, the new value will be cast\n",
      "     |      to the type of the existing column.\n",
      "     |      For numeric replacements all values to be replaced should have unique\n",
      "     |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      "     |      and arbitrary replacement will be used.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      to_replace : bool, int, float, string, list or dict\n",
      "     |          Value to be replaced.\n",
      "     |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      "     |          must be a mapping between a value and a replacement.\n",
      "     |      value : bool, int, float, string or None, optional\n",
      "     |          The replacement value must be a bool, int, float, string or None. If `value` is a\n",
      "     |          list, `value` should be of the same length and type as `to_replace`.\n",
      "     |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      "     |          used as a replacement for each item in `to_replace`.\n",
      "     |      subset : list, optional\n",
      "     |          optional list of column names to consider.\n",
      "     |          Columns specified in subset that do not have matching data types are ignored.\n",
      "     |          For example, if `value` is a string, and subset contains a non-string column,\n",
      "     |          then the non-string column is simply ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with replaced values.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (10, 80, \"Alice\"),\n",
      "     |      ...     (5, None, \"Bob\"),\n",
      "     |      ...     (None, 10, \"Tom\"),\n",
      "     |      ...     (None, None, None)],\n",
      "     |      ...     schema=[\"age\", \"height\", \"name\"])\n",
      "     |      \n",
      "     |      Replace 10 to 20 in all columns.\n",
      "     |      \n",
      "     |      >>> df.na.replace(10, 20).show()\n",
      "     |      +----+------+-----+\n",
      "     |      | age|height| name|\n",
      "     |      +----+------+-----+\n",
      "     |      |  20|    80|Alice|\n",
      "     |      |   5|  NULL|  Bob|\n",
      "     |      |NULL|    20|  Tom|\n",
      "     |      |NULL|  NULL| NULL|\n",
      "     |      +----+------+-----+\n",
      "     |      \n",
      "     |      Replace 'Alice' to null in all columns.\n",
      "     |      \n",
      "     |      >>> df.na.replace('Alice', None).show()\n",
      "     |      +----+------+----+\n",
      "     |      | age|height|name|\n",
      "     |      +----+------+----+\n",
      "     |      |  10|    80|NULL|\n",
      "     |      |   5|  NULL| Bob|\n",
      "     |      |NULL|    10| Tom|\n",
      "     |      |NULL|  NULL|NULL|\n",
      "     |      +----+------+----+\n",
      "     |      \n",
      "     |      Replace 'Alice' to 'A', and 'Bob' to 'B' in the 'name' column.\n",
      "     |      \n",
      "     |      >>> df.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      "     |      +----+------+----+\n",
      "     |      | age|height|name|\n",
      "     |      +----+------+----+\n",
      "     |      |  10|    80|   A|\n",
      "     |      |   5|  NULL|   B|\n",
      "     |      |NULL|    10| Tom|\n",
      "     |      |NULL|  NULL|NULL|\n",
      "     |      +----+------+----+\n",
      "     |  \n",
      "     |  rollup(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
      "     |      Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
      "     |      the specified columns, so we can run aggregation on them.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : list, str or :class:`Column`\n",
      "     |          Columns to roll-up by.\n",
      "     |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
      "     |          or list of them.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`GroupedData`\n",
      "     |          Rolled-up data by given columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      "     |      +-----+----+-----+\n",
      "     |      | name| age|count|\n",
      "     |      +-----+----+-----+\n",
      "     |      | NULL|NULL|    2|\n",
      "     |      |Alice|NULL|    1|\n",
      "     |      |Alice|   2|    1|\n",
      "     |      |  Bob|NULL|    1|\n",
      "     |      |  Bob|   5|    1|\n",
      "     |      +-----+----+-----+\n",
      "     |  \n",
      "     |  sameSemantics(self, other: 'DataFrame') -> bool\n",
      "     |      Returns `True` when the logical query plans inside both :class:`DataFrame`\\s are equal and\n",
      "     |      therefore return the same results.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The equality comparison here is simplified by tolerating the cosmetic differences\n",
      "     |      such as attribute names.\n",
      "     |      \n",
      "     |      This API can compare both :class:`DataFrame`\\s very fast but can still return\n",
      "     |      `False` on the :class:`DataFrame` that return the same results, for instance, from\n",
      "     |      different plans. Such false negative semantic can be useful when caching as an example.\n",
      "     |      \n",
      "     |      This API is a developer API.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          The other DataFrame to compare against.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          Whether these two DataFrames are similar.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df1 = spark.range(10)\n",
      "     |      >>> df2 = spark.range(10)\n",
      "     |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id * 2))\n",
      "     |      True\n",
      "     |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id + 2))\n",
      "     |      False\n",
      "     |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col0\", df2.id * 2))\n",
      "     |      True\n",
      "     |  \n",
      "     |  sample(self, withReplacement: Union[float, bool, NoneType] = None, fraction: Union[int, float, NoneType] = None, seed: Optional[int] = None) -> 'DataFrame'\n",
      "     |      Returns a sampled subset of this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      withReplacement : bool, optional\n",
      "     |          Sample with replacement or not (default ``False``).\n",
      "     |      fraction : float, optional\n",
      "     |          Fraction of rows to generate, range [0.0, 1.0].\n",
      "     |      seed : int, optional\n",
      "     |          Seed for sampling (default a random seed).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Sampled rows from given DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This is not guaranteed to provide exactly the fraction specified of the total\n",
      "     |      count of the given :class:`DataFrame`.\n",
      "     |      \n",
      "     |      `fraction` is required and, `withReplacement` and `seed` are optional.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.range(10)\n",
      "     |      >>> df.sample(0.5, 3).count() # doctest: +SKIP\n",
      "     |      7\n",
      "     |      >>> df.sample(fraction=0.5, seed=3).count() # doctest: +SKIP\n",
      "     |      7\n",
      "     |      >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count() # doctest: +SKIP\n",
      "     |      1\n",
      "     |      >>> df.sample(1.0).count()\n",
      "     |      10\n",
      "     |      >>> df.sample(fraction=1.0).count()\n",
      "     |      10\n",
      "     |      >>> df.sample(False, fraction=1.0).count()\n",
      "     |      10\n",
      "     |  \n",
      "     |  sampleBy(self, col: 'ColumnOrName', fractions: Dict[Any, float], seed: Optional[int] = None) -> 'DataFrame'\n",
      "     |      Returns a stratified sample without replacement based on the\n",
      "     |      fraction given on each stratum.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col : :class:`Column` or str\n",
      "     |          column that defines strata\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.0.0\n",
      "     |             Added sampling by a column of :class:`Column`\n",
      "     |      fractions : dict\n",
      "     |          sampling fraction for each stratum. If a stratum is not\n",
      "     |          specified, we treat its fraction as zero.\n",
      "     |      seed : int, optional\n",
      "     |          random seed\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a new :class:`DataFrame` that represents the stratified sample\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import col\n",
      "     |      >>> dataset = spark.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
      "     |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
      "     |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
      "     |      +---+-----+\n",
      "     |      |key|count|\n",
      "     |      +---+-----+\n",
      "     |      |  0|    3|\n",
      "     |      |  1|    6|\n",
      "     |      +---+-----+\n",
      "     |      >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
      "     |      33\n",
      "     |  \n",
      "     |  select(self, *cols: 'ColumnOrName') -> 'DataFrame'\n",
      "     |      Projects a set of expressions and returns a new :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, :class:`Column`, or list\n",
      "     |          column names (string) or expressions (:class:`Column`).\n",
      "     |          If one of the column names is '*', that column is expanded to include all columns\n",
      "     |          in the current :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          A DataFrame with subset (or all) of columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Select all columns in the DataFrame.\n",
      "     |      \n",
      "     |      >>> df.select('*').show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  2|Alice|\n",
      "     |      |  5|  Bob|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Select a column with other expressions in the DataFrame.\n",
      "     |      \n",
      "     |      >>> df.select(df.name, (df.age + 10).alias('age')).show()\n",
      "     |      +-----+---+\n",
      "     |      | name|age|\n",
      "     |      +-----+---+\n",
      "     |      |Alice| 12|\n",
      "     |      |  Bob| 15|\n",
      "     |      +-----+---+\n",
      "     |  \n",
      "     |  selectExpr(self, *expr: Union[str, List[str]]) -> 'DataFrame'\n",
      "     |      Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
      "     |      \n",
      "     |      This is a variant of :func:`select` that accepts SQL expressions.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          A DataFrame with new/old columns transformed by expressions.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.selectExpr(\"age * 2\", \"abs(age)\").show()\n",
      "     |      +---------+--------+\n",
      "     |      |(age * 2)|abs(age)|\n",
      "     |      +---------+--------+\n",
      "     |      |        4|       2|\n",
      "     |      |       10|       5|\n",
      "     |      +---------+--------+\n",
      "     |  \n",
      "     |  semanticHash(self) -> int\n",
      "     |      Returns a hash code of the logical query plan against this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Unlike the standard hash code, the hash is calculated against the query plan\n",
      "     |      simplified by tolerating the cosmetic differences such as attribute names.\n",
      "     |      \n",
      "     |      This API is a developer API.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          Hash value.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.range(10).selectExpr(\"id as col0\").semanticHash()  # doctest: +SKIP\n",
      "     |      1855039936\n",
      "     |      >>> spark.range(10).selectExpr(\"id as col1\").semanticHash()  # doctest: +SKIP\n",
      "     |      1855039936\n",
      "     |  \n",
      "     |  show(self, n: int = 20, truncate: Union[bool, int] = True, vertical: bool = False) -> None\n",
      "     |      Prints the first ``n`` rows to the console.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int, optional\n",
      "     |          Number of rows to show.\n",
      "     |      truncate : bool or int, optional\n",
      "     |          If set to ``True``, truncate strings longer than 20 chars by default.\n",
      "     |          If set to a number greater than one, truncates long strings to length ``truncate``\n",
      "     |          and align cells right.\n",
      "     |      vertical : bool, optional\n",
      "     |          If set to ``True``, print output rows vertically (one line\n",
      "     |          per column value).\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Show only top 2 rows.\n",
      "     |      \n",
      "     |      >>> df.show(2)\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      | 14|  Tom|\n",
      "     |      | 23|Alice|\n",
      "     |      +---+-----+\n",
      "     |      only showing top 2 rows\n",
      "     |      \n",
      "     |      Show :class:`DataFrame` where the maximum number of characters is 3.\n",
      "     |      \n",
      "     |      >>> df.show(truncate=3)\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      | 14| Tom|\n",
      "     |      | 23| Ali|\n",
      "     |      | 16| Bob|\n",
      "     |      +---+----+\n",
      "     |      \n",
      "     |      Show :class:`DataFrame` vertically.\n",
      "     |      \n",
      "     |      >>> df.show(vertical=True)\n",
      "     |      -RECORD 0-----\n",
      "     |      age  | 14\n",
      "     |      name | Tom\n",
      "     |      -RECORD 1-----\n",
      "     |      age  | 23\n",
      "     |      name | Alice\n",
      "     |      -RECORD 2-----\n",
      "     |      age  | 16\n",
      "     |      name | Bob\n",
      "     |  \n",
      "     |  sort(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, list, or :class:`Column`, optional\n",
      "     |           list of :class:`Column` or column names to sort by.\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      ascending : bool or list, optional, default True\n",
      "     |          boolean or list of boolean.\n",
      "     |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      "     |          If a list is specified, the length of the list must equal the length of the `cols`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Sorted DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import desc, asc\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Sort the DataFrame in ascending order.\n",
      "     |      \n",
      "     |      >>> df.sort(asc(\"age\")).show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  2|Alice|\n",
      "     |      |  5|  Bob|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Sort the DataFrame in descending order.\n",
      "     |      \n",
      "     |      >>> df.sort(df.age.desc()).show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  5|  Bob|\n",
      "     |      |  2|Alice|\n",
      "     |      +---+-----+\n",
      "     |      >>> df.orderBy(df.age.desc()).show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  5|  Bob|\n",
      "     |      |  2|Alice|\n",
      "     |      +---+-----+\n",
      "     |      >>> df.sort(\"age\", ascending=False).show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  5|  Bob|\n",
      "     |      |  2|Alice|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Specify multiple columns\n",
      "     |      \n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.orderBy(desc(\"age\"), \"name\").show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  5|  Bob|\n",
      "     |      |  2|Alice|\n",
      "     |      |  2|  Bob|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Specify multiple columns for sorting order at `ascending`.\n",
      "     |      \n",
      "     |      >>> df.orderBy([\"age\", \"name\"], ascending=[False, False]).show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  5|  Bob|\n",
      "     |      |  2|  Bob|\n",
      "     |      |  2|Alice|\n",
      "     |      +---+-----+\n",
      "     |  \n",
      "     |  sortWithinPartitions(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, list or :class:`Column`, optional\n",
      "     |          list of :class:`Column` or column names to sort by.\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      ascending : bool or list, optional, default True\n",
      "     |          boolean or list of boolean.\n",
      "     |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      "     |          If a list is specified, the length of the list must equal the length of the `cols`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame sorted by partitions.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.sortWithinPartitions(\"age\", ascending=False)\n",
      "     |      DataFrame[age: bigint, name: string]\n",
      "     |  \n",
      "     |  subtract(self, other: 'DataFrame') -> 'DataFrame'\n",
      "     |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n",
      "     |      but not in another :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          Another :class:`DataFrame` that needs to be subtracted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Subtracted DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      "     |      >>> df1.subtract(df2).show()\n",
      "     |      +---+---+\n",
      "     |      | C1| C2|\n",
      "     |      +---+---+\n",
      "     |      |  c|  4|\n",
      "     |      +---+---+\n",
      "     |  \n",
      "     |  summary(self, *statistics: str) -> 'DataFrame'\n",
      "     |      Computes specified statistics for numeric and string columns. Available statistics are:\n",
      "     |      - count\n",
      "     |      - mean\n",
      "     |      - stddev\n",
      "     |      - min\n",
      "     |      - max\n",
      "     |      - arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n",
      "     |      \n",
      "     |      If no statistics are given, this function computes count, mean, stddev, min,\n",
      "     |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      statistics : str, optional\n",
      "     |           Column names to calculate statistics by (default All columns).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          A new DataFrame that provides statistics for the given DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This function is meant for exploratory data analysis, as we make no\n",
      "     |      guarantee about the backward compatibility of the schema of the resulting\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
      "     |      ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
      "     |      ... )\n",
      "     |      >>> df.select(\"age\", \"weight\", \"height\").summary().show()\n",
      "     |      +-------+----+------------------+-----------------+\n",
      "     |      |summary| age|            weight|           height|\n",
      "     |      +-------+----+------------------+-----------------+\n",
      "     |      |  count|   3|                 3|                3|\n",
      "     |      |   mean|12.0| 40.73333333333333|            145.0|\n",
      "     |      | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
      "     |      |    min|  11|              37.8|            142.2|\n",
      "     |      |    25%|  11|              37.8|            142.2|\n",
      "     |      |    50%|  12|              40.3|            142.3|\n",
      "     |      |    75%|  13|              44.1|            150.5|\n",
      "     |      |    max|  13|              44.1|            150.5|\n",
      "     |      +-------+----+------------------+-----------------+\n",
      "     |      \n",
      "     |      >>> df.select(\"age\", \"weight\", \"height\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
      "     |      +-------+---+------+------+\n",
      "     |      |summary|age|weight|height|\n",
      "     |      +-------+---+------+------+\n",
      "     |      |  count|  3|     3|     3|\n",
      "     |      |    min| 11|  37.8| 142.2|\n",
      "     |      |    25%| 11|  37.8| 142.2|\n",
      "     |      |    75%| 13|  44.1| 150.5|\n",
      "     |      |    max| 13|  44.1| 150.5|\n",
      "     |      +-------+---+------+------+\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      DataFrame.display\n",
      "     |  \n",
      "     |  tail(self, num: int) -> List[pyspark.sql.types.Row]\n",
      "     |      Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n",
      "     |      \n",
      "     |      Running tail requires moving data into the application's driver process, and doing so with\n",
      "     |      a very large ``num`` can crash the driver process with OutOfMemoryError.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      num : int\n",
      "     |          Number of records to return. Will return this number of records\n",
      "     |          or all records if the DataFrame contains less than this number of records.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          List of rows\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      >>> df.tail(2)\n",
      "     |      [Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
      "     |  \n",
      "     |  take(self, num: int) -> List[pyspark.sql.types.Row]\n",
      "     |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      num : int\n",
      "     |          Number of records to return. Will return this number of records\n",
      "     |          or all records if the DataFrame contains less than this number of records..\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          List of rows\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Return the first 2 rows of the :class:`DataFrame`.\n",
      "     |      \n",
      "     |      >>> df.take(2)\n",
      "     |      [Row(age=14, name='Tom'), Row(age=23, name='Alice')]\n",
      "     |  \n",
      "     |  to(self, schema: pyspark.sql.types.StructType) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` where each row is reconciled to match the specified\n",
      "     |      schema.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      schema : :class:`StructType`\n",
      "     |          Specified schema.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Reconciled DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      * Reorder columns and/or inner fields by name to match the specified schema.\n",
      "     |      \n",
      "     |      * Project away columns and/or inner fields that are not needed by the specified schema.\n",
      "     |          Missing columns and/or inner fields (present in the specified schema but not input\n",
      "     |          DataFrame) lead to failures.\n",
      "     |      \n",
      "     |      * Cast the columns and/or inner fields to match the data types in the specified schema,\n",
      "     |          if the types are compatible, e.g., numeric to numeric (error if overflows), but\n",
      "     |          not string to int.\n",
      "     |      \n",
      "     |      * Carry over the metadata from the specified schema, while the columns and/or inner fields\n",
      "     |          still keep their own metadata if not overwritten by the specified schema.\n",
      "     |      \n",
      "     |      * Fail if the nullability is not compatible. For example, the column and/or inner field\n",
      "     |          is nullable but the specified schema requires them to be not nullable.\n",
      "     |      \n",
      "     |      Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.types import StructField, StringType\n",
      "     |      >>> df = spark.createDataFrame([(\"a\", 1)], [\"i\", \"j\"])\n",
      "     |      >>> df.schema\n",
      "     |      StructType([StructField('i', StringType(), True), StructField('j', LongType(), True)])\n",
      "     |      \n",
      "     |      >>> schema = StructType([StructField(\"j\", StringType()), StructField(\"i\", StringType())])\n",
      "     |      >>> df2 = df.to(schema)\n",
      "     |      >>> df2.schema\n",
      "     |      StructType([StructField('j', StringType(), True), StructField('i', StringType(), True)])\n",
      "     |      >>> df2.show()\n",
      "     |      +---+---+\n",
      "     |      |  j|  i|\n",
      "     |      +---+---+\n",
      "     |      |  1|  a|\n",
      "     |      +---+---+\n",
      "     |  \n",
      "     |  toDF(self, *cols: str) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` that with new specified column names\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      *cols : tuple\n",
      "     |          a tuple of string new column name. The length of the\n",
      "     |          list needs to be the same as the number of columns in the initial\n",
      "     |          :class:`DataFrame`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with new column names.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(14, \"Tom\"), (23, \"Alice\"),\n",
      "     |      ...     (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.toDF('f1', 'f2').show()\n",
      "     |      +---+-----+\n",
      "     |      | f1|   f2|\n",
      "     |      +---+-----+\n",
      "     |      | 14|  Tom|\n",
      "     |      | 23|Alice|\n",
      "     |      | 16|  Bob|\n",
      "     |      +---+-----+\n",
      "     |  \n",
      "     |  toJSON(self, use_unicode: bool = True) -> pyspark.rdd.RDD[str]\n",
      "     |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n",
      "     |      \n",
      "     |      Each row is turned into a JSON document as one element in the returned RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      use_unicode : bool, optional, default True\n",
      "     |          Whether to convert to unicode or not.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.toJSON().first()\n",
      "     |      '{\"age\":2,\"name\":\"Alice\"}'\n",
      "     |  \n",
      "     |  toLocalIterator(self, prefetchPartitions: bool = False) -> Iterator[pyspark.sql.types.Row]\n",
      "     |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n",
      "     |      The iterator will consume as much memory as the largest partition in this\n",
      "     |      :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n",
      "     |      partitions.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      prefetchPartitions : bool, optional\n",
      "     |          If Spark should pre-fetch the next partition before it is needed.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |              This argument does not take effect for Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Iterator\n",
      "     |          Iterator of rows.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> list(df.toLocalIterator())\n",
      "     |      [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
      "     |  \n",
      "     |  to_koalas(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
      "     |      # Keep to_koalas for backward compatibility for now.\n",
      "     |  \n",
      "     |  to_pandas_on_spark(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
      "     |      # Keep to_pandas_on_spark for backward compatibility for now.\n",
      "     |  \n",
      "     |  transform(self, func: Callable[..., ForwardRef('DataFrame')], *args: Any, **kwargs: Any) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : function\n",
      "     |          a function that takes and returns a :class:`DataFrame`.\n",
      "     |      *args\n",
      "     |          Positional arguments to pass to func.\n",
      "     |      \n",
      "     |          .. versionadded:: 3.3.0\n",
      "     |      **kwargs\n",
      "     |          Keyword arguments to pass to func.\n",
      "     |      \n",
      "     |          .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Transformed DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import col\n",
      "     |      >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
      "     |      >>> def cast_all_to_int(input_df):\n",
      "     |      ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
      "     |      ...\n",
      "     |      >>> def sort_columns_asc(input_df):\n",
      "     |      ...     return input_df.select(*sorted(input_df.columns))\n",
      "     |      ...\n",
      "     |      >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n",
      "     |      +-----+---+\n",
      "     |      |float|int|\n",
      "     |      +-----+---+\n",
      "     |      |    1|  1|\n",
      "     |      |    2|  2|\n",
      "     |      +-----+---+\n",
      "     |      \n",
      "     |      >>> def add_n(input_df, n):\n",
      "     |      ...     return input_df.select([(col(col_name) + n).alias(col_name)\n",
      "     |      ...                             for col_name in input_df.columns])\n",
      "     |      >>> df.transform(add_n, 1).transform(add_n, n=10).show()\n",
      "     |      +---+-----+\n",
      "     |      |int|float|\n",
      "     |      +---+-----+\n",
      "     |      | 12| 12.0|\n",
      "     |      | 13| 13.0|\n",
      "     |      +---+-----+\n",
      "     |  \n",
      "     |  union(self, other: 'DataFrame') -> 'DataFrame'\n",
      "     |      Return a new :class:`DataFrame` containing the union of rows in this and another\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          Another :class:`DataFrame` that needs to be unioned.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          A new :class:`DataFrame` containing the combined rows with corresponding columns.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      DataFrame.unionAll\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method performs a SQL-style set union of the rows from both `DataFrame` objects,\n",
      "     |      with no automatic deduplication of elements.\n",
      "     |      \n",
      "     |      Use the `distinct()` method to perform deduplication of rows.\n",
      "     |      \n",
      "     |      The method resolves columns by position (not by name), following the standard behavior\n",
      "     |      in SQL.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Example 1: Combining two DataFrames with the same schema\n",
      "     |      \n",
      "     |      >>> df1 = spark.createDataFrame([(1, 'A'), (2, 'B')], ['id', 'value'])\n",
      "     |      >>> df2 = spark.createDataFrame([(3, 'C'), (4, 'D')], ['id', 'value'])\n",
      "     |      >>> df3 = df1.union(df2)\n",
      "     |      >>> df3.show()\n",
      "     |      +---+-----+\n",
      "     |      | id|value|\n",
      "     |      +---+-----+\n",
      "     |      |  1|    A|\n",
      "     |      |  2|    B|\n",
      "     |      |  3|    C|\n",
      "     |      |  4|    D|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Example 2: Combining two DataFrames with different schemas\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.functions import lit\n",
      "     |      >>> df1 = spark.createDataFrame([(\"Alice\", 1), (\"Bob\", 2)], [\"name\", \"id\"])\n",
      "     |      >>> df2 = spark.createDataFrame([(3, \"Charlie\"), (4, \"Dave\")], [\"id\", \"name\"])\n",
      "     |      >>> df1 = df1.withColumn(\"age\", lit(30))\n",
      "     |      >>> df2 = df2.withColumn(\"age\", lit(40))\n",
      "     |      >>> df3 = df1.union(df2)\n",
      "     |      >>> df3.show()\n",
      "     |      +-----+-------+---+\n",
      "     |      | name|     id|age|\n",
      "     |      +-----+-------+---+\n",
      "     |      |Alice|      1| 30|\n",
      "     |      |  Bob|      2| 30|\n",
      "     |      |    3|Charlie| 40|\n",
      "     |      |    4|   Dave| 40|\n",
      "     |      +-----+-------+---+\n",
      "     |      \n",
      "     |      Example 3: Combining two DataFrames with mismatched columns\n",
      "     |      \n",
      "     |      >>> df1 = spark.createDataFrame([(1, 2)], [\"A\", \"B\"])\n",
      "     |      >>> df2 = spark.createDataFrame([(3, 4)], [\"C\", \"D\"])\n",
      "     |      >>> df3 = df1.union(df2)\n",
      "     |      >>> df3.show()\n",
      "     |      +---+---+\n",
      "     |      |  A|  B|\n",
      "     |      +---+---+\n",
      "     |      |  1|  2|\n",
      "     |      |  3|  4|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      Example 4: Combining duplicate rows from two different DataFrames\n",
      "     |      \n",
      "     |      >>> df1 = spark.createDataFrame([(1, 'A'), (2, 'B'), (3, 'C')], ['id', 'value'])\n",
      "     |      >>> df2 = spark.createDataFrame([(3, 'C'), (4, 'D')], ['id', 'value'])\n",
      "     |      >>> df3 = df1.union(df2).distinct().sort(\"id\")\n",
      "     |      >>> df3.show()\n",
      "     |      +---+-----+\n",
      "     |      | id|value|\n",
      "     |      +---+-----+\n",
      "     |      |  1|    A|\n",
      "     |      |  2|    B|\n",
      "     |      |  3|    C|\n",
      "     |      |  4|    D|\n",
      "     |      +---+-----+\n",
      "     |  \n",
      "     |  unionAll(self, other: 'DataFrame') -> 'DataFrame'\n",
      "     |      Return a new :class:`DataFrame` containing the union of rows in this and another\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          Another :class:`DataFrame` that needs to be combined\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          A new :class:`DataFrame` containing combined rows from both dataframes.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method combines all rows from both `DataFrame` objects with no automatic\n",
      "     |      deduplication of elements.\n",
      "     |      \n",
      "     |      Use the `distinct()` method to perform deduplication of rows.\n",
      "     |      \n",
      "     |      :func:`unionAll` is an alias to :func:`union`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      DataFrame.union\n",
      "     |  \n",
      "     |  unionByName(self, other: 'DataFrame', allowMissingColumns: bool = False) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` containing union of rows in this and another\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      This method performs a union operation on both input DataFrames, resolving columns by\n",
      "     |      name (rather than position). When `allowMissingColumns` is True, missing columns will\n",
      "     |      be filled with null.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          Another :class:`DataFrame` that needs to be combined.\n",
      "     |      allowMissingColumns : bool, optional, default False\n",
      "     |         Specify whether to allow missing columns.\n",
      "     |      \n",
      "     |         .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          A new :class:`DataFrame` containing the combined rows with corresponding\n",
      "     |          columns of the two given DataFrames.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Example 1: Union of two DataFrames with same columns in different order.\n",
      "     |      \n",
      "     |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
      "     |      >>> df1.unionByName(df2).show()\n",
      "     |      +----+----+----+\n",
      "     |      |col0|col1|col2|\n",
      "     |      +----+----+----+\n",
      "     |      |   1|   2|   3|\n",
      "     |      |   6|   4|   5|\n",
      "     |      +----+----+----+\n",
      "     |      \n",
      "     |      Example 2: Union with missing columns and setting `allowMissingColumns=True`.\n",
      "     |      \n",
      "     |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n",
      "     |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
      "     |      +----+----+----+----+\n",
      "     |      |col0|col1|col2|col3|\n",
      "     |      +----+----+----+----+\n",
      "     |      |   1|   2|   3|NULL|\n",
      "     |      |NULL|   4|   5|   6|\n",
      "     |      +----+----+----+----+\n",
      "     |      \n",
      "     |      Example 3: Union of two DataFrames with few common columns.\n",
      "     |      \n",
      "     |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([[4, 5, 6, 7]], [\"col1\", \"col2\", \"col3\", \"col4\"])\n",
      "     |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
      "     |      +----+----+----+----+----+\n",
      "     |      |col0|col1|col2|col3|col4|\n",
      "     |      +----+----+----+----+----+\n",
      "     |      |   1|   2|   3|NULL|NULL|\n",
      "     |      |NULL|   4|   5|   6|   7|\n",
      "     |      +----+----+----+----+----+\n",
      "     |      \n",
      "     |      Example 4: Union of two DataFrames with completely different columns.\n",
      "     |      \n",
      "     |      >>> df1 = spark.createDataFrame([[0, 1, 2]], [\"col0\", \"col1\", \"col2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([[3, 4, 5]], [\"col3\", \"col4\", \"col5\"])\n",
      "     |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
      "     |      +----+----+----+----+----+----+\n",
      "     |      |col0|col1|col2|col3|col4|col5|\n",
      "     |      +----+----+----+----+----+----+\n",
      "     |      |   0|   1|   2|NULL|NULL|NULL|\n",
      "     |      |NULL|NULL|NULL|   3|   4|   5|\n",
      "     |      +----+----+----+----+----+----+\n",
      "     |  \n",
      "     |  unpersist(self, blocking: bool = False) -> 'DataFrame'\n",
      "     |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n",
      "     |      memory and disk.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      `blocking` default has changed to ``False`` to match Scala in 2.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      blocking : bool\n",
      "     |          Whether to block until all blocks are deleted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Unpersisted DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.range(1)\n",
      "     |      >>> df.persist()\n",
      "     |      DataFrame[id: bigint]\n",
      "     |      >>> df.unpersist()\n",
      "     |      DataFrame[id: bigint]\n",
      "     |      >>> df = spark.range(1)\n",
      "     |      >>> df.unpersist(True)\n",
      "     |      DataFrame[id: bigint]\n",
      "     |  \n",
      "     |  unpivot(self, ids: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...]], values: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...], NoneType], variableColumnName: str, valueColumnName: str) -> 'DataFrame'\n",
      "     |      Unpivot a DataFrame from wide format to long format, optionally leaving\n",
      "     |      identifier columns set. This is the reverse to `groupBy(...).pivot(...).agg(...)`,\n",
      "     |      except for the aggregation, which cannot be reversed.\n",
      "     |      \n",
      "     |      This function is useful to massage a DataFrame into a format where some\n",
      "     |      columns are identifier columns (\"ids\"), while all other columns (\"values\")\n",
      "     |      are \"unpivoted\" to the rows, leaving just two non-id columns, named as given\n",
      "     |      by `variableColumnName` and `valueColumnName`.\n",
      "     |      \n",
      "     |      When no \"id\" columns are given, the unpivoted DataFrame consists of only the\n",
      "     |      \"variable\" and \"value\" columns.\n",
      "     |      \n",
      "     |      The `values` columns must not be empty so at least one value must be given to be unpivoted.\n",
      "     |      When `values` is `None`, all non-id columns will be unpivoted.\n",
      "     |      \n",
      "     |      All \"value\" columns must share a least common data type. Unless they are the same data type,\n",
      "     |      all \"value\" columns are cast to the nearest common data type. For instance, types\n",
      "     |      `IntegerType` and `LongType` are cast to `LongType`, while `IntegerType` and `StringType`\n",
      "     |      do not have a common data type and `unpivot` fails.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ids : str, Column, tuple, list\n",
      "     |          Column(s) to use as identifiers. Can be a single column or column name,\n",
      "     |          or a list or tuple for multiple columns.\n",
      "     |      values : str, Column, tuple, list, optional\n",
      "     |          Column(s) to unpivot. Can be a single column or column name, or a list or tuple\n",
      "     |          for multiple columns. If specified, must not be empty. If not specified, uses all\n",
      "     |          columns that are not set as `ids`.\n",
      "     |      variableColumnName : str\n",
      "     |          Name of the variable column.\n",
      "     |      valueColumnName : str\n",
      "     |          Name of the value column.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Unpivoted DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(1, 11, 1.1), (2, 12, 1.2)],\n",
      "     |      ...     [\"id\", \"int\", \"double\"],\n",
      "     |      ... )\n",
      "     |      >>> df.show()\n",
      "     |      +---+---+------+\n",
      "     |      | id|int|double|\n",
      "     |      +---+---+------+\n",
      "     |      |  1| 11|   1.1|\n",
      "     |      |  2| 12|   1.2|\n",
      "     |      +---+---+------+\n",
      "     |      \n",
      "     |      >>> df.unpivot(\"id\", [\"int\", \"double\"], \"var\", \"val\").show()\n",
      "     |      +---+------+----+\n",
      "     |      | id|   var| val|\n",
      "     |      +---+------+----+\n",
      "     |      |  1|   int|11.0|\n",
      "     |      |  1|double| 1.1|\n",
      "     |      |  2|   int|12.0|\n",
      "     |      |  2|double| 1.2|\n",
      "     |      +---+------+----+\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      DataFrame.melt\n",
      "     |  \n",
      "     |  where = filter(self, condition)\n",
      "     |      :func:`where` is an alias for :func:`filter`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  withColumn(self, colName: str, col: pyspark.sql.column.Column) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      "     |      existing column that has the same name.\n",
      "     |      \n",
      "     |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      "     |      a column from some other :class:`DataFrame` will raise an error.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      colName : str\n",
      "     |          string, name of the new column.\n",
      "     |      col : :class:`Column`\n",
      "     |          a :class:`Column` expression for the new column.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with new or replaced column.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method introduces a projection internally. Therefore, calling it multiple\n",
      "     |      times, for instance, via loops in order to add multiple columns can generate big\n",
      "     |      plans which can cause performance issues and even `StackOverflowException`.\n",
      "     |      To avoid this, use :func:`select` with multiple columns at once.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.withColumn('age2', df.age + 2).show()\n",
      "     |      +---+-----+----+\n",
      "     |      |age| name|age2|\n",
      "     |      +---+-----+----+\n",
      "     |      |  2|Alice|   4|\n",
      "     |      |  5|  Bob|   7|\n",
      "     |      +---+-----+----+\n",
      "     |  \n",
      "     |  withColumnRenamed(self, existing: str, new: str) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` by renaming an existing column.\n",
      "     |      This is a no-op if the schema doesn't contain the given column name.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      existing : str\n",
      "     |          string, name of the existing column to rename.\n",
      "     |      new : str\n",
      "     |          string, new name of the column.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with renamed column.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.withColumnRenamed('age', 'age2').show()\n",
      "     |      +----+-----+\n",
      "     |      |age2| name|\n",
      "     |      +----+-----+\n",
      "     |      |   2|Alice|\n",
      "     |      |   5|  Bob|\n",
      "     |      +----+-----+\n",
      "     |  \n",
      "     |  withColumns(self, *colsMap: Dict[str, pyspark.sql.column.Column]) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` by adding multiple columns or replacing the\n",
      "     |      existing columns that have the same names.\n",
      "     |      \n",
      "     |      The colsMap is a map of column name and column, the column must only refer to attributes\n",
      "     |      supplied by this Dataset. It is an error to add columns that refer to some other Dataset.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |         Added support for multiple columns adding\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      colsMap : dict\n",
      "     |          a dict of column name and :class:`Column`. Currently, only a single map is supported.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with new or replaced columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.withColumns({'age2': df.age + 2, 'age3': df.age + 3}).show()\n",
      "     |      +---+-----+----+----+\n",
      "     |      |age| name|age2|age3|\n",
      "     |      +---+-----+----+----+\n",
      "     |      |  2|Alice|   4|   5|\n",
      "     |      |  5|  Bob|   7|   8|\n",
      "     |      +---+-----+----+----+\n",
      "     |  \n",
      "     |  withColumnsRenamed(self, colsMap: Dict[str, str]) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` by renaming multiple columns.\n",
      "     |      This is a no-op if the schema doesn't contain the given column names.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |         Added support for multiple columns renaming\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      colsMap : dict\n",
      "     |          a dict of existing column names and corresponding desired column names.\n",
      "     |          Currently, only a single map is supported.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with renamed columns.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`withColumnRenamed`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Support Spark Connect\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df = df.withColumns({'age2': df.age + 2, 'age3': df.age + 3})\n",
      "     |      >>> df.withColumnsRenamed({'age2': 'age4', 'age3': 'age5'}).show()\n",
      "     |      +---+-----+----+----+\n",
      "     |      |age| name|age4|age5|\n",
      "     |      +---+-----+----+----+\n",
      "     |      |  2|Alice|   4|   5|\n",
      "     |      |  5|  Bob|   7|   8|\n",
      "     |      +---+-----+----+----+\n",
      "     |  \n",
      "     |  withMetadata(self, columnName: str, metadata: Dict[str, Any]) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` by updating an existing column with metadata.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      columnName : str\n",
      "     |          string, name of the existing column to update the metadata.\n",
      "     |      metadata : dict\n",
      "     |          dict, new metadata to be assigned to df.schema[columnName].metadata\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with updated metadata column.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df_meta = df.withMetadata('age', {'foo': 'bar'})\n",
      "     |      >>> df_meta.schema['age'].metadata\n",
      "     |      {'foo': 'bar'}\n",
      "     |  \n",
      "     |  withWatermark(self, eventTime: str, delayThreshold: str) -> 'DataFrame'\n",
      "     |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n",
      "     |      in time before which we assume no more late data is going to arrive.\n",
      "     |      \n",
      "     |      Spark will use this watermark for several purposes:\n",
      "     |        - To know when a given time window aggregation can be finalized and thus can be emitted\n",
      "     |          when using output modes that do not allow updates.\n",
      "     |      \n",
      "     |        - To minimize the amount of state that we need to keep for on-going aggregations.\n",
      "     |      \n",
      "     |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n",
      "     |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n",
      "     |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n",
      "     |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n",
      "     |      process records that arrive more than `delayThreshold` late.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      eventTime : str\n",
      "     |          the name of the column that contains the event time of the row.\n",
      "     |      delayThreshold : str\n",
      "     |          the minimum delay to wait to data to arrive late, relative to the\n",
      "     |          latest record that has been processed in the form of an interval\n",
      "     |          (e.g. \"1 minute\" or \"5 hours\").\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Watermarked DataFrame\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This is a feature only for Structured Streaming.\n",
      "     |      \n",
      "     |      This API is evolving.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> from pyspark.sql.functions import timestamp_seconds\n",
      "     |      >>> df = spark.readStream.format(\"rate\").load().selectExpr(\n",
      "     |      ...     \"value % 5 AS value\", \"timestamp\")\n",
      "     |      >>> df.select(\"value\", df.timestamp.alias(\"time\")).withWatermark(\"time\", '10 minutes')\n",
      "     |      DataFrame[value: bigint, time: timestamp]\n",
      "     |      \n",
      "     |      Group the data by window and value (0 - 4), and compute the count of each group.\n",
      "     |      \n",
      "     |      >>> import time\n",
      "     |      >>> from pyspark.sql.functions import window\n",
      "     |      >>> query = (df\n",
      "     |      ...     .withWatermark(\"timestamp\", \"10 minutes\")\n",
      "     |      ...     .groupBy(\n",
      "     |      ...         window(df.timestamp, \"10 minutes\", \"5 minutes\"),\n",
      "     |      ...         df.value)\n",
      "     |      ...     ).count().writeStream.outputMode(\"complete\").format(\"console\").start()\n",
      "     |      >>> time.sleep(3)\n",
      "     |      >>> query.stop()\n",
      "     |  \n",
      "     |  writeTo(self, table: str) -> pyspark.sql.readwriter.DataFrameWriterV2\n",
      "     |      Create a write configuration builder for v2 sources.\n",
      "     |      \n",
      "     |      This builder is used to configure and execute write operations.\n",
      "     |      \n",
      "     |      For example, to append or create or replace existing tables.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      table : str\n",
      "     |          Target table name to write to.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrameWriterV2`\n",
      "     |          DataFrameWriterV2 to use further to specify how to save the data\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.writeTo(\"catalog.db.table\").append()  # doctest: +SKIP\n",
      "     |      >>> df.writeTo(                              # doctest: +SKIP\n",
      "     |      ...     \"catalog.db.table\"\n",
      "     |      ... ).partitionedBy(\"col\").createOrReplace()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  columns\n",
      "     |      Retrieves the names of all columns in the :class:`DataFrame` as a list.\n",
      "     |      \n",
      "     |      The order of the column names in the list reflects their order in the DataFrame.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          List of column names in the DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Example 1: Retrieve column names of a DataFrame\n",
      "     |      \n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\", \"CA\"), (23, \"Alice\", \"NY\"), (16, \"Bob\", \"TX\")],\n",
      "     |      ...     [\"age\", \"name\", \"state\"]\n",
      "     |      ... )\n",
      "     |      >>> df.columns\n",
      "     |      ['age', 'name', 'state']\n",
      "     |      \n",
      "     |      Example 2: Using column names to project specific columns\n",
      "     |      \n",
      "     |      >>> selected_cols = [col for col in df.columns if col != \"age\"]\n",
      "     |      >>> df.select(selected_cols).show()\n",
      "     |      +-----+-----+\n",
      "     |      | name|state|\n",
      "     |      +-----+-----+\n",
      "     |      |  Tom|   CA|\n",
      "     |      |Alice|   NY|\n",
      "     |      |  Bob|   TX|\n",
      "     |      +-----+-----+\n",
      "     |      \n",
      "     |      Example 3: Checking if a specific column exists in a DataFrame\n",
      "     |      \n",
      "     |      >>> \"state\" in df.columns\n",
      "     |      True\n",
      "     |      >>> \"salary\" in df.columns\n",
      "     |      False\n",
      "     |      \n",
      "     |      Example 4: Iterating over columns to apply a transformation\n",
      "     |      \n",
      "     |      >>> import pyspark.sql.functions as f\n",
      "     |      >>> for col_name in df.columns:\n",
      "     |      ...     df = df.withColumn(col_name, f.upper(f.col(col_name)))\n",
      "     |      >>> df.show()\n",
      "     |      +---+-----+-----+\n",
      "     |      |age| name|state|\n",
      "     |      +---+-----+-----+\n",
      "     |      | 14|  TOM|   CA|\n",
      "     |      | 23|ALICE|   NY|\n",
      "     |      | 16|  BOB|   TX|\n",
      "     |      +---+-----+-----+\n",
      "     |      \n",
      "     |      Example 5: Renaming columns and checking the updated column names\n",
      "     |      \n",
      "     |      >>> df = df.withColumnRenamed(\"name\", \"first_name\")\n",
      "     |      >>> df.columns\n",
      "     |      ['age', 'first_name', 'state']\n",
      "     |      \n",
      "     |      Example 6: Using the `columns` property to ensure two DataFrames have the\n",
      "     |      same columns before a union\n",
      "     |      \n",
      "     |      >>> df2 = spark.createDataFrame(\n",
      "     |      ...     [(30, \"Eve\", \"FL\"), (40, \"Sam\", \"WA\")], [\"age\", \"name\", \"location\"])\n",
      "     |      >>> df.columns == df2.columns\n",
      "     |      False\n",
      "     |  \n",
      "     |  dtypes\n",
      "     |      Returns all column names and their data types as a list.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          List of columns as tuple pairs.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.dtypes\n",
      "     |      [('age', 'bigint'), ('name', 'string')]\n",
      "     |  \n",
      "     |  isStreaming\n",
      "     |      Returns ``True`` if this :class:`DataFrame` contains one or more sources that\n",
      "     |      continuously return data as it arrives. A :class:`DataFrame` that reads data from a\n",
      "     |      streaming source must be executed as a :class:`StreamingQuery` using the :func:`start`\n",
      "     |      method in :class:`DataStreamWriter`.  Methods that return a single answer, (e.g.,\n",
      "     |      :func:`count` or :func:`collect`) will throw an :class:`AnalysisException` when there\n",
      "     |      is a streaming source present.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          Whether it's streaming DataFrame or not.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.readStream.format(\"rate\").load()\n",
      "     |      >>> df.isStreaming\n",
      "     |      True\n",
      "     |  \n",
      "     |  na\n",
      "     |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrameNaFunctions`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.sql(\"SELECT 1 AS c1, int(NULL) AS c2\")\n",
      "     |      >>> type(df.na)\n",
      "     |      <class '...dataframe.DataFrameNaFunctions'>\n",
      "     |      \n",
      "     |      Replace the missing values as 2.\n",
      "     |      \n",
      "     |      >>> df.na.fill(2).show()\n",
      "     |      +---+---+\n",
      "     |      | c1| c2|\n",
      "     |      +---+---+\n",
      "     |      |  1|  2|\n",
      "     |      +---+---+\n",
      "     |  \n",
      "     |  rdd\n",
      "     |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.range(1)\n",
      "     |      >>> type(df.rdd)\n",
      "     |      <class 'pyspark.rdd.RDD'>\n",
      "     |  \n",
      "     |  schema\n",
      "     |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`StructType`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Retrieve the schema of the current DataFrame.\n",
      "     |      \n",
      "     |      >>> df.schema\n",
      "     |      StructType([StructField('age', LongType(), True),\n",
      "     |                  StructField('name', StringType(), True)])\n",
      "     |  \n",
      "     |  sparkSession\n",
      "     |      Returns Spark session that created this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`SparkSession`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.range(1)\n",
      "     |      >>> type(df.sparkSession)\n",
      "     |      <class '...session.SparkSession'>\n",
      "     |  \n",
      "     |  sql_ctx\n",
      "     |  \n",
      "     |  stat\n",
      "     |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrameStatFunctions`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import pyspark.sql.functions as f\n",
      "     |      >>> df = spark.range(3).withColumn(\"c\", f.expr(\"id + 1\"))\n",
      "     |      >>> type(df.stat)\n",
      "     |      <class '...dataframe.DataFrameStatFunctions'>\n",
      "     |      >>> df.stat.corr(\"id\", \"c\")\n",
      "     |      1.0\n",
      "     |  \n",
      "     |  storageLevel\n",
      "     |      Get the :class:`DataFrame`'s current storage level.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`StorageLevel`\n",
      "     |          Currently defined storage level.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df1 = spark.range(10)\n",
      "     |      >>> df1.storageLevel\n",
      "     |      StorageLevel(False, False, False, False, 1)\n",
      "     |      >>> df1.cache().storageLevel\n",
      "     |      StorageLevel(True, True, False, True, 1)\n",
      "     |      \n",
      "     |      >>> df2 = spark.range(5)\n",
      "     |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n",
      "     |      StorageLevel(True, False, False, False, 2)\n",
      "     |  \n",
      "     |  write\n",
      "     |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
      "     |      storage.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrameWriter`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> type(df.write)\n",
      "     |      <class '...readwriter.DataFrameWriter'>\n",
      "     |      \n",
      "     |      Write the DataFrame as a table.\n",
      "     |      \n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tab2\")\n",
      "     |      >>> df.write.saveAsTable(\"tab2\")\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tab2\")\n",
      "     |  \n",
      "     |  writeStream\n",
      "     |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n",
      "     |      storage.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataStreamWriter`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import tempfile\n",
      "     |      >>> df = spark.readStream.format(\"rate\").load()\n",
      "     |      >>> type(df.writeStream)\n",
      "     |      <class '...streaming.readwriter.DataStreamWriter'>\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Create a table with Rate source.\n",
      "     |      ...     df.writeStream.toTable(\n",
      "     |      ...         \"my_table\", checkpointLocation=d)\n",
      "     |      <...streaming.query.StreamingQuery object at 0x...>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      "     |  \n",
      "     |  mapInArrow(self, func: 'ArrowMapIterFunction', schema: Union[pyspark.sql.types.StructType, str], barrier: bool = False) -> 'DataFrame'\n",
      "     |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      "     |      function that takes and outputs a PyArrow's `RecordBatch`, and returns the result as a\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The function should take an iterator of `pyarrow.RecordBatch`\\s and return\n",
      "     |      another iterator of `pyarrow.RecordBatch`\\s. All columns are passed\n",
      "     |      together as an iterator of `pyarrow.RecordBatch`\\s to the function and the\n",
      "     |      returned iterator of `pyarrow.RecordBatch`\\s are combined as a :class:`DataFrame`.\n",
      "     |      Each `pyarrow.RecordBatch` size can be controlled by\n",
      "     |      `spark.sql.execution.arrow.maxRecordsPerBatch`. The size of the function's input and\n",
      "     |      output can be different.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : function\n",
      "     |          a Python native function that takes an iterator of `pyarrow.RecordBatch`\\s, and\n",
      "     |          outputs an iterator of `pyarrow.RecordBatch`\\s.\n",
      "     |      schema : :class:`pyspark.sql.types.DataType` or str\n",
      "     |          the return type of the `func` in PySpark. The value can be either a\n",
      "     |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "     |      barrier : bool, optional, default True\n",
      "     |          Use barrier mode execution.\n",
      "     |      \n",
      "     |          .. versionchanged: 3.5.0\n",
      "     |              Added ``barrier`` argument.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import pyarrow  # doctest: +SKIP\n",
      "     |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      "     |      >>> def filter_func(iterator):\n",
      "     |      ...     for batch in iterator:\n",
      "     |      ...         pdf = batch.to_pandas()\n",
      "     |      ...         yield pyarrow.RecordBatch.from_pandas(pdf[pdf.id == 1])\n",
      "     |      >>> df.mapInArrow(filter_func, df.schema).show()  # doctest: +SKIP\n",
      "     |      +---+---+\n",
      "     |      | id|age|\n",
      "     |      +---+---+\n",
      "     |      |  1| 21|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      Set ``barrier`` to ``True`` to force the ``mapInArrow`` stage running in the\n",
      "     |      barrier mode, it ensures all Python workers in the stage will be\n",
      "     |      launched concurrently.\n",
      "     |      \n",
      "     |      >>> df.mapInArrow(filter_func, df.schema, barrier=True).show()  # doctest: +SKIP\n",
      "     |      +---+---+\n",
      "     |      | id|age|\n",
      "     |      +---+---+\n",
      "     |      |  1| 21|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is unstable, and for developers.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.functions.pandas_udf\n",
      "     |      pyspark.sql.DataFrame.mapInPandas\n",
      "     |  \n",
      "     |  mapInPandas(self, func: 'PandasMapIterFunction', schema: Union[pyspark.sql.types.StructType, str], barrier: bool = False) -> 'DataFrame'\n",
      "     |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      "     |      function that takes and outputs a pandas DataFrame, and returns the result as a\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The function should take an iterator of `pandas.DataFrame`\\s and return\n",
      "     |      another iterator of `pandas.DataFrame`\\s. All columns are passed\n",
      "     |      together as an iterator of `pandas.DataFrame`\\s to the function and the\n",
      "     |      returned iterator of `pandas.DataFrame`\\s are combined as a :class:`DataFrame`.\n",
      "     |      Each `pandas.DataFrame` size can be controlled by\n",
      "     |      `spark.sql.execution.arrow.maxRecordsPerBatch`. The size of the function's input and\n",
      "     |      output can be different.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : function\n",
      "     |          a Python native function that takes an iterator of `pandas.DataFrame`\\s, and\n",
      "     |          outputs an iterator of `pandas.DataFrame`\\s.\n",
      "     |      schema : :class:`pyspark.sql.types.DataType` or str\n",
      "     |          the return type of the `func` in PySpark. The value can be either a\n",
      "     |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "     |      barrier : bool, optional, default True\n",
      "     |          Use barrier mode execution.\n",
      "     |      \n",
      "     |          .. versionchanged: 3.5.0\n",
      "     |              Added ``barrier`` argument.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import pandas_udf\n",
      "     |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      "     |      >>> def filter_func(iterator):\n",
      "     |      ...     for pdf in iterator:\n",
      "     |      ...         yield pdf[pdf.id == 1]\n",
      "     |      ...\n",
      "     |      >>> df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n",
      "     |      +---+---+\n",
      "     |      | id|age|\n",
      "     |      +---+---+\n",
      "     |      |  1| 21|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      Set ``barrier`` to ``True`` to force the ``mapInPandas`` stage running in the\n",
      "     |      barrier mode, it ensures all Python workers in the stage will be\n",
      "     |      launched concurrently.\n",
      "     |      \n",
      "     |      >>> df.mapInPandas(filter_func, df.schema, barrier=True).show()  # doctest: +SKIP\n",
      "     |      +---+---+\n",
      "     |      | id|age|\n",
      "     |      +---+---+\n",
      "     |      |  1| 21|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is experimental\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.functions.pandas_udf\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pyspark.sql.pandas.conversion.PandasConversionMixin:\n",
      "     |  \n",
      "     |  toPandas(self) -> 'PandasDataFrameLike'\n",
      "     |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n",
      "     |      \n",
      "     |      This is only available if Pandas is installed and available.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method should only be used if the resulting Pandas ``pandas.DataFrame`` is\n",
      "     |      expected to be small, as all the data is loaded into the driver's memory.\n",
      "     |      \n",
      "     |      Usage with ``spark.sql.execution.arrow.pyspark.enabled=True`` is experimental.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.toPandas()  # doctest: +SKIP\n",
      "     |         age   name\n",
      "     |      0    2  Alice\n",
      "     |      1    5    Bob\n",
      "    \n",
      "    class DataFrameNaFunctions(builtins.object)\n",
      "     |  DataFrameNaFunctions(df: pyspark.sql.dataframe.DataFrame)\n",
      "     |  \n",
      "     |  Functionality for working with missing data in :class:`DataFrame`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df: pyspark.sql.dataframe.DataFrame)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  drop(self, how: str = 'any', thresh: Optional[int] = None, subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      "     |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      how : str, optional\n",
      "     |          'any' or 'all'.\n",
      "     |          If 'any', drop a row if it contains any nulls.\n",
      "     |          If 'all', drop a row only if all its values are null.\n",
      "     |      thresh: int, optional\n",
      "     |          default None\n",
      "     |          If specified, drop rows that have less than `thresh` non-null values.\n",
      "     |          This overwrites the `how` parameter.\n",
      "     |      subset : str, tuple or list, optional\n",
      "     |          optional list of column names to consider.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with null only rows excluded.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
      "     |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
      "     |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
      "     |      ...     Row(age=None, height=None, name=None),\n",
      "     |      ... ])\n",
      "     |      >>> df.na.drop().show()\n",
      "     |      +---+------+-----+\n",
      "     |      |age|height| name|\n",
      "     |      +---+------+-----+\n",
      "     |      | 10|    80|Alice|\n",
      "     |      +---+------+-----+\n",
      "     |  \n",
      "     |  fill(self, value: Union[ForwardRef('LiteralType'), Dict[str, ForwardRef('LiteralType')]], subset: Optional[List[str]] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Replace null values, alias for ``na.fill()``.\n",
      "     |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      value : int, float, string, bool or dict\n",
      "     |          Value to replace null values with.\n",
      "     |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      "     |          from column name (string) to replacement value. The replacement value must be\n",
      "     |          an int, float, boolean, or string.\n",
      "     |      subset : str, tuple or list, optional\n",
      "     |          optional list of column names to consider.\n",
      "     |          Columns specified in subset that do not have matching data types are ignored.\n",
      "     |          For example, if `value` is a string, and subset contains a non-string column,\n",
      "     |          then the non-string column is simply ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with replaced null values.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (10, 80.5, \"Alice\", None),\n",
      "     |      ...     (5, None, \"Bob\", None),\n",
      "     |      ...     (None, None, \"Tom\", None),\n",
      "     |      ...     (None, None, None, True)],\n",
      "     |      ...     schema=[\"age\", \"height\", \"name\", \"bool\"])\n",
      "     |      \n",
      "     |      Fill all null values with 50 for numeric columns.\n",
      "     |      \n",
      "     |      >>> df.na.fill(50).show()\n",
      "     |      +---+------+-----+----+\n",
      "     |      |age|height| name|bool|\n",
      "     |      +---+------+-----+----+\n",
      "     |      | 10|  80.5|Alice|NULL|\n",
      "     |      |  5|  50.0|  Bob|NULL|\n",
      "     |      | 50|  50.0|  Tom|NULL|\n",
      "     |      | 50|  50.0| NULL|true|\n",
      "     |      +---+------+-----+----+\n",
      "     |      \n",
      "     |      Fill all null values with ``False`` for boolean columns.\n",
      "     |      \n",
      "     |      >>> df.na.fill(False).show()\n",
      "     |      +----+------+-----+-----+\n",
      "     |      | age|height| name| bool|\n",
      "     |      +----+------+-----+-----+\n",
      "     |      |  10|  80.5|Alice|false|\n",
      "     |      |   5|  NULL|  Bob|false|\n",
      "     |      |NULL|  NULL|  Tom|false|\n",
      "     |      |NULL|  NULL| NULL| true|\n",
      "     |      +----+------+-----+-----+\n",
      "     |      \n",
      "     |      Fill all null values with to 50 and \"unknown\" for 'age' and 'name' column respectively.\n",
      "     |      \n",
      "     |      >>> df.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      "     |      +---+------+-------+----+\n",
      "     |      |age|height|   name|bool|\n",
      "     |      +---+------+-------+----+\n",
      "     |      | 10|  80.5|  Alice|NULL|\n",
      "     |      |  5|  NULL|    Bob|NULL|\n",
      "     |      | 50|  NULL|    Tom|NULL|\n",
      "     |      | 50|  NULL|unknown|true|\n",
      "     |      +---+------+-------+----+\n",
      "     |  \n",
      "     |  replace(self, to_replace: Union[List[ForwardRef('LiteralType')], Dict[ForwardRef('LiteralType'), ForwardRef('OptionalPrimitiveType')]], value: Union[ForwardRef('OptionalPrimitiveType'), List[ForwardRef('OptionalPrimitiveType')], pyspark._globals._NoValueType, NoneType] = <no value>, subset: Optional[List[str]] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      "     |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      "     |      aliases of each other.\n",
      "     |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      "     |      or strings. Value can have None. When replacing, the new value will be cast\n",
      "     |      to the type of the existing column.\n",
      "     |      For numeric replacements all values to be replaced should have unique\n",
      "     |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      "     |      and arbitrary replacement will be used.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      to_replace : bool, int, float, string, list or dict\n",
      "     |          Value to be replaced.\n",
      "     |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      "     |          must be a mapping between a value and a replacement.\n",
      "     |      value : bool, int, float, string or None, optional\n",
      "     |          The replacement value must be a bool, int, float, string or None. If `value` is a\n",
      "     |          list, `value` should be of the same length and type as `to_replace`.\n",
      "     |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      "     |          used as a replacement for each item in `to_replace`.\n",
      "     |      subset : list, optional\n",
      "     |          optional list of column names to consider.\n",
      "     |          Columns specified in subset that do not have matching data types are ignored.\n",
      "     |          For example, if `value` is a string, and subset contains a non-string column,\n",
      "     |          then the non-string column is simply ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with replaced values.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (10, 80, \"Alice\"),\n",
      "     |      ...     (5, None, \"Bob\"),\n",
      "     |      ...     (None, 10, \"Tom\"),\n",
      "     |      ...     (None, None, None)],\n",
      "     |      ...     schema=[\"age\", \"height\", \"name\"])\n",
      "     |      \n",
      "     |      Replace 10 to 20 in all columns.\n",
      "     |      \n",
      "     |      >>> df.na.replace(10, 20).show()\n",
      "     |      +----+------+-----+\n",
      "     |      | age|height| name|\n",
      "     |      +----+------+-----+\n",
      "     |      |  20|    80|Alice|\n",
      "     |      |   5|  NULL|  Bob|\n",
      "     |      |NULL|    20|  Tom|\n",
      "     |      |NULL|  NULL| NULL|\n",
      "     |      +----+------+-----+\n",
      "     |      \n",
      "     |      Replace 'Alice' to null in all columns.\n",
      "     |      \n",
      "     |      >>> df.na.replace('Alice', None).show()\n",
      "     |      +----+------+----+\n",
      "     |      | age|height|name|\n",
      "     |      +----+------+----+\n",
      "     |      |  10|    80|NULL|\n",
      "     |      |   5|  NULL| Bob|\n",
      "     |      |NULL|    10| Tom|\n",
      "     |      |NULL|  NULL|NULL|\n",
      "     |      +----+------+----+\n",
      "     |      \n",
      "     |      Replace 'Alice' to 'A', and 'Bob' to 'B' in the 'name' column.\n",
      "     |      \n",
      "     |      >>> df.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      "     |      +----+------+----+\n",
      "     |      | age|height|name|\n",
      "     |      +----+------+----+\n",
      "     |      |  10|    80|   A|\n",
      "     |      |   5|  NULL|   B|\n",
      "     |      |NULL|    10| Tom|\n",
      "     |      |NULL|  NULL|NULL|\n",
      "     |      +----+------+----+\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DataFrameReader(OptionUtils)\n",
      "     |  DataFrameReader(spark: 'SparkSession')\n",
      "     |  \n",
      "     |  Interface used to load a :class:`DataFrame` from external storage systems\n",
      "     |  (e.g. file systems, key-value stores, etc). Use :attr:`SparkSession.read`\n",
      "     |  to access this.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DataFrameReader\n",
      "     |      OptionUtils\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, spark: 'SparkSession')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  csv(self, path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[int, str, NoneType] = None, maxCharsPerColumn: Union[int, str, NoneType] = None, maxMalformedLogPerPartition: Union[int, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame'\n",
      "     |      Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      "     |      \n",
      "     |      This function will go through the input once to determine the input schema if\n",
      "     |      ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      "     |      ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str or list\n",
      "     |          string, or list of strings, for input path(s),\n",
      "     |          or RDD of Strings storing CSV rows.\n",
      "     |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      "     |          an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      "     |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a CSV file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a CSV file\n",
      "     |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      "     |      ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
      "     |      ...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |100|NULL|\n",
      "     |      +---+----+\n",
      "     |  \n",
      "     |  format(self, source: str) -> 'DataFrameReader'\n",
      "     |      Specifies the input data source format.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      source : str\n",
      "     |          string, name of the data source, e.g. 'json', 'parquet'.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.read.format('json')\n",
      "     |      <...readwriter.DataFrameReader object ...>\n",
      "     |      \n",
      "     |      Write a DataFrame into a JSON file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a JSON file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the JSON file as a DataFrame.\n",
      "     |      ...     spark.read.format('json').load(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  jdbc(self, url: str, table: str, column: Optional[str] = None, lowerBound: Union[int, str, NoneType] = None, upperBound: Union[int, str, NoneType] = None, numPartitions: Optional[int] = None, predicates: Optional[List[str]] = None, properties: Optional[Dict[str, str]] = None) -> 'DataFrame'\n",
      "     |      Construct a :class:`DataFrame` representing the database table named ``table``\n",
      "     |      accessible via JDBC URL ``url`` and connection ``properties``.\n",
      "     |      \n",
      "     |      Partitions of the table will be retrieved in parallel if either ``column`` or\n",
      "     |      ``predicates`` is specified. ``lowerBound``, ``upperBound`` and ``numPartitions``\n",
      "     |      is needed when ``column`` is specified.\n",
      "     |      \n",
      "     |      If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      table : str\n",
      "     |          the name of the table\n",
      "     |      column : str, optional\n",
      "     |          alias of ``partitionColumn`` option. Refer to ``partitionColumn`` in\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      predicates : list, optional\n",
      "     |          a list of expressions suitable for inclusion in WHERE clauses;\n",
      "     |          each one defines one partition of the :class:`DataFrame`\n",
      "     |      properties : dict, optional\n",
      "     |          a dictionary of JDBC database connection arguments. Normally at\n",
      "     |          least properties \"user\" and \"password\" with their corresponding values.\n",
      "     |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Don't create too many partitions in parallel on a large cluster;\n",
      "     |      otherwise Spark might crash your external database systems.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |  \n",
      "     |  json(self, path: Union[str, List[str], pyspark.rdd.RDD[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, primitivesAsString: Union[bool, str, NoneType] = None, prefersDecimal: Union[bool, str, NoneType] = None, allowComments: Union[bool, str, NoneType] = None, allowUnquotedFieldNames: Union[bool, str, NoneType] = None, allowSingleQuotes: Union[bool, str, NoneType] = None, allowNumericLeadingZero: Union[bool, str, NoneType] = None, allowBackslashEscapingAnyCharacter: Union[bool, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, allowUnquotedControlChars: Union[bool, str, NoneType] = None, lineSep: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, dropFieldIfAllNull: Union[bool, str, NoneType] = None, encoding: Optional[str] = None, locale: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, allowNonNumericNumbers: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
      "     |      Loads JSON files and returns the results as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
      "     |      For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
      "     |      \n",
      "     |      If the ``schema`` parameter is not specified, this function goes\n",
      "     |      through the input once to determine the input schema.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str, list or :class:`RDD`\n",
      "     |          string represents path to the JSON dataset, or a list of paths,\n",
      "     |          or RDD of Strings storing JSON objects.\n",
      "     |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      "     |          an optional :class:`pyspark.sql.types.StructType` for the input schema or\n",
      "     |          a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a JSON file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a JSON file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the JSON file as a DataFrame.\n",
      "     |      ...     spark.read.json(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  load(self, path: Union[str, List[str], NoneType] = None, format: Optional[str] = None, schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n",
      "     |      Loads data from a data source and returns it as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str or list, optional\n",
      "     |          optional string or a list of string for file-system backed data sources.\n",
      "     |      format : str, optional\n",
      "     |          optional string for format of the data source. Default to 'parquet'.\n",
      "     |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      "     |          optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      "     |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "     |      **options : dict\n",
      "     |          all other string options\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Load a CSV file with format, schema and options specified.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a CSV file with a header\n",
      "     |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      "     |      ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n",
      "     |      ...     # and 'header' option set to `True`.\n",
      "     |      ...     df = spark.read.load(\n",
      "     |      ...         d, schema=df.schema, format=\"csv\", nullValue=\"Hyukjin Kwon\", header=True)\n",
      "     |      ...     df.printSchema()\n",
      "     |      ...     df.show()\n",
      "     |      root\n",
      "     |       |-- age: long (nullable = true)\n",
      "     |       |-- name: string (nullable = true)\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |100|NULL|\n",
      "     |      +---+----+\n",
      "     |  \n",
      "     |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameReader'\n",
      "     |      Adds an input option for the underlying data source.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key : str\n",
      "     |          The key for the option to set.\n",
      "     |      value\n",
      "     |          The value for the option to set.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.read.option(\"key\", \"value\")\n",
      "     |      <...readwriter.DataFrameReader object ...>\n",
      "     |      \n",
      "     |      Specify the option 'nullValue' with reading a CSV file.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a CSV file\n",
      "     |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      "     |      ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
      "     |      ...     spark.read.schema(df.schema).option(\n",
      "     |      ...         \"nullValue\", \"Hyukjin Kwon\").format('csv').load(d).show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |100|NULL|\n",
      "     |      +---+----+\n",
      "     |  \n",
      "     |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameReader'\n",
      "     |      Adds input options for the underlying data source.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **options : dict\n",
      "     |          The dictionary of string keys and prmitive-type values.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.read.option(\"key\", \"value\")\n",
      "     |      <...readwriter.DataFrameReader object ...>\n",
      "     |      \n",
      "     |      Specify the option 'nullValue' and 'header' with reading a CSV file.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a CSV file with a header.\n",
      "     |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      "     |      ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n",
      "     |      ...     # and 'header' option set to `True`.\n",
      "     |      ...     spark.read.options(\n",
      "     |      ...         nullValue=\"Hyukjin Kwon\",\n",
      "     |      ...         header=True\n",
      "     |      ...     ).format('csv').load(d).show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |100|NULL|\n",
      "     |      +---+----+\n",
      "     |  \n",
      "     |  orc(self, path: Union[str, List[str]], mergeSchema: Optional[bool] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
      "     |      Loads ORC files, returning the result as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str or list\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a ORC file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a ORC file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.mode(\"overwrite\").format(\"orc\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the Parquet file as a DataFrame.\n",
      "     |      ...     spark.read.orc(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  parquet(self, *paths: str, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n",
      "     |      Loads Parquet files, returning the result as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      paths : str\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      **options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a Parquet file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a Parquet file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the Parquet file as a DataFrame.\n",
      "     |      ...     spark.read.parquet(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  schema(self, schema: Union[pyspark.sql.types.StructType, str]) -> 'DataFrameReader'\n",
      "     |      Specifies the input schema.\n",
      "     |      \n",
      "     |      Some data sources (e.g. JSON) can infer the input schema automatically from data.\n",
      "     |      By specifying the schema here, the underlying data source can skip the schema\n",
      "     |      inference step, and thus speed up data loading.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      schema : :class:`pyspark.sql.types.StructType` or str\n",
      "     |          a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\n",
      "     |          (For example ``col0 INT, col1 DOUBLE``).\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.read.schema(\"col0 INT, col1 DOUBLE\")\n",
      "     |      <...readwriter.DataFrameReader object ...>\n",
      "     |      \n",
      "     |      Specify the schema with reading a CSV file.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     spark.read.schema(\"col0 INT, col1 DOUBLE\").format(\"csv\").load(d).printSchema()\n",
      "     |      root\n",
      "     |       |-- col0: integer (nullable = true)\n",
      "     |       |-- col1: double (nullable = true)\n",
      "     |  \n",
      "     |  table(self, tableName: str) -> 'DataFrame'\n",
      "     |      Returns the specified table as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          string, name of the table.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.range(10)\n",
      "     |      >>> df.createOrReplaceTempView('tblA')\n",
      "     |      >>> spark.read.table('tblA').show()\n",
      "     |      +---+\n",
      "     |      | id|\n",
      "     |      +---+\n",
      "     |      |  0|\n",
      "     |      |  1|\n",
      "     |      |  2|\n",
      "     |      |  3|\n",
      "     |      |  4|\n",
      "     |      |  5|\n",
      "     |      |  6|\n",
      "     |      |  7|\n",
      "     |      |  8|\n",
      "     |      |  9|\n",
      "     |      +---+\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n",
      "     |  \n",
      "     |  text(self, paths: Union[str, List[str]], wholetext: bool = False, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
      "     |      Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
      "     |      string column named \"value\", and followed by partitioned columns if there\n",
      "     |      are any.\n",
      "     |      The text files must be encoded as UTF-8.\n",
      "     |      \n",
      "     |      By default, each line in the text file is a new row in the resulting DataFrame.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      paths : str or list\n",
      "     |          string, or list of strings, for input path(s).\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a text file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a text file\n",
      "     |      ...     df = spark.createDataFrame([(\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n",
      "     |      ...     df.write.mode(\"overwrite\").format(\"text\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the text file as a DataFrame.\n",
      "     |      ...     spark.read.schema(df.schema).text(d).sort(\"alphabets\").show()\n",
      "     |      +---------+\n",
      "     |      |alphabets|\n",
      "     |      +---------+\n",
      "     |      |        a|\n",
      "     |      |        b|\n",
      "     |      |        c|\n",
      "     |      +---------+\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from OptionUtils:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DataFrameStatFunctions(builtins.object)\n",
      "     |  DataFrameStatFunctions(df: pyspark.sql.dataframe.DataFrame)\n",
      "     |  \n",
      "     |  Functionality for statistic functions with :class:`DataFrame`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df: pyspark.sql.dataframe.DataFrame)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  approxQuantile(self, col: Union[str, List[str], Tuple[str]], probabilities: Union[List[float], Tuple[float]], relativeError: float) -> Union[List[float], List[List[float]]]\n",
      "     |      Calculates the approximate quantiles of numerical columns of a\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The result of this algorithm has the following deterministic bound:\n",
      "     |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
      "     |      probability `p` up to error `err`, then the algorithm will return\n",
      "     |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
      "     |      close to (p * N). More precisely,\n",
      "     |      \n",
      "     |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
      "     |      \n",
      "     |      This method implements a variation of the Greenwald-Khanna\n",
      "     |      algorithm (with some speed optimizations). The algorithm was first\n",
      "     |      present in [[https://doi.org/10.1145/375663.375670\n",
      "     |      Space-efficient Online Computation of Quantile Summaries]]\n",
      "     |      by Greenwald and Khanna.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col: str, tuple or list\n",
      "     |          Can be a single column name, or a list of names for multiple columns.\n",
      "     |      \n",
      "     |          .. versionchanged:: 2.2.0\n",
      "     |             Added support for multiple columns.\n",
      "     |      probabilities : list or tuple\n",
      "     |          a list of quantile probabilities\n",
      "     |          Each number must belong to [0, 1].\n",
      "     |          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
      "     |      relativeError : float\n",
      "     |          The relative target precision to achieve\n",
      "     |          (>= 0). If set to zero, the exact quantiles are computed, which\n",
      "     |          could be very expensive. Note that values greater than 1 are\n",
      "     |          accepted but gives the same result as 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          the approximate quantiles at the given probabilities.\n",
      "     |      \n",
      "     |          * If the input `col` is a string, the output is a list of floats.\n",
      "     |      \n",
      "     |          * If the input `col` is a list or tuple of strings, the output is also a\n",
      "     |              list, but each element in it is a list of floats, i.e., the output\n",
      "     |              is a list of list of floats.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Null values will be ignored in numerical columns before calculation.\n",
      "     |      For columns only containing null values, an empty list is returned.\n",
      "     |  \n",
      "     |  corr(self, col1: str, col2: str, method: Optional[str] = None) -> float\n",
      "     |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
      "     |      Currently only supports the Pearson Correlation Coefficient.\n",
      "     |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col1 : str\n",
      "     |          The name of the first column\n",
      "     |      col2 : str\n",
      "     |          The name of the second column\n",
      "     |      method : str, optional\n",
      "     |          The correlation method. Currently only supports \"pearson\"\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float\n",
      "     |          Pearson Correlation Coefficient of two columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
      "     |      >>> df.corr(\"c1\", \"c2\")\n",
      "     |      -0.3592106040535498\n",
      "     |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
      "     |      >>> df.corr(\"small\", \"bigger\")\n",
      "     |      1.0\n",
      "     |  \n",
      "     |  cov(self, col1: str, col2: str) -> float\n",
      "     |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
      "     |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col1 : str\n",
      "     |          The name of the first column\n",
      "     |      col2 : str\n",
      "     |          The name of the second column\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float\n",
      "     |          Covariance of two columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
      "     |      >>> df.cov(\"c1\", \"c2\")\n",
      "     |      -18.0\n",
      "     |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
      "     |      >>> df.cov(\"small\", \"bigger\")\n",
      "     |      1.0\n",
      "     |  \n",
      "     |  crosstab(self, col1: str, col2: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
      "     |      table.\n",
      "     |      The first column of each row will be the distinct values of `col1` and the column names\n",
      "     |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
      "     |      Pairs that have no occurrences will have zero as their counts.\n",
      "     |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col1 : str\n",
      "     |          The name of the first column. Distinct items will make the first item of\n",
      "     |          each row.\n",
      "     |      col2 : str\n",
      "     |          The name of the second column. Distinct items will make the column names\n",
      "     |          of the :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Frequency matrix of two columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
      "     |      >>> df.crosstab(\"c1\", \"c2\").sort(\"c1_c2\").show()\n",
      "     |      +-----+---+---+---+\n",
      "     |      |c1_c2| 10| 11|  8|\n",
      "     |      +-----+---+---+---+\n",
      "     |      |    1|  0|  2|  0|\n",
      "     |      |    3|  1|  0|  0|\n",
      "     |      |    4|  0|  0|  2|\n",
      "     |      +-----+---+---+---+\n",
      "     |  \n",
      "     |  freqItems(self, cols: List[str], support: Optional[float] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Finding frequent items for columns, possibly with false positives. Using the\n",
      "     |      frequent element count algorithm described in\n",
      "     |      \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
      "     |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : list or tuple\n",
      "     |          Names of the columns to calculate frequent items for as a list or tuple of\n",
      "     |          strings.\n",
      "     |      support : float, optional\n",
      "     |          The frequency with which to consider an item 'frequent'. Default is 1%.\n",
      "     |          The support must be greater than 1e-4.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with frequent items.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This function is meant for exploratory data analysis, as we make no\n",
      "     |      guarantee about the backward compatibility of the schema of the resulting\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
      "     |      >>> df.freqItems([\"c1\", \"c2\"]).show()  # doctest: +SKIP\n",
      "     |      +------------+------------+\n",
      "     |      |c1_freqItems|c2_freqItems|\n",
      "     |      +------------+------------+\n",
      "     |      |   [4, 1, 3]| [8, 11, 10]|\n",
      "     |      +------------+------------+\n",
      "     |  \n",
      "     |  sampleBy(self, col: str, fractions: Dict[Any, float], seed: Optional[int] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns a stratified sample without replacement based on the\n",
      "     |      fraction given on each stratum.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col : :class:`Column` or str\n",
      "     |          column that defines strata\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.0.0\n",
      "     |             Added sampling by a column of :class:`Column`\n",
      "     |      fractions : dict\n",
      "     |          sampling fraction for each stratum. If a stratum is not\n",
      "     |          specified, we treat its fraction as zero.\n",
      "     |      seed : int, optional\n",
      "     |          random seed\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a new :class:`DataFrame` that represents the stratified sample\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import col\n",
      "     |      >>> dataset = spark.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
      "     |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
      "     |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
      "     |      +---+-----+\n",
      "     |      |key|count|\n",
      "     |      +---+-----+\n",
      "     |      |  0|    3|\n",
      "     |      |  1|    6|\n",
      "     |      +---+-----+\n",
      "     |      >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
      "     |      33\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DataFrameWriter(OptionUtils)\n",
      "     |  DataFrameWriter(df: 'DataFrame')\n",
      "     |  \n",
      "     |  Interface used to write a :class:`DataFrame` to external storage systems\n",
      "     |  (e.g. file systems, key-value stores, etc). Use :attr:`DataFrame.write`\n",
      "     |  to access this.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DataFrameWriter\n",
      "     |      OptionUtils\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df: 'DataFrame')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  bucketBy(self, numBuckets: int, col: Union[str, List[str], Tuple[str, ...]], *cols: Optional[str]) -> 'DataFrameWriter'\n",
      "     |      Buckets the output by the given columns. If specified,\n",
      "     |      the output is laid out on the file system similar to Hive's bucketing scheme,\n",
      "     |      but with a different bucket hash function and is not compatible with Hive's bucketing.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      numBuckets : int\n",
      "     |          the number of buckets to save\n",
      "     |      col : str, list or tuple\n",
      "     |          a name of a column, or a list of names.\n",
      "     |      cols : str\n",
      "     |          additional names (optional). If `col` is a list it should be empty.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Applicable for file-based data sources in combination with\n",
      "     |      :py:meth:`DataFrameWriter.saveAsTable`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a Parquet file in a buckted manner, and read it back.\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.functions import input_file_name\n",
      "     |      >>> # Write a DataFrame into a Parquet file in a bucketed manner.\n",
      "     |      ... _ = spark.sql(\"DROP TABLE IF EXISTS bucketed_table\")\n",
      "     |      >>> spark.createDataFrame([\n",
      "     |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n",
      "     |      ...     schema=[\"age\", \"name\"]\n",
      "     |      ... ).write.bucketBy(2, \"name\").mode(\"overwrite\").saveAsTable(\"bucketed_table\")\n",
      "     |      >>> # Read the Parquet file as a DataFrame.\n",
      "     |      ... spark.read.table(\"bucketed_table\").sort(\"age\").show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      |120|Hyukjin Kwon|\n",
      "     |      |140| Haejoon Lee|\n",
      "     |      +---+------------+\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE bucketed_table\")\n",
      "     |  \n",
      "     |  csv(self, path: str, mode: Optional[str] = None, compression: Optional[str] = None, sep: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, header: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, escapeQuotes: Union[bool, str, NoneType] = None, quoteAll: Union[bool, str, NoneType] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, encoding: Optional[str] = None, emptyValue: Optional[str] = None, lineSep: Optional[str] = None) -> None\n",
      "     |      Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          the path in any Hadoop supported file system\n",
      "     |      mode : str, optional\n",
      "     |          specifies the behavior of the save operation when data already exists.\n",
      "     |      \n",
      "     |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "     |          * ``overwrite``: Overwrite existing data.\n",
      "     |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      "     |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n",
      "     |              exists.\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a CSV file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a CSV file\n",
      "     |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      "     |      ...     df.write.csv(d, mode=\"overwrite\")\n",
      "     |      ...\n",
      "     |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
      "     |      ...     spark.read.schema(df.schema).format(\"csv\").option(\n",
      "     |      ...         \"nullValue\", \"Hyukjin Kwon\").load(d).show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |100|NULL|\n",
      "     |      +---+----+\n",
      "     |  \n",
      "     |  format(self, source: str) -> 'DataFrameWriter'\n",
      "     |      Specifies the underlying output data source.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      source : str\n",
      "     |          string, name of the data source, e.g. 'json', 'parquet'.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.range(1).write.format('parquet')\n",
      "     |      <...readwriter.DataFrameWriter object ...>\n",
      "     |      \n",
      "     |      Write a DataFrame into a Parquet file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a Parquet file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the Parquet file as a DataFrame.\n",
      "     |      ...     spark.read.format('parquet').load(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  insertInto(self, tableName: str, overwrite: Optional[bool] = None) -> None\n",
      "     |      Inserts the content of the :class:`DataFrame` to the specified table.\n",
      "     |      \n",
      "     |      It requires that the schema of the :class:`DataFrame` is the same as the\n",
      "     |      schema of the table.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      overwrite : bool, optional\n",
      "     |          If true, overwrites existing data. Disabled by default\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Unlike :meth:`DataFrameWriter.saveAsTable`, :meth:`DataFrameWriter.insertInto` ignores\n",
      "     |      the column names and just uses position-based resolution.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tblA\")\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n",
      "     |      ...     schema=[\"age\", \"name\"]\n",
      "     |      ... )\n",
      "     |      >>> df.write.saveAsTable(\"tblA\")\n",
      "     |      \n",
      "     |      Insert the data into 'tblA' table but with different column names.\n",
      "     |      \n",
      "     |      >>> df.selectExpr(\"age AS col1\", \"name AS col2\").write.insertInto(\"tblA\")\n",
      "     |      >>> spark.read.table(\"tblA\").sort(\"age\").show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      |120|Hyukjin Kwon|\n",
      "     |      |120|Hyukjin Kwon|\n",
      "     |      |140| Haejoon Lee|\n",
      "     |      |140| Haejoon Lee|\n",
      "     |      +---+------------+\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n",
      "     |  \n",
      "     |  jdbc(self, url: str, table: str, mode: Optional[str] = None, properties: Optional[Dict[str, str]] = None) -> None\n",
      "     |      Saves the content of the :class:`DataFrame` to an external database table via JDBC.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      table : str\n",
      "     |          Name of the table in the external database.\n",
      "     |      mode : str, optional\n",
      "     |          specifies the behavior of the save operation when data already exists.\n",
      "     |      \n",
      "     |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "     |          * ``overwrite``: Overwrite existing data.\n",
      "     |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      "     |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "     |      properties : dict\n",
      "     |          a dictionary of JDBC database connection arguments. Normally at\n",
      "     |          least properties \"user\" and \"password\" with their corresponding values.\n",
      "     |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Don't create too many partitions in parallel on a large cluster;\n",
      "     |      otherwise Spark might crash your external database systems.\n",
      "     |  \n",
      "     |  json(self, path: str, mode: Optional[str] = None, compression: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, lineSep: Optional[str] = None, encoding: Optional[str] = None, ignoreNullFields: Union[bool, str, NoneType] = None) -> None\n",
      "     |      Saves the content of the :class:`DataFrame` in JSON format\n",
      "     |      (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\n",
      "     |      specified path.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          the path in any Hadoop supported file system\n",
      "     |      mode : str, optional\n",
      "     |          specifies the behavior of the save operation when data already exists.\n",
      "     |      \n",
      "     |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "     |          * ``overwrite``: Overwrite existing data.\n",
      "     |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      "     |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a JSON file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a JSON file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.json(d, mode=\"overwrite\")\n",
      "     |      ...\n",
      "     |      ...     # Read the JSON file as a DataFrame.\n",
      "     |      ...     spark.read.format(\"json\").load(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  mode(self, saveMode: Optional[str]) -> 'DataFrameWriter'\n",
      "     |      Specifies the behavior when data or table already exists.\n",
      "     |      \n",
      "     |      Options include:\n",
      "     |      \n",
      "     |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n",
      "     |      * `overwrite`: Overwrite existing data.\n",
      "     |      * `error` or `errorifexists`: Throw an exception if data already exists.\n",
      "     |      * `ignore`: Silently ignore this operation if data already exists.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Raise an error when writing to an existing path.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 80, \"name\": \"Xinrong Meng\"}]\n",
      "     |      ...     ).write.mode(\"error\").format(\"parquet\").save(d) # doctest: +SKIP\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      ...AnalysisException: ...\n",
      "     |      \n",
      "     |      Write a Parquet file back with various options, and read it back.\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Overwrite the path with a new Parquet file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Append another DataFrame into the Parquet file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 120, \"name\": \"Takuya Ueshin\"}]\n",
      "     |      ...     ).write.mode(\"append\").format(\"parquet\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Append another DataFrame into the Parquet file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 140, \"name\": \"Haejoon Lee\"}]\n",
      "     |      ...     ).write.mode(\"ignore\").format(\"parquet\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the Parquet file as a DataFrame.\n",
      "     |      ...     spark.read.parquet(d).show()\n",
      "     |      +---+-------------+\n",
      "     |      |age|         name|\n",
      "     |      +---+-------------+\n",
      "     |      |120|Takuya Ueshin|\n",
      "     |      |100| Hyukjin Kwon|\n",
      "     |      +---+-------------+\n",
      "     |  \n",
      "     |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameWriter'\n",
      "     |      Adds an output option for the underlying data source.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key : str\n",
      "     |          The key for the option to set.\n",
      "     |      value\n",
      "     |          The value for the option to set.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.range(1).write.option(\"key\", \"value\")\n",
      "     |      <...readwriter.DataFrameWriter object ...>\n",
      "     |      \n",
      "     |      Specify the option 'nullValue' with writing a CSV file.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a CSV file with 'nullValue' option set to 'Hyukjin Kwon'.\n",
      "     |      ...     df = spark.createDataFrame([(100, None)], \"age INT, name STRING\")\n",
      "     |      ...     df.write.option(\"nullValue\", \"Hyukjin Kwon\").mode(\"overwrite\").format(\"csv\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the CSV file as a DataFrame.\n",
      "     |      ...     spark.read.schema(df.schema).format('csv').load(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameWriter'\n",
      "     |      Adds output options for the underlying data source.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **options : dict\n",
      "     |          The dictionary of string keys and primitive-type values.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.range(1).write.option(\"key\", \"value\")\n",
      "     |      <...readwriter.DataFrameWriter object ...>\n",
      "     |      \n",
      "     |      Specify the option 'nullValue' and 'header' with writing a CSV file.\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
      "     |      >>> schema = StructType([\n",
      "     |      ...     StructField(\"age\",IntegerType(),True),\n",
      "     |      ...     StructField(\"name\",StringType(),True),\n",
      "     |      ... ])\n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a CSV file with 'nullValue' option set to 'Hyukjin Kwon',\n",
      "     |      ...     # and 'header' option set to `True`.\n",
      "     |      ...     df = spark.createDataFrame([(100, None)], schema=schema)\n",
      "     |      ...     df.write.options(nullValue=\"Hyukjin Kwon\", header=True).mode(\n",
      "     |      ...         \"overwrite\").format(\"csv\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the CSV file as a DataFrame.\n",
      "     |      ...     spark.read.option(\"header\", True).format('csv').load(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  orc(self, path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None\n",
      "     |      Saves the content of the :class:`DataFrame` in ORC format at the specified path.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          the path in any Hadoop supported file system\n",
      "     |      mode : str, optional\n",
      "     |          specifies the behavior of the save operation when data already exists.\n",
      "     |      \n",
      "     |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "     |          * ``overwrite``: Overwrite existing data.\n",
      "     |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      "     |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "     |      partitionBy : str or list, optional\n",
      "     |          names of partitioning columns\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a ORC file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a ORC file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.orc(d, mode=\"overwrite\")\n",
      "     |      ...\n",
      "     |      ...     # Read the Parquet file as a DataFrame.\n",
      "     |      ...     spark.read.format(\"orc\").load(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  parquet(self, path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None\n",
      "     |      Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          the path in any Hadoop supported file system\n",
      "     |      mode : str, optional\n",
      "     |          specifies the behavior of the save operation when data already exists.\n",
      "     |      \n",
      "     |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "     |          * ``overwrite``: Overwrite existing data.\n",
      "     |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      "     |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "     |      partitionBy : str or list, optional\n",
      "     |          names of partitioning columns\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a Parquet file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a Parquet file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.parquet(d, mode=\"overwrite\")\n",
      "     |      ...\n",
      "     |      ...     # Read the Parquet file as a DataFrame.\n",
      "     |      ...     spark.read.format(\"parquet\").load(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  partitionBy(self, *cols: Union[str, List[str]]) -> 'DataFrameWriter'\n",
      "     |      Partitions the output by the given columns on the file system.\n",
      "     |      \n",
      "     |      If specified, the output is laid out on the file system similar\n",
      "     |      to Hive's partitioning scheme.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str or list\n",
      "     |          name of columns\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a Parquet file in a partitioned manner, and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> import os\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a Parquet file in a partitioned manner.\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}, {\"age\": 120, \"name\": \"Ruifeng Zheng\"}]\n",
      "     |      ...     ).write.partitionBy(\"name\").mode(\"overwrite\").format(\"parquet\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the Parquet file as a DataFrame.\n",
      "     |      ...     spark.read.parquet(d).sort(\"age\").show()\n",
      "     |      ...\n",
      "     |      ...     # Read one partition as a DataFrame.\n",
      "     |      ...     spark.read.parquet(f\"{d}{os.path.sep}name=Hyukjin Kwon\").show()\n",
      "     |      +---+-------------+\n",
      "     |      |age|         name|\n",
      "     |      +---+-------------+\n",
      "     |      |100| Hyukjin Kwon|\n",
      "     |      |120|Ruifeng Zheng|\n",
      "     |      +---+-------------+\n",
      "     |      +---+\n",
      "     |      |age|\n",
      "     |      +---+\n",
      "     |      |100|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  save(self, path: Optional[str] = None, format: Optional[str] = None, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, **options: 'OptionalPrimitiveType') -> None\n",
      "     |      Saves the contents of the :class:`DataFrame` to a data source.\n",
      "     |      \n",
      "     |      The data source is specified by the ``format`` and a set of ``options``.\n",
      "     |      If ``format`` is not specified, the default data source configured by\n",
      "     |      ``spark.sql.sources.default`` will be used.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str, optional\n",
      "     |          the path in a Hadoop supported file system\n",
      "     |      format : str, optional\n",
      "     |          the format used to save\n",
      "     |      mode : str, optional\n",
      "     |          specifies the behavior of the save operation when data already exists.\n",
      "     |      \n",
      "     |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "     |          * ``overwrite``: Overwrite existing data.\n",
      "     |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      "     |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "     |      partitionBy : list, optional\n",
      "     |          names of partitioning columns\n",
      "     |      **options : dict\n",
      "     |          all other string options\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a JSON file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a JSON file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the JSON file as a DataFrame.\n",
      "     |      ...     spark.read.format('json').load(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  saveAsTable(self, name: str, format: Optional[str] = None, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, **options: 'OptionalPrimitiveType') -> None\n",
      "     |      Saves the content of the :class:`DataFrame` as the specified table.\n",
      "     |      \n",
      "     |      In the case the table already exists, behavior of this function depends on the\n",
      "     |      save mode, specified by the `mode` function (default to throwing an exception).\n",
      "     |      When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be\n",
      "     |      the same as that of the existing table.\n",
      "     |      \n",
      "     |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n",
      "     |      * `overwrite`: Overwrite existing data.\n",
      "     |      * `error` or `errorifexists`: Throw an exception if data already exists.\n",
      "     |      * `ignore`: Silently ignore this operation if data already exists.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      When `mode` is `Append`, if there is an existing table, we will use the format and\n",
      "     |      options of the existing table. The column order in the schema of the :class:`DataFrame`\n",
      "     |      doesn't need to be the same as that of the existing table. Unlike\n",
      "     |      :meth:`DataFrameWriter.insertInto`, :meth:`DataFrameWriter.saveAsTable` will use the\n",
      "     |      column names to find the correct column positions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          the table name\n",
      "     |      format : str, optional\n",
      "     |          the format used to save\n",
      "     |      mode : str, optional\n",
      "     |          one of `append`, `overwrite`, `error`, `errorifexists`, `ignore`             (default: error)\n",
      "     |      partitionBy : str or list\n",
      "     |          names of partitioning columns\n",
      "     |      **options : dict\n",
      "     |          all other string options\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Creates a table from a DataFrame, and read it back.\n",
      "     |      \n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tblA\")\n",
      "     |      >>> spark.createDataFrame([\n",
      "     |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n",
      "     |      ...     schema=[\"age\", \"name\"]\n",
      "     |      ... ).write.saveAsTable(\"tblA\")\n",
      "     |      >>> spark.read.table(\"tblA\").sort(\"age\").show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      |120|Hyukjin Kwon|\n",
      "     |      |140| Haejoon Lee|\n",
      "     |      +---+------------+\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n",
      "     |  \n",
      "     |  sortBy(self, col: Union[str, List[str], Tuple[str, ...]], *cols: Optional[str]) -> 'DataFrameWriter'\n",
      "     |      Sorts the output in each bucket by the given columns on the file system.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col : str, tuple or list\n",
      "     |          a name of a column, or a list of names.\n",
      "     |      cols : str\n",
      "     |          additional names (optional). If `col` is a list it should be empty.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a Parquet file in a sorted-buckted manner, and read it back.\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.functions import input_file_name\n",
      "     |      >>> # Write a DataFrame into a Parquet file in a sorted-bucketed manner.\n",
      "     |      ... _ = spark.sql(\"DROP TABLE IF EXISTS sorted_bucketed_table\")\n",
      "     |      >>> spark.createDataFrame([\n",
      "     |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n",
      "     |      ...     schema=[\"age\", \"name\"]\n",
      "     |      ... ).write.bucketBy(1, \"name\").sortBy(\"age\").mode(\n",
      "     |      ...     \"overwrite\").saveAsTable(\"sorted_bucketed_table\")\n",
      "     |      >>> # Read the Parquet file as a DataFrame.\n",
      "     |      ... spark.read.table(\"sorted_bucketed_table\").sort(\"age\").show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      |120|Hyukjin Kwon|\n",
      "     |      |140| Haejoon Lee|\n",
      "     |      +---+------------+\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE sorted_bucketed_table\")\n",
      "     |  \n",
      "     |  text(self, path: str, compression: Optional[str] = None, lineSep: Optional[str] = None) -> None\n",
      "     |      Saves the content of the DataFrame in a text file at the specified path.\n",
      "     |      The text files will be encoded as UTF-8.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          the path in any Hadoop supported file system\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The DataFrame must have only one column that is of string type.\n",
      "     |      Each row becomes a new line in the output file.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a text file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a text file\n",
      "     |      ...     df = spark.createDataFrame([(\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n",
      "     |      ...     df.write.mode(\"overwrite\").text(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the text file as a DataFrame.\n",
      "     |      ...     spark.read.schema(df.schema).format(\"text\").load(d).sort(\"alphabets\").show()\n",
      "     |      +---------+\n",
      "     |      |alphabets|\n",
      "     |      +---------+\n",
      "     |      |        a|\n",
      "     |      |        b|\n",
      "     |      |        c|\n",
      "     |      +---------+\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from OptionUtils:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DataFrameWriterV2(builtins.object)\n",
      "     |  DataFrameWriterV2(df: 'DataFrame', table: str)\n",
      "     |  \n",
      "     |  Interface used to write a class:`pyspark.sql.dataframe.DataFrame`\n",
      "     |  to external storage using the v2 API.\n",
      "     |  \n",
      "     |  .. versionadded:: 3.1.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df: 'DataFrame', table: str)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  append(self) -> None\n",
      "     |      Append the contents of the data frame to the output table.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  create(self) -> None\n",
      "     |      Create a new table from the contents of the data frame.\n",
      "     |      \n",
      "     |      The new table's schema, partition layout, properties, and other configuration will be\n",
      "     |      based on the configuration set on this writer.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  createOrReplace(self) -> None\n",
      "     |      Create a new table or replace an existing table with the contents of the data frame.\n",
      "     |      \n",
      "     |      The output table's schema, partition layout, properties,\n",
      "     |      and other configuration will be based on the contents of the data frame\n",
      "     |      and the configuration set on this writer.\n",
      "     |      If the table exists, its configuration and data will be replaced.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameWriterV2'\n",
      "     |      Add a write option.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameWriterV2'\n",
      "     |      Add write options.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  overwrite(self, condition: pyspark.sql.column.Column) -> None\n",
      "     |      Overwrite rows matching the given filter condition with the contents of the data frame in\n",
      "     |      the output table.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  overwritePartitions(self) -> None\n",
      "     |      Overwrite all partition for which the data frame contains at least one row with the contents\n",
      "     |      of the data frame in the output table.\n",
      "     |      \n",
      "     |      This operation is equivalent to Hive's `INSERT OVERWRITE ... PARTITION`, which replaces\n",
      "     |      partitions dynamically depending on the contents of the data frame.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  partitionedBy(self, col: pyspark.sql.column.Column, *cols: pyspark.sql.column.Column) -> 'DataFrameWriterV2'\n",
      "     |      Partition the output table created by `create`, `createOrReplace`, or `replace` using\n",
      "     |      the given columns or transforms.\n",
      "     |      \n",
      "     |      When specified, the table data will be stored by these values for efficient reads.\n",
      "     |      \n",
      "     |      For example, when a table is partitioned by day, it may be stored\n",
      "     |      in a directory layout like:\n",
      "     |      \n",
      "     |      * `table/day=2019-06-01/`\n",
      "     |      * `table/day=2019-06-02/`\n",
      "     |      \n",
      "     |      Partitioning is one of the most widely used techniques to optimize physical data layout.\n",
      "     |      It provides a coarse-grained index for skipping unnecessary data reads when queries have\n",
      "     |      predicates on the partitioned columns. In order for partitioning to work well, the number\n",
      "     |      of distinct values in each column should typically be less than tens of thousands.\n",
      "     |      \n",
      "     |      `col` and `cols` support only the following functions:\n",
      "     |      \n",
      "     |      * :py:func:`pyspark.sql.functions.years`\n",
      "     |      * :py:func:`pyspark.sql.functions.months`\n",
      "     |      * :py:func:`pyspark.sql.functions.days`\n",
      "     |      * :py:func:`pyspark.sql.functions.hours`\n",
      "     |      * :py:func:`pyspark.sql.functions.bucket`\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  replace(self) -> None\n",
      "     |      Replace an existing table with the contents of the data frame.\n",
      "     |      \n",
      "     |      The existing table's schema, partition layout, properties, and other configuration will be\n",
      "     |      replaced with the contents of the data frame and the configuration set on this writer.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  tableProperty(self, property: str, value: str) -> 'DataFrameWriterV2'\n",
      "     |      Add table property.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  using(self, provider: str) -> 'DataFrameWriterV2'\n",
      "     |      Specifies a provider for the underlying output data source.\n",
      "     |      Spark's default catalog supports \"parquet\", \"json\", etc.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class GroupedData(pyspark.sql.pandas.group_ops.PandasGroupedOpsMixin)\n",
      "     |  GroupedData(jgd: py4j.java_gateway.JavaObject, df: pyspark.sql.dataframe.DataFrame)\n",
      "     |  \n",
      "     |  A set of methods for aggregations on a :class:`DataFrame`,\n",
      "     |  created by :func:`DataFrame.groupBy`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.3.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GroupedData\n",
      "     |      pyspark.sql.pandas.group_ops.PandasGroupedOpsMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, jgd: py4j.java_gateway.JavaObject, df: pyspark.sql.dataframe.DataFrame)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  agg(self, *exprs: Union[pyspark.sql.column.Column, Dict[str, str]]) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Compute aggregates and returns the result as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The available aggregate functions can be:\n",
      "     |      \n",
      "     |      1. built-in aggregation functions, such as `avg`, `max`, `min`, `sum`, `count`\n",
      "     |      \n",
      "     |      2. group aggregate pandas UDFs, created with :func:`pyspark.sql.functions.pandas_udf`\n",
      "     |      \n",
      "     |         .. note:: There is no partial aggregation with group aggregate UDFs, i.e.,\n",
      "     |             a full shuffle is required. Also, all the data of a group will be loaded into\n",
      "     |             memory, so the user should be aware of the potential OOM risk if data is skewed\n",
      "     |             and certain groups are too large to fit in memory.\n",
      "     |      \n",
      "     |         .. seealso:: :func:`pyspark.sql.functions.pandas_udf`\n",
      "     |      \n",
      "     |      If ``exprs`` is a single :class:`dict` mapping from string to string, then the key\n",
      "     |      is the column to perform aggregation on, and the value is the aggregate function.\n",
      "     |      \n",
      "     |      Alternatively, ``exprs`` can also be a list of aggregate :class:`Column` expressions.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      exprs : dict\n",
      "     |          a dict mapping from column name (string) to aggregate functions (string),\n",
      "     |          or a list of :class:`Column`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Built-in aggregation functions and group aggregate pandas UDFs cannot be mixed\n",
      "     |      in a single call to this function.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import functions as sf\n",
      "     |      >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (3, \"Alice\"), (5, \"Bob\"), (10, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  2|Alice|\n",
      "     |      |  3|Alice|\n",
      "     |      |  5|  Bob|\n",
      "     |      | 10|  Bob|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Group-by name, and count each group.\n",
      "     |      \n",
      "     |      >>> df.groupBy(df.name)\n",
      "     |      GroupedData[grouping...: [name...], value: [age: bigint, name: string], type: GroupBy]\n",
      "     |      \n",
      "     |      >>> df.groupBy(df.name).agg({\"*\": \"count\"}).sort(\"name\").show()\n",
      "     |      +-----+--------+\n",
      "     |      | name|count(1)|\n",
      "     |      +-----+--------+\n",
      "     |      |Alice|       2|\n",
      "     |      |  Bob|       2|\n",
      "     |      +-----+--------+\n",
      "     |      \n",
      "     |      Group-by name, and calculate the minimum age.\n",
      "     |      \n",
      "     |      >>> df.groupBy(df.name).agg(sf.min(df.age)).sort(\"name\").show()\n",
      "     |      +-----+--------+\n",
      "     |      | name|min(age)|\n",
      "     |      +-----+--------+\n",
      "     |      |Alice|       2|\n",
      "     |      |  Bob|       5|\n",
      "     |      +-----+--------+\n",
      "     |      \n",
      "     |      Same as above but uses pandas UDF.\n",
      "     |      \n",
      "     |      >>> @pandas_udf('int', PandasUDFType.GROUPED_AGG)  # doctest: +SKIP\n",
      "     |      ... def min_udf(v):\n",
      "     |      ...     return v.min()\n",
      "     |      ...\n",
      "     |      >>> df.groupBy(df.name).agg(min_udf(df.age)).sort(\"name\").show()  # doctest: +SKIP\n",
      "     |      +-----+------------+\n",
      "     |      | name|min_udf(age)|\n",
      "     |      +-----+------------+\n",
      "     |      |Alice|           2|\n",
      "     |      |  Bob|           5|\n",
      "     |      +-----+------------+\n",
      "     |  \n",
      "     |  avg(self: 'GroupedData', *cols: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Computes average values for each numeric columns for each group.\n",
      "     |      \n",
      "     |      :func:`mean` is an alias for :func:`avg`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str\n",
      "     |          column names. Non-numeric columns are ignored.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\", 80), (3, \"Alice\", 100),\n",
      "     |      ...     (5, \"Bob\", 120), (10, \"Bob\", 140)], [\"age\", \"name\", \"height\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+-----+------+\n",
      "     |      |age| name|height|\n",
      "     |      +---+-----+------+\n",
      "     |      |  2|Alice|    80|\n",
      "     |      |  3|Alice|   100|\n",
      "     |      |  5|  Bob|   120|\n",
      "     |      | 10|  Bob|   140|\n",
      "     |      +---+-----+------+\n",
      "     |      \n",
      "     |      Group-by name, and calculate the mean of the age in each group.\n",
      "     |      \n",
      "     |      >>> df.groupBy(\"name\").avg('age').sort(\"name\").show()\n",
      "     |      +-----+--------+\n",
      "     |      | name|avg(age)|\n",
      "     |      +-----+--------+\n",
      "     |      |Alice|     2.5|\n",
      "     |      |  Bob|     7.5|\n",
      "     |      +-----+--------+\n",
      "     |      \n",
      "     |      Calculate the mean of the age and height in all data.\n",
      "     |      \n",
      "     |      >>> df.groupBy().avg('age', 'height').show()\n",
      "     |      +--------+-----------+\n",
      "     |      |avg(age)|avg(height)|\n",
      "     |      +--------+-----------+\n",
      "     |      |     5.0|      110.0|\n",
      "     |      +--------+-----------+\n",
      "     |  \n",
      "     |  count(self: 'GroupedData') -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Counts the number of records for each group.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (3, \"Alice\"), (5, \"Bob\"), (10, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  2|Alice|\n",
      "     |      |  3|Alice|\n",
      "     |      |  5|  Bob|\n",
      "     |      | 10|  Bob|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Group-by name, and count each group.\n",
      "     |      \n",
      "     |      >>> df.groupBy(df.name).count().sort(\"name\").show()\n",
      "     |      +-----+-----+\n",
      "     |      | name|count|\n",
      "     |      +-----+-----+\n",
      "     |      |Alice|    2|\n",
      "     |      |  Bob|    2|\n",
      "     |      +-----+-----+\n",
      "     |  \n",
      "     |  max(self: 'GroupedData', *cols: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Computes the max value for each numeric columns for each group.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\", 80), (3, \"Alice\", 100),\n",
      "     |      ...     (5, \"Bob\", 120), (10, \"Bob\", 140)], [\"age\", \"name\", \"height\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+-----+------+\n",
      "     |      |age| name|height|\n",
      "     |      +---+-----+------+\n",
      "     |      |  2|Alice|    80|\n",
      "     |      |  3|Alice|   100|\n",
      "     |      |  5|  Bob|   120|\n",
      "     |      | 10|  Bob|   140|\n",
      "     |      +---+-----+------+\n",
      "     |      \n",
      "     |      Group-by name, and calculate the max of the age in each group.\n",
      "     |      \n",
      "     |      >>> df.groupBy(\"name\").max(\"age\").sort(\"name\").show()\n",
      "     |      +-----+--------+\n",
      "     |      | name|max(age)|\n",
      "     |      +-----+--------+\n",
      "     |      |Alice|       3|\n",
      "     |      |  Bob|      10|\n",
      "     |      +-----+--------+\n",
      "     |      \n",
      "     |      Calculate the max of the age and height in all data.\n",
      "     |      \n",
      "     |      >>> df.groupBy().max(\"age\", \"height\").show()\n",
      "     |      +--------+-----------+\n",
      "     |      |max(age)|max(height)|\n",
      "     |      +--------+-----------+\n",
      "     |      |      10|        140|\n",
      "     |      +--------+-----------+\n",
      "     |  \n",
      "     |  mean(self: 'GroupedData', *cols: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Computes average values for each numeric columns for each group.\n",
      "     |      \n",
      "     |      :func:`mean` is an alias for :func:`avg`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str\n",
      "     |          column names. Non-numeric columns are ignored.\n",
      "     |  \n",
      "     |  min(self: 'GroupedData', *cols: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Computes the min value for each numeric column for each group.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str\n",
      "     |          column names. Non-numeric columns are ignored.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\", 80), (3, \"Alice\", 100),\n",
      "     |      ...     (5, \"Bob\", 120), (10, \"Bob\", 140)], [\"age\", \"name\", \"height\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+-----+------+\n",
      "     |      |age| name|height|\n",
      "     |      +---+-----+------+\n",
      "     |      |  2|Alice|    80|\n",
      "     |      |  3|Alice|   100|\n",
      "     |      |  5|  Bob|   120|\n",
      "     |      | 10|  Bob|   140|\n",
      "     |      +---+-----+------+\n",
      "     |      \n",
      "     |      Group-by name, and calculate the min of the age in each group.\n",
      "     |      \n",
      "     |      >>> df.groupBy(\"name\").min(\"age\").sort(\"name\").show()\n",
      "     |      +-----+--------+\n",
      "     |      | name|min(age)|\n",
      "     |      +-----+--------+\n",
      "     |      |Alice|       2|\n",
      "     |      |  Bob|       5|\n",
      "     |      +-----+--------+\n",
      "     |      \n",
      "     |      Calculate the min of the age and height in all data.\n",
      "     |      \n",
      "     |      >>> df.groupBy().min(\"age\", \"height\").show()\n",
      "     |      +--------+-----------+\n",
      "     |      |min(age)|min(height)|\n",
      "     |      +--------+-----------+\n",
      "     |      |       2|         80|\n",
      "     |      +--------+-----------+\n",
      "     |  \n",
      "     |  pivot(self, pivot_col: str, values: Optional[List[ForwardRef('LiteralType')]] = None) -> 'GroupedData'\n",
      "     |      Pivots a column of the current :class:`DataFrame` and perform the specified aggregation.\n",
      "     |      There are two versions of the pivot function: one that requires the caller\n",
      "     |      to specify the list of distinct values to pivot on, and one that does not.\n",
      "     |      The latter is more concise but less efficient,\n",
      "     |      because Spark needs to first compute the list of distinct values internally.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      pivot_col : str\n",
      "     |          Name of the column to pivot.\n",
      "     |      values : list, optional\n",
      "     |          List of values that will be translated to columns in the output DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df1 = spark.createDataFrame([\n",
      "     |      ...     Row(course=\"dotNET\", year=2012, earnings=10000),\n",
      "     |      ...     Row(course=\"Java\", year=2012, earnings=20000),\n",
      "     |      ...     Row(course=\"dotNET\", year=2012, earnings=5000),\n",
      "     |      ...     Row(course=\"dotNET\", year=2013, earnings=48000),\n",
      "     |      ...     Row(course=\"Java\", year=2013, earnings=30000),\n",
      "     |      ... ])\n",
      "     |      >>> df1.show()\n",
      "     |      +------+----+--------+\n",
      "     |      |course|year|earnings|\n",
      "     |      +------+----+--------+\n",
      "     |      |dotNET|2012|   10000|\n",
      "     |      |  Java|2012|   20000|\n",
      "     |      |dotNET|2012|    5000|\n",
      "     |      |dotNET|2013|   48000|\n",
      "     |      |  Java|2013|   30000|\n",
      "     |      +------+----+--------+\n",
      "     |      >>> df2 = spark.createDataFrame([\n",
      "     |      ...     Row(training=\"expert\", sales=Row(course=\"dotNET\", year=2012, earnings=10000)),\n",
      "     |      ...     Row(training=\"junior\", sales=Row(course=\"Java\", year=2012, earnings=20000)),\n",
      "     |      ...     Row(training=\"expert\", sales=Row(course=\"dotNET\", year=2012, earnings=5000)),\n",
      "     |      ...     Row(training=\"junior\", sales=Row(course=\"dotNET\", year=2013, earnings=48000)),\n",
      "     |      ...     Row(training=\"expert\", sales=Row(course=\"Java\", year=2013, earnings=30000)),\n",
      "     |      ... ])  # doctest: +SKIP\n",
      "     |      >>> df2.show()  # doctest: +SKIP\n",
      "     |      +--------+--------------------+\n",
      "     |      |training|               sales|\n",
      "     |      +--------+--------------------+\n",
      "     |      |  expert|{dotNET, 2012, 10...|\n",
      "     |      |  junior| {Java, 2012, 20000}|\n",
      "     |      |  expert|{dotNET, 2012, 5000}|\n",
      "     |      |  junior|{dotNET, 2013, 48...|\n",
      "     |      |  expert| {Java, 2013, 30000}|\n",
      "     |      +--------+--------------------+\n",
      "     |      \n",
      "     |      Compute the sum of earnings for each year by course with each course as a separate column\n",
      "     |      \n",
      "     |      >>> df1.groupBy(\"year\").pivot(\"course\", [\"dotNET\", \"Java\"]).sum(\"earnings\").show()\n",
      "     |      +----+------+-----+\n",
      "     |      |year|dotNET| Java|\n",
      "     |      +----+------+-----+\n",
      "     |      |2012| 15000|20000|\n",
      "     |      |2013| 48000|30000|\n",
      "     |      +----+------+-----+\n",
      "     |      \n",
      "     |      Or without specifying column values (less efficient)\n",
      "     |      \n",
      "     |      >>> df1.groupBy(\"year\").pivot(\"course\").sum(\"earnings\").show()\n",
      "     |      +----+-----+------+\n",
      "     |      |year| Java|dotNET|\n",
      "     |      +----+-----+------+\n",
      "     |      |2012|20000| 15000|\n",
      "     |      |2013|30000| 48000|\n",
      "     |      +----+-----+------+\n",
      "     |      >>> df2.groupBy(\"sales.year\").pivot(\"sales.course\").sum(\"sales.earnings\").show()\n",
      "     |      ... # doctest: +SKIP\n",
      "     |      +----+-----+------+\n",
      "     |      |year| Java|dotNET|\n",
      "     |      +----+-----+------+\n",
      "     |      |2012|20000| 15000|\n",
      "     |      |2013|30000| 48000|\n",
      "     |      +----+-----+------+\n",
      "     |  \n",
      "     |  sum(self: 'GroupedData', *cols: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Computes the sum for each numeric columns for each group.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str\n",
      "     |          column names. Non-numeric columns are ignored.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\", 80), (3, \"Alice\", 100),\n",
      "     |      ...     (5, \"Bob\", 120), (10, \"Bob\", 140)], [\"age\", \"name\", \"height\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+-----+------+\n",
      "     |      |age| name|height|\n",
      "     |      +---+-----+------+\n",
      "     |      |  2|Alice|    80|\n",
      "     |      |  3|Alice|   100|\n",
      "     |      |  5|  Bob|   120|\n",
      "     |      | 10|  Bob|   140|\n",
      "     |      +---+-----+------+\n",
      "     |      \n",
      "     |      Group-by name, and calculate the sum of the age in each group.\n",
      "     |      \n",
      "     |      >>> df.groupBy(\"name\").sum(\"age\").sort(\"name\").show()\n",
      "     |      +-----+--------+\n",
      "     |      | name|sum(age)|\n",
      "     |      +-----+--------+\n",
      "     |      |Alice|       5|\n",
      "     |      |  Bob|      15|\n",
      "     |      +-----+--------+\n",
      "     |      \n",
      "     |      Calculate the sum of the age and height in all data.\n",
      "     |      \n",
      "     |      >>> df.groupBy().sum(\"age\", \"height\").show()\n",
      "     |      +--------+-----------+\n",
      "     |      |sum(age)|sum(height)|\n",
      "     |      +--------+-----------+\n",
      "     |      |      20|        440|\n",
      "     |      +--------+-----------+\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pyspark.sql.pandas.group_ops.PandasGroupedOpsMixin:\n",
      "     |  \n",
      "     |  apply(self, udf: 'GroupedMapPandasUserDefinedFunction') -> pyspark.sql.dataframe.DataFrame\n",
      "     |      It is an alias of :meth:`pyspark.sql.GroupedData.applyInPandas`; however, it takes a\n",
      "     |      :meth:`pyspark.sql.functions.pandas_udf` whereas\n",
      "     |      :meth:`pyspark.sql.GroupedData.applyInPandas` takes a Python native function.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Support Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      udf : :func:`pyspark.sql.functions.pandas_udf`\n",
      "     |          a grouped map user-defined function returned by\n",
      "     |          :func:`pyspark.sql.functions.pandas_udf`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      It is preferred to use :meth:`pyspark.sql.GroupedData.applyInPandas` over this\n",
      "     |      API. This API will be deprecated in the future releases.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
      "     |      ...     (\"id\", \"v\"))\n",
      "     |      >>> @pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\n",
      "     |      ... def normalize(pdf):\n",
      "     |      ...     v = pdf.v\n",
      "     |      ...     return pdf.assign(v=(v - v.mean()) / v.std())\n",
      "     |      ...\n",
      "     |      >>> df.groupby(\"id\").apply(normalize).show()  # doctest: +SKIP\n",
      "     |      +---+-------------------+\n",
      "     |      | id|                  v|\n",
      "     |      +---+-------------------+\n",
      "     |      |  1|-0.7071067811865475|\n",
      "     |      |  1| 0.7071067811865475|\n",
      "     |      |  2|-0.8320502943378437|\n",
      "     |      |  2|-0.2773500981126146|\n",
      "     |      |  2| 1.1094003924504583|\n",
      "     |      +---+-------------------+\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.functions.pandas_udf\n",
      "     |  \n",
      "     |  applyInPandas(self, func: 'PandasGroupedMapFunction', schema: Union[pyspark.sql.types.StructType, str]) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Maps each group of the current :class:`DataFrame` using a pandas udf and returns the result\n",
      "     |      as a `DataFrame`.\n",
      "     |      \n",
      "     |      The function should take a `pandas.DataFrame` and return another\n",
      "     |      `pandas.DataFrame`. Alternatively, the user can pass a function that takes\n",
      "     |      a tuple of the grouping key(s) and a `pandas.DataFrame`.\n",
      "     |      For each group, all columns are passed together as a `pandas.DataFrame`\n",
      "     |      to the user-function and the returned `pandas.DataFrame` are combined as a\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The `schema` should be a :class:`StructType` describing the schema of the returned\n",
      "     |      `pandas.DataFrame`. The column labels of the returned `pandas.DataFrame` must either match\n",
      "     |      the field names in the defined schema if specified as strings, or match the\n",
      "     |      field data types by position if not strings, e.g. integer indices.\n",
      "     |      The length of the returned `pandas.DataFrame` can be arbitrary.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Support Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : function\n",
      "     |          a Python native function that takes a `pandas.DataFrame` and outputs a\n",
      "     |          `pandas.DataFrame`, or that takes one tuple (grouping keys) and a\n",
      "     |          `pandas.DataFrame` and outputs a `pandas.DataFrame`.\n",
      "     |      schema : :class:`pyspark.sql.types.DataType` or str\n",
      "     |          the return type of the `func` in PySpark. The value can be either a\n",
      "     |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import pandas as pd  # doctest: +SKIP\n",
      "     |      >>> from pyspark.sql.functions import pandas_udf, ceil\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
      "     |      ...     (\"id\", \"v\"))  # doctest: +SKIP\n",
      "     |      >>> def normalize(pdf):\n",
      "     |      ...     v = pdf.v\n",
      "     |      ...     return pdf.assign(v=(v - v.mean()) / v.std())\n",
      "     |      ...\n",
      "     |      >>> df.groupby(\"id\").applyInPandas(\n",
      "     |      ...     normalize, schema=\"id long, v double\").show()  # doctest: +SKIP\n",
      "     |      +---+-------------------+\n",
      "     |      | id|                  v|\n",
      "     |      +---+-------------------+\n",
      "     |      |  1|-0.7071067811865475|\n",
      "     |      |  1| 0.7071067811865475|\n",
      "     |      |  2|-0.8320502943378437|\n",
      "     |      |  2|-0.2773500981126146|\n",
      "     |      |  2| 1.1094003924504583|\n",
      "     |      +---+-------------------+\n",
      "     |      \n",
      "     |      Alternatively, the user can pass a function that takes two arguments.\n",
      "     |      In this case, the grouping key(s) will be passed as the first argument and the data will\n",
      "     |      be passed as the second argument. The grouping key(s) will be passed as a tuple of numpy\n",
      "     |      data types, e.g., `numpy.int32` and `numpy.float64`. The data will still be passed in\n",
      "     |      as a `pandas.DataFrame` containing all columns from the original Spark DataFrame.\n",
      "     |      This is useful when the user does not want to hardcode grouping key(s) in the function.\n",
      "     |      \n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
      "     |      ...     (\"id\", \"v\"))  # doctest: +SKIP\n",
      "     |      >>> def mean_func(key, pdf):\n",
      "     |      ...     # key is a tuple of one numpy.int64, which is the value\n",
      "     |      ...     # of 'id' for the current group\n",
      "     |      ...     return pd.DataFrame([key + (pdf.v.mean(),)])\n",
      "     |      ...\n",
      "     |      >>> df.groupby('id').applyInPandas(\n",
      "     |      ...     mean_func, schema=\"id long, v double\").show()  # doctest: +SKIP\n",
      "     |      +---+---+\n",
      "     |      | id|  v|\n",
      "     |      +---+---+\n",
      "     |      |  1|1.5|\n",
      "     |      |  2|6.0|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      >>> def sum_func(key, pdf):\n",
      "     |      ...     # key is a tuple of two numpy.int64s, which is the values\n",
      "     |      ...     # of 'id' and 'ceil(df.v / 2)' for the current group\n",
      "     |      ...     return pd.DataFrame([key + (pdf.v.sum(),)])\n",
      "     |      ...\n",
      "     |      >>> df.groupby(df.id, ceil(df.v / 2)).applyInPandas(\n",
      "     |      ...     sum_func, schema=\"id long, `ceil(v / 2)` long, v double\").show()  # doctest: +SKIP\n",
      "     |      +---+-----------+----+\n",
      "     |      | id|ceil(v / 2)|   v|\n",
      "     |      +---+-----------+----+\n",
      "     |      |  2|          5|10.0|\n",
      "     |      |  1|          1| 3.0|\n",
      "     |      |  2|          3| 5.0|\n",
      "     |      |  2|          2| 3.0|\n",
      "     |      +---+-----------+----+\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This function requires a full shuffle. All the data of a group will be loaded\n",
      "     |      into memory, so the user should be aware of the potential OOM risk if data is skewed\n",
      "     |      and certain groups are too large to fit in memory.\n",
      "     |      \n",
      "     |      This API is experimental.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.functions.pandas_udf\n",
      "     |  \n",
      "     |  applyInPandasWithState(self, func: 'PandasGroupedMapFunctionWithState', outputStructType: Union[pyspark.sql.types.StructType, str], stateStructType: Union[pyspark.sql.types.StructType, str], outputMode: str, timeoutConf: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Applies the given function to each group of data, while maintaining a user-defined\n",
      "     |      per-group state. The result Dataset will represent the flattened record returned by the\n",
      "     |      function.\n",
      "     |      \n",
      "     |      For a streaming :class:`DataFrame`, the function will be invoked first for all input groups\n",
      "     |      and then for all timed out states where the input data is set to be empty. Updates to each\n",
      "     |      group's state will be saved across invocations.\n",
      "     |      \n",
      "     |      The function should take parameters (key, Iterator[`pandas.DataFrame`], state) and\n",
      "     |      return another Iterator[`pandas.DataFrame`]. The grouping key(s) will be passed as a tuple\n",
      "     |      of numpy data types, e.g., `numpy.int32` and `numpy.float64`. The state will be passed as\n",
      "     |      :class:`pyspark.sql.streaming.state.GroupState`.\n",
      "     |      \n",
      "     |      For each group, all columns are passed together as `pandas.DataFrame` to the user-function,\n",
      "     |      and the returned `pandas.DataFrame` across all invocations are combined as a\n",
      "     |      :class:`DataFrame`. Note that the user function should not make a guess of the number of\n",
      "     |      elements in the iterator. To process all data, the user function needs to iterate all\n",
      "     |      elements and process them. On the other hand, the user function is not strictly required to\n",
      "     |      iterate through all elements in the iterator if it intends to read a part of data.\n",
      "     |      \n",
      "     |      The `outputStructType` should be a :class:`StructType` describing the schema of all\n",
      "     |      elements in the returned value, `pandas.DataFrame`. The column labels of all elements in\n",
      "     |      returned `pandas.DataFrame` must either match the field names in the defined schema if\n",
      "     |      specified as strings, or match the field data types by position if not strings,\n",
      "     |      e.g. integer indices.\n",
      "     |      \n",
      "     |      The `stateStructType` should be :class:`StructType` describing the schema of the\n",
      "     |      user-defined state. The value of the state will be presented as a tuple, as well as the\n",
      "     |      update should be performed with the tuple. The corresponding Python types for\n",
      "     |      :class:DataType are supported. Please refer to the page\n",
      "     |      https://spark.apache.org/docs/latest/sql-ref-datatypes.html (Python tab).\n",
      "     |      \n",
      "     |      The size of each `pandas.DataFrame` in both the input and output can be arbitrary. The\n",
      "     |      number of `pandas.DataFrame` in both the input and output can also be arbitrary.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : function\n",
      "     |          a Python native function to be called on every group. It should take parameters\n",
      "     |          (key, Iterator[`pandas.DataFrame`], state) and return Iterator[`pandas.DataFrame`].\n",
      "     |          Note that the type of the key is tuple and the type of the state is\n",
      "     |          :class:`pyspark.sql.streaming.state.GroupState`.\n",
      "     |      outputStructType : :class:`pyspark.sql.types.DataType` or str\n",
      "     |          the type of the output records. The value can be either a\n",
      "     |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "     |      stateStructType : :class:`pyspark.sql.types.DataType` or str\n",
      "     |          the type of the user-defined state. The value can be either a\n",
      "     |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "     |      outputMode : str\n",
      "     |          the output mode of the function.\n",
      "     |      timeoutConf : str\n",
      "     |          timeout configuration for groups that do not receive data for a while. valid values\n",
      "     |          are defined in :class:`pyspark.sql.streaming.state.GroupStateTimeout`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import pandas as pd  # doctest: +SKIP\n",
      "     |      >>> from pyspark.sql.streaming.state import GroupStateTimeout\n",
      "     |      >>> def count_fn(key, pdf_iter, state):\n",
      "     |      ...     assert isinstance(state, GroupStateImpl)\n",
      "     |      ...     total_len = 0\n",
      "     |      ...     for pdf in pdf_iter:\n",
      "     |      ...         total_len += len(pdf)\n",
      "     |      ...     state.update((total_len,))\n",
      "     |      ...     yield pd.DataFrame({\"id\": [key[0]], \"countAsString\": [str(total_len)]})\n",
      "     |      ...\n",
      "     |      >>> df.groupby(\"id\").applyInPandasWithState(\n",
      "     |      ...     count_fn, outputStructType=\"id long, countAsString string\",\n",
      "     |      ...     stateStructType=\"len long\", outputMode=\"Update\",\n",
      "     |      ...     timeoutConf=GroupStateTimeout.NoTimeout) # doctest: +SKIP\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This function requires a full shuffle.\n",
      "     |      \n",
      "     |      This API is experimental.\n",
      "     |  \n",
      "     |  cogroup(self, other: 'GroupedData') -> 'PandasCogroupedOps'\n",
      "     |      Cogroups this group with another group so that we can run cogrouped operations.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Support Spark Connect.\n",
      "     |      \n",
      "     |      See :class:`PandasCogroupedOps` for the operations that can be run.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pyspark.sql.pandas.group_ops.PandasGroupedOpsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class HiveContext(SQLContext)\n",
      "     |  HiveContext(sparkContext: pyspark.context.SparkContext, sparkSession: Optional[pyspark.sql.session.SparkSession] = None, jhiveContext: Optional[py4j.java_gateway.JavaObject] = None)\n",
      "     |  \n",
      "     |  A variant of Spark SQL that integrates with data stored in Hive.\n",
      "     |  \n",
      "     |  Configuration for Hive is read from ``hive-site.xml`` on the classpath.\n",
      "     |  It supports running both SQL and HiveQL commands.\n",
      "     |  \n",
      "     |  .. deprecated:: 2.0.0\n",
      "     |      Use SparkSession.builder.enableHiveSupport().getOrCreate().\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  sparkContext : :class:`SparkContext`\n",
      "     |      The SparkContext to wrap.\n",
      "     |  jhiveContext : optional\n",
      "     |      An optional JVM Scala HiveContext. If set, we do not instantiate a new\n",
      "     |      :class:`HiveContext` in the JVM, instead we make all calls to this object.\n",
      "     |      This is only for internal use.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HiveContext\n",
      "     |      SQLContext\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sparkContext: pyspark.context.SparkContext, sparkSession: Optional[pyspark.sql.session.SparkSession] = None, jhiveContext: Optional[py4j.java_gateway.JavaObject] = None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  refreshTable(self, tableName: str) -> None\n",
      "     |      Invalidate and refresh all the cached metadata of the given\n",
      "     |      table. For performance reasons, Spark SQL or the external data source\n",
      "     |      library it uses might cache certain metadata about a table, such as the\n",
      "     |      location of blocks. When those change outside of Spark SQL, users should\n",
      "     |      call this function to invalidate the cache.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SQLContext:\n",
      "     |  \n",
      "     |  cacheTable(self, tableName: str) -> None\n",
      "     |      Caches the specified table in-memory.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  clearCache(self) -> None\n",
      "     |      Removes all cached tables from the in-memory cache.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  createDataFrame(self, data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      "     |      \n",
      "     |      When ``schema`` is a list of column names, the type of each column\n",
      "     |      will be inferred from ``data``.\n",
      "     |      \n",
      "     |      When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "     |      from ``data``, which should be an RDD of :class:`Row`,\n",
      "     |      or :class:`namedtuple`, or :class:`dict`.\n",
      "     |      \n",
      "     |      When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string it must match\n",
      "     |      the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      "     |      :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "     |      :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\",\n",
      "     |      each record will also be wrapped into a tuple, which can be converted to row later.\n",
      "     |      \n",
      "     |      If schema inference is needed, ``samplingRatio`` is used to determine the ratio of\n",
      "     |      rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 2.0.0\n",
      "     |         The ``schema`` parameter can be a :class:`pyspark.sql.types.DataType` or a\n",
      "     |         datatype string after 2.0.\n",
      "     |         If it's not a :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "     |         :class:`pyspark.sql.types.StructType` and each record will also be wrapped into a tuple.\n",
      "     |      \n",
      "     |      .. versionchanged:: 2.1.0\n",
      "     |         Added verifySchema.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : :class:`RDD` or iterable\n",
      "     |          an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "     |          :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
      "     |          :class:`pandas.DataFrame`.\n",
      "     |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "     |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "     |          column names, default is None.  The data type string format equals to\n",
      "     |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "     |          omit the ``struct<>``.\n",
      "     |      samplingRatio : float, optional\n",
      "     |          the sample ratio of rows used for inferring\n",
      "     |      verifySchema : bool, optional\n",
      "     |          verify data types of every row against schema. Enabled by default.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> l = [('Alice', 1)]\n",
      "     |      >>> sqlContext.createDataFrame(l).collect()\n",
      "     |      [Row(_1='Alice', _2=1)]\n",
      "     |      >>> sqlContext.createDataFrame(l, ['name', 'age']).collect()\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      \n",
      "     |      >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "     |      >>> sqlContext.createDataFrame(d).collect()\n",
      "     |      [Row(age=1, name='Alice')]\n",
      "     |      \n",
      "     |      >>> rdd = sc.parallelize(l)\n",
      "     |      >>> sqlContext.createDataFrame(rdd).collect()\n",
      "     |      [Row(_1='Alice', _2=1)]\n",
      "     |      >>> df = sqlContext.createDataFrame(rdd, ['name', 'age'])\n",
      "     |      >>> df.collect()\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      \n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> Person = Row('name', 'age')\n",
      "     |      >>> person = rdd.map(lambda r: Person(*r))\n",
      "     |      >>> df2 = sqlContext.createDataFrame(person)\n",
      "     |      >>> df2.collect()\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.types import *\n",
      "     |      >>> schema = StructType([\n",
      "     |      ...    StructField(\"name\", StringType(), True),\n",
      "     |      ...    StructField(\"age\", IntegerType(), True)])\n",
      "     |      >>> df3 = sqlContext.createDataFrame(rdd, schema)\n",
      "     |      >>> df3.collect()\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      \n",
      "     |      >>> sqlContext.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      >>> sqlContext.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "     |      [Row(0=1, 1=2)]\n",
      "     |      \n",
      "     |      >>> sqlContext.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      "     |      [Row(a='Alice', b=1)]\n",
      "     |      >>> rdd = rdd.map(lambda row: row[1])\n",
      "     |      >>> sqlContext.createDataFrame(rdd, \"int\").collect()\n",
      "     |      [Row(value=1)]\n",
      "     |      >>> sqlContext.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      Py4JJavaError: ...\n",
      "     |  \n",
      "     |  createExternalTable(self, tableName: str, path: Optional[str] = None, source: Optional[str] = None, schema: Optional[pyspark.sql.types.StructType] = None, **options: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Creates an external table based on the dataset in a data source.\n",
      "     |      \n",
      "     |      It returns the DataFrame associated with the external table.\n",
      "     |      \n",
      "     |      The data source is specified by the ``source`` and a set of ``options``.\n",
      "     |      If ``source`` is not specified, the default data source configured by\n",
      "     |      ``spark.sql.sources.default`` will be used.\n",
      "     |      \n",
      "     |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
      "     |      created external table.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |  \n",
      "     |  dropTempTable(self, tableName: str) -> None\n",
      "     |      Remove the temporary table from catalog.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> sqlContext.dropTempTable(\"table1\")\n",
      "     |  \n",
      "     |  getConf(self, key: str, defaultValue: Union[str, NoneType, pyspark._globals._NoValueType] = <no value>) -> Optional[str]\n",
      "     |      Returns the value of Spark SQL configuration property for the given key.\n",
      "     |      \n",
      "     |      If the key is not set and defaultValue is set, return\n",
      "     |      defaultValue. If the key is not set and defaultValue is not set, return\n",
      "     |      the system default value.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\")\n",
      "     |      '200'\n",
      "     |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", \"10\")\n",
      "     |      '10'\n",
      "     |      >>> sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"50\")\n",
      "     |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", \"10\")\n",
      "     |      '50'\n",
      "     |  \n",
      "     |  newSession(self) -> 'SQLContext'\n",
      "     |      Returns a new SQLContext as new session, that has separate SQLConf,\n",
      "     |      registered temporary views and UDFs, but shared SparkContext and\n",
      "     |      table cache.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |  \n",
      "     |  range(self, start: int, end: Optional[int] = None, step: int = 1, numPartitions: Optional[int] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n",
      "     |      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n",
      "     |      step value ``step``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      start : int\n",
      "     |          the start value\n",
      "     |      end : int, optional\n",
      "     |          the end value (exclusive)\n",
      "     |      step : int, optional\n",
      "     |          the incremental step (default: 1)\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions of the DataFrame\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.range(1, 7, 2).collect()\n",
      "     |      [Row(id=1), Row(id=3), Row(id=5)]\n",
      "     |      \n",
      "     |      If only one argument is specified, it will be used as the end value.\n",
      "     |      \n",
      "     |      >>> sqlContext.range(3).collect()\n",
      "     |      [Row(id=0), Row(id=1), Row(id=2)]\n",
      "     |  \n",
      "     |  registerDataFrameAsTable(self, df: pyspark.sql.dataframe.DataFrame, tableName: str) -> None\n",
      "     |      Registers the given :class:`DataFrame` as a temporary table in the catalog.\n",
      "     |      \n",
      "     |      Temporary tables exist only during the lifetime of this instance of :class:`SQLContext`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |  \n",
      "     |  registerFunction(self, name: str, f: Callable[..., Any], returnType: Optional[pyspark.sql.types.DataType] = None) -> 'UserDefinedFunctionLike'\n",
      "     |      An alias for :func:`spark.udf.register`.\n",
      "     |      See :meth:`pyspark.sql.UDFRegistration.register`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2.0\n",
      "     |      \n",
      "     |      .. deprecated:: 2.3.0\n",
      "     |          Use :func:`spark.udf.register` instead.\n",
      "     |  \n",
      "     |  registerJavaFunction(self, name: str, javaClassName: str, returnType: Optional[pyspark.sql.types.DataType] = None) -> None\n",
      "     |      An alias for :func:`spark.udf.registerJavaFunction`.\n",
      "     |      See :meth:`pyspark.sql.UDFRegistration.registerJavaFunction`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      .. deprecated:: 2.3.0\n",
      "     |          Use :func:`spark.udf.registerJavaFunction` instead.\n",
      "     |  \n",
      "     |  setConf(self, key: str, value: Union[bool, int, str]) -> None\n",
      "     |      Sets the given Spark SQL configuration property.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |  \n",
      "     |  sql(self, sqlQuery: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns a :class:`DataFrame` representing the result of the given query.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> df2 = sqlContext.sql(\"SELECT field1 AS f1, field2 as f2 from table1\")\n",
      "     |      >>> df2.collect()\n",
      "     |      [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\n",
      "     |  \n",
      "     |  table(self, tableName: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns the specified table or view as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> df2 = sqlContext.table(\"table1\")\n",
      "     |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |  \n",
      "     |  tableNames(self, dbName: Optional[str] = None) -> List[str]\n",
      "     |      Returns a list of names of tables in the database ``dbName``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dbName: str\n",
      "     |          name of the database to use. Default to the current database.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          list of table names, in string\n",
      "     |      \n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> \"table1\" in sqlContext.tableNames()\n",
      "     |      True\n",
      "     |      >>> \"table1\" in sqlContext.tableNames(\"default\")\n",
      "     |      True\n",
      "     |  \n",
      "     |  tables(self, dbName: Optional[str] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns a :class:`DataFrame` containing names of tables in the given database.\n",
      "     |      \n",
      "     |      If ``dbName`` is not specified, the current database will be used.\n",
      "     |      \n",
      "     |      The returned DataFrame has two columns: ``tableName`` and ``isTemporary``\n",
      "     |      (a column with :class:`BooleanType` indicating if a table is a temporary one or not).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dbName: str, optional\n",
      "     |          name of the database to use.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> df2 = sqlContext.tables()\n",
      "     |      >>> df2.filter(\"tableName = 'table1'\").first()\n",
      "     |      Row(namespace='', tableName='table1', isTemporary=True)\n",
      "     |  \n",
      "     |  uncacheTable(self, tableName: str) -> None\n",
      "     |      Removes the specified table from the in-memory cache.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from SQLContext:\n",
      "     |  \n",
      "     |  getOrCreate(sc: pyspark.context.SparkContext) -> 'SQLContext' from builtins.type\n",
      "     |      Get the existing SQLContext or create a new one with given SparkContext.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      .. deprecated:: 3.0.0\n",
      "     |          Use :func:`SparkSession.builder.getOrCreate()` instead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sc : :class:`SparkContext`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from SQLContext:\n",
      "     |  \n",
      "     |  read\n",
      "     |      Returns a :class:`DataFrameReader` that can be used to read data\n",
      "     |      in as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrameReader`\n",
      "     |  \n",
      "     |  readStream\n",
      "     |      Returns a :class:`DataStreamReader` that can be used to read data streams\n",
      "     |      as a streaming :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataStreamReader`\n",
      "     |      \n",
      "     |      >>> text_sdf = sqlContext.readStream.text(tempfile.mkdtemp())\n",
      "     |      >>> text_sdf.isStreaming\n",
      "     |      True\n",
      "     |  \n",
      "     |  streams\n",
      "     |      Returns a :class:`StreamingQueryManager` that allows managing all the\n",
      "     |      :class:`StreamingQuery` StreamingQueries active on `this` context.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |  \n",
      "     |  udf\n",
      "     |      Returns a :class:`UDFRegistration` for UDF registration.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`UDFRegistration`\n",
      "     |  \n",
      "     |  udtf\n",
      "     |      Returns a :class:`UDTFRegistration` for UDTF registration.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`UDTFRegistration`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SQLContext:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Observation(builtins.object)\n",
      "     |  Observation(name: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  Class to observe (named) metrics on a :class:`DataFrame`.\n",
      "     |  \n",
      "     |  Metrics are aggregation expressions, which are applied to the DataFrame while it is being\n",
      "     |  processed by an action.\n",
      "     |  \n",
      "     |  The metrics have the following guarantees:\n",
      "     |  \n",
      "     |  - It will compute the defined aggregates (metrics) on all the data that is flowing through\n",
      "     |    the Dataset during the action.\n",
      "     |  - It will report the value of the defined aggregate columns as soon as we reach the end of\n",
      "     |    the action.\n",
      "     |  \n",
      "     |  The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or\n",
      "     |  more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that\n",
      "     |  contain references to the input Dataset's columns must always be wrapped in an aggregate\n",
      "     |  function.\n",
      "     |  \n",
      "     |  An Observation instance collects the metrics while the first action is executed. Subsequent\n",
      "     |  actions do not modify the metrics returned by `Observation.get`. Retrieval of the metric via\n",
      "     |  `Observation.get` blocks until the first action has finished and metrics become available.\n",
      "     |  \n",
      "     |  .. versionadded:: 3.3.0\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  This class does not support streaming datasets.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from pyspark.sql.functions import col, count, lit, max\n",
      "     |  >>> from pyspark.sql import Observation\n",
      "     |  >>> df = spark.createDataFrame([[\"Alice\", 2], [\"Bob\", 5]], [\"name\", \"age\"])\n",
      "     |  >>> observation = Observation(\"my metrics\")\n",
      "     |  >>> observed_df = df.observe(observation, count(lit(1)).alias(\"count\"), max(col(\"age\")))\n",
      "     |  >>> observed_df.count()\n",
      "     |  2\n",
      "     |  >>> observation.get\n",
      "     |  {'count': 2, 'max(age)': 5}\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, name: Optional[str] = None) -> None\n",
      "     |      Constructs a named or unnamed Observation instance.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str, optional\n",
      "     |          default is a random UUID string. This is the name of the Observation and the metric.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  get\n",
      "     |      Get the observed metrics.\n",
      "     |      \n",
      "     |      Waits until the observed dataset finishes its first action. Only the result of the\n",
      "     |      first action is available. Subsequent actions do not modify the result.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dict\n",
      "     |          the observed metrics\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PandasCogroupedOps(builtins.object)\n",
      "     |  PandasCogroupedOps(gd1: 'GroupedData', gd2: 'GroupedData')\n",
      "     |  \n",
      "     |  A logical grouping of two :class:`GroupedData`,\n",
      "     |  created by :func:`GroupedData.cogroup`.\n",
      "     |  \n",
      "     |  .. versionadded:: 3.0.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Support Spark Connect.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  This API is experimental.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, gd1: 'GroupedData', gd2: 'GroupedData')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  applyInPandas(self, func: 'PandasCogroupedMapFunction', schema: Union[pyspark.sql.types.StructType, str]) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Applies a function to each cogroup using pandas and returns the result\n",
      "     |      as a `DataFrame`.\n",
      "     |      \n",
      "     |      The function should take two `pandas.DataFrame`\\s and return another\n",
      "     |      `pandas.DataFrame`. Alternatively, the user can pass a function that takes\n",
      "     |      a tuple of the grouping key(s) and the two `pandas.DataFrame`\\s.\n",
      "     |      For each side of the cogroup, all columns are passed together as a\n",
      "     |      `pandas.DataFrame` to the user-function and the returned `pandas.DataFrame` are combined as\n",
      "     |      a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The `schema` should be a :class:`StructType` describing the schema of the returned\n",
      "     |      `pandas.DataFrame`. The column labels of the returned `pandas.DataFrame` must either match\n",
      "     |      the field names in the defined schema if specified as strings, or match the\n",
      "     |      field data types by position if not strings, e.g. integer indices.\n",
      "     |      The length of the returned `pandas.DataFrame` can be arbitrary.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Support Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : function\n",
      "     |          a Python native function that takes two `pandas.DataFrame`\\s, and\n",
      "     |          outputs a `pandas.DataFrame`, or that takes one tuple (grouping keys) and two\n",
      "     |          ``pandas.DataFrame``\\s, and outputs a ``pandas.DataFrame``.\n",
      "     |      schema : :class:`pyspark.sql.types.DataType` or str\n",
      "     |          the return type of the `func` in PySpark. The value can be either a\n",
      "     |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import pandas_udf\n",
      "     |      >>> df1 = spark.createDataFrame(\n",
      "     |      ...     [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
      "     |      ...     (\"time\", \"id\", \"v1\"))\n",
      "     |      >>> df2 = spark.createDataFrame(\n",
      "     |      ...     [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n",
      "     |      ...     (\"time\", \"id\", \"v2\"))\n",
      "     |      >>> def asof_join(l, r):\n",
      "     |      ...     return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n",
      "     |      ...\n",
      "     |      >>> df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(\n",
      "     |      ...     asof_join, schema=\"time int, id int, v1 double, v2 string\"\n",
      "     |      ... ).show()  # doctest: +SKIP\n",
      "     |      +--------+---+---+---+\n",
      "     |      |    time| id| v1| v2|\n",
      "     |      +--------+---+---+---+\n",
      "     |      |20000101|  1|1.0|  x|\n",
      "     |      |20000102|  1|3.0|  x|\n",
      "     |      |20000101|  2|2.0|  y|\n",
      "     |      |20000102|  2|4.0|  y|\n",
      "     |      +--------+---+---+---+\n",
      "     |      \n",
      "     |      Alternatively, the user can define a function that takes three arguments.  In this case,\n",
      "     |      the grouping key(s) will be passed as the first argument and the data will be passed as the\n",
      "     |      second and third arguments.  The grouping key(s) will be passed as a tuple of numpy data\n",
      "     |      types, e.g., `numpy.int32` and `numpy.float64`. The data will still be passed in as two\n",
      "     |      `pandas.DataFrame` containing all columns from the original Spark DataFrames.\n",
      "     |      \n",
      "     |      >>> def asof_join(k, l, r):\n",
      "     |      ...     if k == (1,):\n",
      "     |      ...         return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n",
      "     |      ...     else:\n",
      "     |      ...         return pd.DataFrame(columns=['time', 'id', 'v1', 'v2'])\n",
      "     |      ...\n",
      "     |      >>> df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(\n",
      "     |      ...     asof_join, \"time int, id int, v1 double, v2 string\").show()  # doctest: +SKIP\n",
      "     |      +--------+---+---+---+\n",
      "     |      |    time| id| v1| v2|\n",
      "     |      +--------+---+---+---+\n",
      "     |      |20000101|  1|1.0|  x|\n",
      "     |      |20000102|  1|3.0|  x|\n",
      "     |      +--------+---+---+---+\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This function requires a full shuffle. All the data of a cogroup will be loaded\n",
      "     |      into memory, so the user should be aware of the potential OOM risk if data is skewed\n",
      "     |      and certain groups are too large to fit in memory.\n",
      "     |      \n",
      "     |      This API is experimental.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.functions.pandas_udf\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Row(builtins.tuple)\n",
      "     |  Row(*args: Optional[str], **kwargs: Optional[Any]) -> 'Row'\n",
      "     |  \n",
      "     |  A row in :class:`DataFrame`.\n",
      "     |  The fields in it can be accessed:\n",
      "     |  \n",
      "     |  * like attributes (``row.key``)\n",
      "     |  * like dictionary values (``row[key]``)\n",
      "     |  \n",
      "     |  ``key in row`` will search through row keys.\n",
      "     |  \n",
      "     |  Row can be used to create a row object by using named arguments.\n",
      "     |  It is not allowed to omit a named argument to represent that the value is\n",
      "     |  None or missing. This should be explicitly set to None in this case.\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.0.0\n",
      "     |      Rows created from named arguments no longer have\n",
      "     |      field names sorted alphabetically and will be ordered in the position as\n",
      "     |      entered.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from pyspark.sql import Row\n",
      "     |  >>> row = Row(name=\"Alice\", age=11)\n",
      "     |  >>> row\n",
      "     |  Row(name='Alice', age=11)\n",
      "     |  >>> row['name'], row['age']\n",
      "     |  ('Alice', 11)\n",
      "     |  >>> row.name, row.age\n",
      "     |  ('Alice', 11)\n",
      "     |  >>> 'name' in row\n",
      "     |  True\n",
      "     |  >>> 'wrong_key' in row\n",
      "     |  False\n",
      "     |  \n",
      "     |  Row also can be used to create another Row like class, then it\n",
      "     |  could be used to create Row objects, such as\n",
      "     |  \n",
      "     |  >>> Person = Row(\"name\", \"age\")\n",
      "     |  >>> Person\n",
      "     |  <Row('name', 'age')>\n",
      "     |  >>> 'name' in Person\n",
      "     |  True\n",
      "     |  >>> 'wrong_key' in Person\n",
      "     |  False\n",
      "     |  >>> Person(\"Alice\", 11)\n",
      "     |  Row(name='Alice', age=11)\n",
      "     |  \n",
      "     |  This form can also be used to create rows as tuple values, i.e. with unnamed\n",
      "     |  fields.\n",
      "     |  \n",
      "     |  >>> row1 = Row(\"Alice\", 11)\n",
      "     |  >>> row2 = Row(name=\"Alice\", age=11)\n",
      "     |  >>> row1 == row2\n",
      "     |  True\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Row\n",
      "     |      builtins.tuple\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, *args: Any) -> 'Row'\n",
      "     |      create new Row object\n",
      "     |  \n",
      "     |  __contains__(self, item: Any) -> bool\n",
      "     |      Return key in self.\n",
      "     |  \n",
      "     |  __getattr__(self, item: str) -> Any\n",
      "     |  \n",
      "     |  __getitem__(self, item: Any) -> Any\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __reduce__(self) -> Union[str, Tuple[Any, ...]]\n",
      "     |      Returns a tuple so Python knows how to pickle Row.\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Printable representation of Row used in Python REPL.\n",
      "     |  \n",
      "     |  __setattr__(self, key: Any, value: Any) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  asDict(self, recursive: bool = False) -> Dict[str, Any]\n",
      "     |      Return as a dict\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      recursive : bool, optional\n",
      "     |          turns the nested Rows to dict (default: False).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If a row contains duplicate field names, e.g., the rows of a join\n",
      "     |      between two :class:`DataFrame` that both have the fields of same names,\n",
      "     |      one of the duplicate fields will be selected by ``asDict``. ``__getitem__``\n",
      "     |      will also return one of the duplicate fields, however returned value might\n",
      "     |      be different to ``asDict``.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n",
      "     |      True\n",
      "     |      >>> row = Row(key=1, value=Row(name='a', age=2))\n",
      "     |      >>> row.asDict() == {'key': 1, 'value': Row(name='a', age=2)}\n",
      "     |      True\n",
      "     |      >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n",
      "     |      True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(cls, *args: Optional[str], **kwargs: Optional[Any]) -> 'Row'\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.tuple:\n",
      "     |  \n",
      "     |  __add__(self, value, /)\n",
      "     |      Return self+value.\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getnewargs__(self, /)\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __hash__(self, /)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __mul__(self, value, /)\n",
      "     |      Return self*value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __rmul__(self, value, /)\n",
      "     |      Return value*self.\n",
      "     |  \n",
      "     |  count(self, value, /)\n",
      "     |      Return number of occurrences of value.\n",
      "     |  \n",
      "     |  index(self, value, start=0, stop=9223372036854775807, /)\n",
      "     |      Return first index of value.\n",
      "     |      \n",
      "     |      Raises ValueError if the value is not present.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.tuple:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "    \n",
      "    class SQLContext(builtins.object)\n",
      "     |  SQLContext(sparkContext: pyspark.context.SparkContext, sparkSession: Optional[pyspark.sql.session.SparkSession] = None, jsqlContext: Optional[py4j.java_gateway.JavaObject] = None)\n",
      "     |  \n",
      "     |  The entry point for working with structured data (rows and columns) in Spark, in Spark 1.x.\n",
      "     |  \n",
      "     |  As of Spark 2.0, this is replaced by :class:`SparkSession`. However, we are keeping the class\n",
      "     |  here for backward compatibility.\n",
      "     |  \n",
      "     |  A SQLContext can be used to create :class:`DataFrame`, register :class:`DataFrame` as\n",
      "     |  tables, execute SQL over tables, cache tables, and read parquet files.\n",
      "     |  \n",
      "     |  .. deprecated:: 3.0.0\n",
      "     |      Use :func:`SparkSession.builder.getOrCreate()` instead.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  sparkContext : :class:`SparkContext`\n",
      "     |      The :class:`SparkContext` backing this SQLContext.\n",
      "     |  sparkSession : :class:`SparkSession`\n",
      "     |      The :class:`SparkSession` around which this SQLContext wraps.\n",
      "     |  jsqlContext : optional\n",
      "     |      An optional JVM Scala SQLContext. If set, we do not instantiate a new\n",
      "     |      SQLContext in the JVM, instead we make all calls to this object.\n",
      "     |      This is only for internal.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from datetime import datetime\n",
      "     |  >>> from pyspark.sql import Row\n",
      "     |  >>> sqlContext = SQLContext(sc)\n",
      "     |  >>> allTypes = sc.parallelize([Row(i=1, s=\"string\", d=1.0, l=1,\n",
      "     |  ...     b=True, list=[1, 2, 3], dict={\"s\": 0}, row=Row(a=1),\n",
      "     |  ...     time=datetime(2014, 8, 1, 14, 1, 5))])\n",
      "     |  >>> df = allTypes.toDF()\n",
      "     |  >>> df.createOrReplaceTempView(\"allTypes\")\n",
      "     |  >>> sqlContext.sql('select i+1, d+1, not b, list[1], dict[\"s\"], time, row.a '\n",
      "     |  ...            'from allTypes where b and i > 0').collect()\n",
      "     |  [Row((i + 1)=2, (d + 1)=2.0, (NOT b)=False, list[1]=2,         dict[s]=0, time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]\n",
      "     |  >>> df.rdd.map(lambda x: (x.i, x.s, x.d, x.l, x.b, x.time, x.row.a, x.list)).collect()\n",
      "     |  [(1, 'string', 1.0, 1, True, datetime.datetime(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sparkContext: pyspark.context.SparkContext, sparkSession: Optional[pyspark.sql.session.SparkSession] = None, jsqlContext: Optional[py4j.java_gateway.JavaObject] = None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cacheTable(self, tableName: str) -> None\n",
      "     |      Caches the specified table in-memory.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  clearCache(self) -> None\n",
      "     |      Removes all cached tables from the in-memory cache.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  createDataFrame(self, data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      "     |      \n",
      "     |      When ``schema`` is a list of column names, the type of each column\n",
      "     |      will be inferred from ``data``.\n",
      "     |      \n",
      "     |      When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "     |      from ``data``, which should be an RDD of :class:`Row`,\n",
      "     |      or :class:`namedtuple`, or :class:`dict`.\n",
      "     |      \n",
      "     |      When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string it must match\n",
      "     |      the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      "     |      :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "     |      :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\",\n",
      "     |      each record will also be wrapped into a tuple, which can be converted to row later.\n",
      "     |      \n",
      "     |      If schema inference is needed, ``samplingRatio`` is used to determine the ratio of\n",
      "     |      rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 2.0.0\n",
      "     |         The ``schema`` parameter can be a :class:`pyspark.sql.types.DataType` or a\n",
      "     |         datatype string after 2.0.\n",
      "     |         If it's not a :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "     |         :class:`pyspark.sql.types.StructType` and each record will also be wrapped into a tuple.\n",
      "     |      \n",
      "     |      .. versionchanged:: 2.1.0\n",
      "     |         Added verifySchema.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : :class:`RDD` or iterable\n",
      "     |          an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "     |          :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
      "     |          :class:`pandas.DataFrame`.\n",
      "     |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "     |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "     |          column names, default is None.  The data type string format equals to\n",
      "     |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "     |          omit the ``struct<>``.\n",
      "     |      samplingRatio : float, optional\n",
      "     |          the sample ratio of rows used for inferring\n",
      "     |      verifySchema : bool, optional\n",
      "     |          verify data types of every row against schema. Enabled by default.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> l = [('Alice', 1)]\n",
      "     |      >>> sqlContext.createDataFrame(l).collect()\n",
      "     |      [Row(_1='Alice', _2=1)]\n",
      "     |      >>> sqlContext.createDataFrame(l, ['name', 'age']).collect()\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      \n",
      "     |      >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "     |      >>> sqlContext.createDataFrame(d).collect()\n",
      "     |      [Row(age=1, name='Alice')]\n",
      "     |      \n",
      "     |      >>> rdd = sc.parallelize(l)\n",
      "     |      >>> sqlContext.createDataFrame(rdd).collect()\n",
      "     |      [Row(_1='Alice', _2=1)]\n",
      "     |      >>> df = sqlContext.createDataFrame(rdd, ['name', 'age'])\n",
      "     |      >>> df.collect()\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      \n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> Person = Row('name', 'age')\n",
      "     |      >>> person = rdd.map(lambda r: Person(*r))\n",
      "     |      >>> df2 = sqlContext.createDataFrame(person)\n",
      "     |      >>> df2.collect()\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.types import *\n",
      "     |      >>> schema = StructType([\n",
      "     |      ...    StructField(\"name\", StringType(), True),\n",
      "     |      ...    StructField(\"age\", IntegerType(), True)])\n",
      "     |      >>> df3 = sqlContext.createDataFrame(rdd, schema)\n",
      "     |      >>> df3.collect()\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      \n",
      "     |      >>> sqlContext.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      >>> sqlContext.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "     |      [Row(0=1, 1=2)]\n",
      "     |      \n",
      "     |      >>> sqlContext.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      "     |      [Row(a='Alice', b=1)]\n",
      "     |      >>> rdd = rdd.map(lambda row: row[1])\n",
      "     |      >>> sqlContext.createDataFrame(rdd, \"int\").collect()\n",
      "     |      [Row(value=1)]\n",
      "     |      >>> sqlContext.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      Py4JJavaError: ...\n",
      "     |  \n",
      "     |  createExternalTable(self, tableName: str, path: Optional[str] = None, source: Optional[str] = None, schema: Optional[pyspark.sql.types.StructType] = None, **options: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Creates an external table based on the dataset in a data source.\n",
      "     |      \n",
      "     |      It returns the DataFrame associated with the external table.\n",
      "     |      \n",
      "     |      The data source is specified by the ``source`` and a set of ``options``.\n",
      "     |      If ``source`` is not specified, the default data source configured by\n",
      "     |      ``spark.sql.sources.default`` will be used.\n",
      "     |      \n",
      "     |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
      "     |      created external table.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |  \n",
      "     |  dropTempTable(self, tableName: str) -> None\n",
      "     |      Remove the temporary table from catalog.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> sqlContext.dropTempTable(\"table1\")\n",
      "     |  \n",
      "     |  getConf(self, key: str, defaultValue: Union[str, NoneType, pyspark._globals._NoValueType] = <no value>) -> Optional[str]\n",
      "     |      Returns the value of Spark SQL configuration property for the given key.\n",
      "     |      \n",
      "     |      If the key is not set and defaultValue is set, return\n",
      "     |      defaultValue. If the key is not set and defaultValue is not set, return\n",
      "     |      the system default value.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\")\n",
      "     |      '200'\n",
      "     |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", \"10\")\n",
      "     |      '10'\n",
      "     |      >>> sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"50\")\n",
      "     |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", \"10\")\n",
      "     |      '50'\n",
      "     |  \n",
      "     |  newSession(self) -> 'SQLContext'\n",
      "     |      Returns a new SQLContext as new session, that has separate SQLConf,\n",
      "     |      registered temporary views and UDFs, but shared SparkContext and\n",
      "     |      table cache.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |  \n",
      "     |  range(self, start: int, end: Optional[int] = None, step: int = 1, numPartitions: Optional[int] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n",
      "     |      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n",
      "     |      step value ``step``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      start : int\n",
      "     |          the start value\n",
      "     |      end : int, optional\n",
      "     |          the end value (exclusive)\n",
      "     |      step : int, optional\n",
      "     |          the incremental step (default: 1)\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions of the DataFrame\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.range(1, 7, 2).collect()\n",
      "     |      [Row(id=1), Row(id=3), Row(id=5)]\n",
      "     |      \n",
      "     |      If only one argument is specified, it will be used as the end value.\n",
      "     |      \n",
      "     |      >>> sqlContext.range(3).collect()\n",
      "     |      [Row(id=0), Row(id=1), Row(id=2)]\n",
      "     |  \n",
      "     |  registerDataFrameAsTable(self, df: pyspark.sql.dataframe.DataFrame, tableName: str) -> None\n",
      "     |      Registers the given :class:`DataFrame` as a temporary table in the catalog.\n",
      "     |      \n",
      "     |      Temporary tables exist only during the lifetime of this instance of :class:`SQLContext`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |  \n",
      "     |  registerFunction(self, name: str, f: Callable[..., Any], returnType: Optional[pyspark.sql.types.DataType] = None) -> 'UserDefinedFunctionLike'\n",
      "     |      An alias for :func:`spark.udf.register`.\n",
      "     |      See :meth:`pyspark.sql.UDFRegistration.register`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2.0\n",
      "     |      \n",
      "     |      .. deprecated:: 2.3.0\n",
      "     |          Use :func:`spark.udf.register` instead.\n",
      "     |  \n",
      "     |  registerJavaFunction(self, name: str, javaClassName: str, returnType: Optional[pyspark.sql.types.DataType] = None) -> None\n",
      "     |      An alias for :func:`spark.udf.registerJavaFunction`.\n",
      "     |      See :meth:`pyspark.sql.UDFRegistration.registerJavaFunction`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      .. deprecated:: 2.3.0\n",
      "     |          Use :func:`spark.udf.registerJavaFunction` instead.\n",
      "     |  \n",
      "     |  setConf(self, key: str, value: Union[bool, int, str]) -> None\n",
      "     |      Sets the given Spark SQL configuration property.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |  \n",
      "     |  sql(self, sqlQuery: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns a :class:`DataFrame` representing the result of the given query.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> df2 = sqlContext.sql(\"SELECT field1 AS f1, field2 as f2 from table1\")\n",
      "     |      >>> df2.collect()\n",
      "     |      [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\n",
      "     |  \n",
      "     |  table(self, tableName: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns the specified table or view as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> df2 = sqlContext.table(\"table1\")\n",
      "     |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |  \n",
      "     |  tableNames(self, dbName: Optional[str] = None) -> List[str]\n",
      "     |      Returns a list of names of tables in the database ``dbName``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dbName: str\n",
      "     |          name of the database to use. Default to the current database.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          list of table names, in string\n",
      "     |      \n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> \"table1\" in sqlContext.tableNames()\n",
      "     |      True\n",
      "     |      >>> \"table1\" in sqlContext.tableNames(\"default\")\n",
      "     |      True\n",
      "     |  \n",
      "     |  tables(self, dbName: Optional[str] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns a :class:`DataFrame` containing names of tables in the given database.\n",
      "     |      \n",
      "     |      If ``dbName`` is not specified, the current database will be used.\n",
      "     |      \n",
      "     |      The returned DataFrame has two columns: ``tableName`` and ``isTemporary``\n",
      "     |      (a column with :class:`BooleanType` indicating if a table is a temporary one or not).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dbName: str, optional\n",
      "     |          name of the database to use.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> df2 = sqlContext.tables()\n",
      "     |      >>> df2.filter(\"tableName = 'table1'\").first()\n",
      "     |      Row(namespace='', tableName='table1', isTemporary=True)\n",
      "     |  \n",
      "     |  uncacheTable(self, tableName: str) -> None\n",
      "     |      Removes the specified table from the in-memory cache.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  getOrCreate(sc: pyspark.context.SparkContext) -> 'SQLContext' from builtins.type\n",
      "     |      Get the existing SQLContext or create a new one with given SparkContext.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      .. deprecated:: 3.0.0\n",
      "     |          Use :func:`SparkSession.builder.getOrCreate()` instead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sc : :class:`SparkContext`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  read\n",
      "     |      Returns a :class:`DataFrameReader` that can be used to read data\n",
      "     |      in as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrameReader`\n",
      "     |  \n",
      "     |  readStream\n",
      "     |      Returns a :class:`DataStreamReader` that can be used to read data streams\n",
      "     |      as a streaming :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataStreamReader`\n",
      "     |      \n",
      "     |      >>> text_sdf = sqlContext.readStream.text(tempfile.mkdtemp())\n",
      "     |      >>> text_sdf.isStreaming\n",
      "     |      True\n",
      "     |  \n",
      "     |  streams\n",
      "     |      Returns a :class:`StreamingQueryManager` that allows managing all the\n",
      "     |      :class:`StreamingQuery` StreamingQueries active on `this` context.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |  \n",
      "     |  udf\n",
      "     |      Returns a :class:`UDFRegistration` for UDF registration.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`UDFRegistration`\n",
      "     |  \n",
      "     |  udtf\n",
      "     |      Returns a :class:`UDTFRegistration` for UDTF registration.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`UDTFRegistration`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'_instantiatedContext': typing.ClassVar[typing.Opti...\n",
      "    \n",
      "    class SparkSession(pyspark.sql.pandas.conversion.SparkConversionMixin)\n",
      "     |  SparkSession(sparkContext: pyspark.context.SparkContext, jsparkSession: Optional[py4j.java_gateway.JavaObject] = None, options: Dict[str, Any] = {})\n",
      "     |  \n",
      "     |  The entry point to programming Spark with the Dataset and DataFrame API.\n",
      "     |  \n",
      "     |  A SparkSession can be used to create :class:`DataFrame`, register :class:`DataFrame` as\n",
      "     |  tables, execute SQL over tables, cache tables, and read parquet files.\n",
      "     |  To create a :class:`SparkSession`, use the following builder pattern:\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  .. autoattribute:: builder\n",
      "     |     :annotation:\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Create a Spark session.\n",
      "     |  \n",
      "     |  >>> spark = (\n",
      "     |  ...     SparkSession.builder\n",
      "     |  ...         .master(\"local\")\n",
      "     |  ...         .appName(\"Word Count\")\n",
      "     |  ...         .config(\"spark.some.config.option\", \"some-value\")\n",
      "     |  ...         .getOrCreate()\n",
      "     |  ... )\n",
      "     |  \n",
      "     |  Create a Spark session with Spark Connect.\n",
      "     |  \n",
      "     |  >>> spark = (\n",
      "     |  ...     SparkSession.builder\n",
      "     |  ...         .remote(\"sc://localhost\")\n",
      "     |  ...         .appName(\"Word Count\")\n",
      "     |  ...         .config(\"spark.some.config.option\", \"some-value\")\n",
      "     |  ...         .getOrCreate()\n",
      "     |  ... )  # doctest: +SKIP\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SparkSession\n",
      "     |      pyspark.sql.pandas.conversion.SparkConversionMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __enter__(self) -> 'SparkSession'\n",
      "     |      Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> with SparkSession.builder.master(\"local\").getOrCreate() as session:\n",
      "     |      ...     session.range(5).show()  # doctest: +SKIP\n",
      "     |      +---+\n",
      "     |      | id|\n",
      "     |      +---+\n",
      "     |      |  0|\n",
      "     |      |  1|\n",
      "     |      |  2|\n",
      "     |      |  3|\n",
      "     |      |  4|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  __exit__(self, exc_type: Optional[Type[BaseException]], exc_val: Optional[BaseException], exc_tb: Optional[traceback]) -> None\n",
      "     |      Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n",
      "     |      \n",
      "     |      Specifically stop the SparkSession on exit of the with block.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> with SparkSession.builder.master(\"local\").getOrCreate() as session:\n",
      "     |      ...     session.range(5).show()  # doctest: +SKIP\n",
      "     |      +---+\n",
      "     |      | id|\n",
      "     |      +---+\n",
      "     |      |  0|\n",
      "     |      |  1|\n",
      "     |      |  2|\n",
      "     |      |  3|\n",
      "     |      |  4|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  __init__(self, sparkContext: pyspark.context.SparkContext, jsparkSession: Optional[py4j.java_gateway.JavaObject] = None, options: Dict[str, Any] = {})\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  addArtifact = addArtifacts(self, *path: str, pyfile: bool = False, archive: bool = False, file: bool = False) -> None\n",
      "     |  \n",
      "     |  addArtifacts(self, *path: str, pyfile: bool = False, archive: bool = False, file: bool = False) -> None\n",
      "     |      Add artifact(s) to the client session. Currently only local files are supported.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      *path : tuple of str\n",
      "     |          Artifact's URIs to add.\n",
      "     |      pyfile : bool\n",
      "     |          Whether to add them as Python dependencies such as .py, .egg, .zip or .jar files.\n",
      "     |          The pyfiles are directly inserted into the path when executing Python functions\n",
      "     |          in executors.\n",
      "     |      archive : bool\n",
      "     |          Whether to add them as archives such as .zip, .jar, .tar.gz, .tgz, or .tar files.\n",
      "     |          The archives are unpacked on the executor side automatically.\n",
      "     |      file : bool\n",
      "     |          Add a file to be downloaded with this Spark job on every node.\n",
      "     |          The ``path`` passed can only be a local file for now.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This is an API dedicated to Spark Connect client only. With regular Spark Session, it throws\n",
      "     |      an exception.\n",
      "     |  \n",
      "     |  addTag(self, tag: str) -> None\n",
      "     |      Add a tag to be assigned to all the operations started by this thread in this session.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tag : list of str\n",
      "     |          The tag to be added. Cannot contain ',' (comma) character or be an empty string.\n",
      "     |  \n",
      "     |  clearTags(self) -> None\n",
      "     |      Clear the current thread's operation tags.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |  \n",
      "     |  copyFromLocalToFs(self, local_path: str, dest_path: str) -> None\n",
      "     |      Copy file from local to cloud storage file system.\n",
      "     |      If the file already exits in destination path, old file is overwritten.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      local_path: str\n",
      "     |          Path to a local file. Directories are not supported.\n",
      "     |          The path can be either an absolute path or a relative path.\n",
      "     |      dest_path: str\n",
      "     |          The cloud storage path to the destination the file will\n",
      "     |          be copied to.\n",
      "     |          The path must be an an absolute path.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is a developer API.\n",
      "     |      Also, this is an API dedicated to Spark Connect client only. With regular\n",
      "     |      Spark Session, it throws an exception.\n",
      "     |  \n",
      "     |  createDataFrame(self, data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n",
      "     |      or a :class:`numpy.ndarray`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : :class:`RDD` or iterable\n",
      "     |          an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "     |          :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n",
      "     |          :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n",
      "     |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "     |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "     |          column names, default is None. The data type string format equals to\n",
      "     |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "     |          omit the ``struct<>``.\n",
      "     |      \n",
      "     |          When ``schema`` is a list of column names, the type of each column\n",
      "     |          will be inferred from ``data``.\n",
      "     |      \n",
      "     |          When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "     |          from ``data``, which should be an RDD of either :class:`Row`,\n",
      "     |          :class:`namedtuple`, or :class:`dict`.\n",
      "     |      \n",
      "     |          When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n",
      "     |          match the real data, or an exception will be thrown at runtime. If the given schema is\n",
      "     |          not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "     |          :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n",
      "     |          \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n",
      "     |          later.\n",
      "     |      samplingRatio : float, optional\n",
      "     |          the sample ratio of rows used for inferring. The first few rows will be used\n",
      "     |          if ``samplingRatio`` is ``None``.\n",
      "     |      verifySchema : bool, optional\n",
      "     |          verify data types of every row against schema. Enabled by default.\n",
      "     |      \n",
      "     |          .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Create a DataFrame from a list of tuples.\n",
      "     |      \n",
      "     |      >>> spark.createDataFrame([('Alice', 1)]).show()\n",
      "     |      +-----+---+\n",
      "     |      |   _1| _2|\n",
      "     |      +-----+---+\n",
      "     |      |Alice|  1|\n",
      "     |      +-----+---+\n",
      "     |      \n",
      "     |      Create a DataFrame from a list of dictionaries.\n",
      "     |      \n",
      "     |      >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "     |      >>> spark.createDataFrame(d).show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  1|Alice|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Create a DataFrame with column names specified.\n",
      "     |      \n",
      "     |      >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).show()\n",
      "     |      +-----+---+\n",
      "     |      | name|age|\n",
      "     |      +-----+---+\n",
      "     |      |Alice|  1|\n",
      "     |      +-----+---+\n",
      "     |      \n",
      "     |      Create a DataFrame with the explicit schema specified.\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.types import *\n",
      "     |      >>> schema = StructType([\n",
      "     |      ...    StructField(\"name\", StringType(), True),\n",
      "     |      ...    StructField(\"age\", IntegerType(), True)])\n",
      "     |      >>> spark.createDataFrame([('Alice', 1)], schema).show()\n",
      "     |      +-----+---+\n",
      "     |      | name|age|\n",
      "     |      +-----+---+\n",
      "     |      |Alice|  1|\n",
      "     |      +-----+---+\n",
      "     |      \n",
      "     |      Create a DataFrame with the schema in DDL formatted string.\n",
      "     |      \n",
      "     |      >>> spark.createDataFrame([('Alice', 1)], \"name: string, age: int\").show()\n",
      "     |      +-----+---+\n",
      "     |      | name|age|\n",
      "     |      +-----+---+\n",
      "     |      |Alice|  1|\n",
      "     |      +-----+---+\n",
      "     |      \n",
      "     |      Create an empty DataFrame.\n",
      "     |      When initializing an empty DataFrame in PySpark, it's mandatory to specify its schema,\n",
      "     |      as the DataFrame lacks data from which the schema can be inferred.\n",
      "     |      \n",
      "     |      >>> spark.createDataFrame([], \"name: string, age: int\").show()\n",
      "     |      +----+---+\n",
      "     |      |name|age|\n",
      "     |      +----+---+\n",
      "     |      +----+---+\n",
      "     |      \n",
      "     |      Create a DataFrame from Row objects.\n",
      "     |      \n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> Person = Row('name', 'age')\n",
      "     |      >>> df = spark.createDataFrame([Person(\"Alice\", 1)])\n",
      "     |      >>> df.show()\n",
      "     |      +-----+---+\n",
      "     |      | name|age|\n",
      "     |      +-----+---+\n",
      "     |      |Alice|  1|\n",
      "     |      +-----+---+\n",
      "     |      \n",
      "     |      Create a DataFrame from a pandas DataFrame.\n",
      "     |      \n",
      "     |      >>> spark.createDataFrame(df.toPandas()).show()  # doctest: +SKIP\n",
      "     |      +-----+---+\n",
      "     |      | name|age|\n",
      "     |      +-----+---+\n",
      "     |      |Alice|  1|\n",
      "     |      +-----+---+\n",
      "     |      >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "     |      +---+---+\n",
      "     |      |  0|  1|\n",
      "     |      +---+---+\n",
      "     |      |  1|  2|\n",
      "     |      +---+---+\n",
      "     |  \n",
      "     |  getTags(self) -> Set[str]\n",
      "     |      Get the tags that are currently set to be assigned to all the operations started by this\n",
      "     |      thread.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      set of str\n",
      "     |          Set of tags of interrupted operations.\n",
      "     |  \n",
      "     |  interruptAll(self) -> List[str]\n",
      "     |      Interrupt all operations of this session currently running on the connected server.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list of str\n",
      "     |          List of operationIds of interrupted operations.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      There is still a possibility of operation finishing just as it is interrupted.\n",
      "     |  \n",
      "     |  interruptOperation(self, op_id: str) -> List[str]\n",
      "     |      Interrupt an operation of this session with the given operationId.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list of str\n",
      "     |          List of operationIds of interrupted operations.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      There is still a possibility of operation finishing just as it is interrupted.\n",
      "     |  \n",
      "     |  interruptTag(self, tag: str) -> List[str]\n",
      "     |      Interrupt all operations of this session with the given operation tag.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list of str\n",
      "     |          List of operationIds of interrupted operations.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      There is still a possibility of operation finishing just as it is interrupted.\n",
      "     |  \n",
      "     |  newSession(self) -> 'SparkSession'\n",
      "     |      Returns a new :class:`SparkSession` as new session, that has separate SQLConf,\n",
      "     |      registered temporary views and UDFs, but shared :class:`SparkContext` and\n",
      "     |      table cache.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`SparkSession`\n",
      "     |          Spark session if an active session exists for the current thread\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.newSession()\n",
      "     |      <...SparkSession object ...>\n",
      "     |  \n",
      "     |  range(self, start: int, end: Optional[int] = None, step: int = 1, numPartitions: Optional[int] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n",
      "     |      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n",
      "     |      step value ``step``.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      start : int\n",
      "     |          the start value\n",
      "     |      end : int, optional\n",
      "     |          the end value (exclusive)\n",
      "     |      step : int, optional\n",
      "     |          the incremental step (default: 1)\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions of the DataFrame\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.range(1, 7, 2).show()\n",
      "     |      +---+\n",
      "     |      | id|\n",
      "     |      +---+\n",
      "     |      |  1|\n",
      "     |      |  3|\n",
      "     |      |  5|\n",
      "     |      +---+\n",
      "     |      \n",
      "     |      If only one argument is specified, it will be used as the end value.\n",
      "     |      \n",
      "     |      >>> spark.range(3).show()\n",
      "     |      +---+\n",
      "     |      | id|\n",
      "     |      +---+\n",
      "     |      |  0|\n",
      "     |      |  1|\n",
      "     |      |  2|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  removeTag(self, tag: str) -> None\n",
      "     |      Remove a tag previously added to be assigned to all the operations started by this thread in\n",
      "     |      this session. Noop if such a tag was not added earlier.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tag : list of str\n",
      "     |          The tag to be removed. Cannot contain ',' (comma) character or be an empty string.\n",
      "     |  \n",
      "     |  sql(self, sqlQuery: str, args: Union[Dict[str, Any], List, NoneType] = None, **kwargs: Any) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns a :class:`DataFrame` representing the result of the given query.\n",
      "     |      When ``kwargs`` is specified, this method formats the given string by using the Python\n",
      "     |      standard formatter. The method binds named parameters to SQL literals or\n",
      "     |      positional parameters from `args`. It doesn't support named and positional parameters\n",
      "     |      in the same SQL query.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect and parameterized SQL.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Added positional parameters.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sqlQuery : str\n",
      "     |          SQL query string.\n",
      "     |      args : dict or list\n",
      "     |          A dictionary of parameter names to Python objects or a list of Python objects\n",
      "     |          that can be converted to SQL literal expressions. See\n",
      "     |          <a href=\"https://spark.apache.org/docs/latest/sql-ref-datatypes.html\">\n",
      "     |          Supported Data Types</a> for supported value types in Python.\n",
      "     |          For example, dictionary keys: \"rank\", \"name\", \"birthdate\";\n",
      "     |          dictionary or list values: 1, \"Steven\", datetime.date(2023, 4, 2).\n",
      "     |          A value can be also a `Column` of literal expression, in that case it is taken as is.\n",
      "     |      \n",
      "     |          .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      kwargs : dict\n",
      "     |          Other variables that the user wants to set that can be referenced in the query\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.3.0\n",
      "     |             Added optional argument ``kwargs`` to specify the mapping of variables in the query.\n",
      "     |             This feature is experimental and unstable.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Executing a SQL query.\n",
      "     |      \n",
      "     |      >>> spark.sql(\"SELECT * FROM range(10) where id > 7\").show()\n",
      "     |      +---+\n",
      "     |      | id|\n",
      "     |      +---+\n",
      "     |      |  8|\n",
      "     |      |  9|\n",
      "     |      +---+\n",
      "     |      \n",
      "     |      Executing a SQL query with variables as Python formatter standard.\n",
      "     |      \n",
      "     |      >>> spark.sql(\n",
      "     |      ...     \"SELECT * FROM range(10) WHERE id > {bound1} AND id < {bound2}\", bound1=7, bound2=9\n",
      "     |      ... ).show()\n",
      "     |      +---+\n",
      "     |      | id|\n",
      "     |      +---+\n",
      "     |      |  8|\n",
      "     |      +---+\n",
      "     |      \n",
      "     |      >>> mydf = spark.range(10)\n",
      "     |      >>> spark.sql(\n",
      "     |      ...     \"SELECT {col} FROM {mydf} WHERE id IN {x}\",\n",
      "     |      ...     col=mydf.id, mydf=mydf, x=tuple(range(4))).show()\n",
      "     |      +---+\n",
      "     |      | id|\n",
      "     |      +---+\n",
      "     |      |  0|\n",
      "     |      |  1|\n",
      "     |      |  2|\n",
      "     |      |  3|\n",
      "     |      +---+\n",
      "     |      \n",
      "     |      >>> spark.sql('''\n",
      "     |      ...   SELECT m1.a, m2.b\n",
      "     |      ...   FROM {table1} m1 INNER JOIN {table2} m2\n",
      "     |      ...   ON m1.key = m2.key\n",
      "     |      ...   ORDER BY m1.a, m2.b''',\n",
      "     |      ...   table1=spark.createDataFrame([(1, \"a\"), (2, \"b\")], [\"a\", \"key\"]),\n",
      "     |      ...   table2=spark.createDataFrame([(3, \"a\"), (4, \"b\"), (5, \"b\")], [\"b\", \"key\"])).show()\n",
      "     |      +---+---+\n",
      "     |      |  a|  b|\n",
      "     |      +---+---+\n",
      "     |      |  1|  3|\n",
      "     |      |  2|  4|\n",
      "     |      |  2|  5|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      Also, it is possible to query using class:`Column` from :class:`DataFrame`.\n",
      "     |      \n",
      "     |      >>> mydf = spark.createDataFrame([(1, 4), (2, 4), (3, 6)], [\"A\", \"B\"])\n",
      "     |      >>> spark.sql(\"SELECT {df.A}, {df[B]} FROM {df}\", df=mydf).show()\n",
      "     |      +---+---+\n",
      "     |      |  A|  B|\n",
      "     |      +---+---+\n",
      "     |      |  1|  4|\n",
      "     |      |  2|  4|\n",
      "     |      |  3|  6|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      And substitude named parameters with the `:` prefix by SQL literals.\n",
      "     |      \n",
      "     |      >>> spark.sql(\"SELECT * FROM {df} WHERE {df[B]} > :minB\", {\"minB\" : 5}, df=mydf).show()\n",
      "     |      +---+---+\n",
      "     |      |  A|  B|\n",
      "     |      +---+---+\n",
      "     |      |  3|  6|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      Or positional parameters marked by `?` in the SQL query by SQL literals.\n",
      "     |      \n",
      "     |      >>> spark.sql(\n",
      "     |      ...   \"SELECT * FROM {df} WHERE {df[B]} > ? and ? < {df[A]}\",\n",
      "     |      ...   args=[5, 2], df=mydf).show()\n",
      "     |      +---+---+\n",
      "     |      |  A|  B|\n",
      "     |      +---+---+\n",
      "     |      |  3|  6|\n",
      "     |      +---+---+\n",
      "     |  \n",
      "     |  stop(self) -> None\n",
      "     |      Stop the underlying :class:`SparkContext`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.stop()  # doctest: +SKIP\n",
      "     |  \n",
      "     |  table(self, tableName: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns the specified table as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          the table name to retrieve.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.range(5).createOrReplaceTempView(\"table1\")\n",
      "     |      >>> spark.table(\"table1\").sort(\"id\").show()\n",
      "     |      +---+\n",
      "     |      | id|\n",
      "     |      +---+\n",
      "     |      |  0|\n",
      "     |      |  1|\n",
      "     |      |  2|\n",
      "     |      |  3|\n",
      "     |      |  4|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  active() -> 'SparkSession' from builtins.type\n",
      "     |      Returns the active or default :class:`SparkSession` for the current thread, returned by\n",
      "     |      the builder.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`SparkSession`\n",
      "     |          Spark session if an active or default session exists for the current thread.\n",
      "     |  \n",
      "     |  getActiveSession() -> Optional[ForwardRef('SparkSession')] from builtins.type\n",
      "     |      Returns the active :class:`SparkSession` for the current thread, returned by the builder\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`SparkSession`\n",
      "     |          Spark session if an active session exists for the current thread\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> s = SparkSession.getActiveSession()\n",
      "     |      >>> df = s.createDataFrame([('Alice', 1)], ['name', 'age'])\n",
      "     |      >>> df.select(\"age\").show()\n",
      "     |      +---+\n",
      "     |      |age|\n",
      "     |      +---+\n",
      "     |      |  1|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  builder\n",
      "     |      Creates a :class:`Builder` for constructing a :class:`SparkSession`.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |  \n",
      "     |  catalog\n",
      "     |      Interface through which the user may create, drop, alter or query underlying\n",
      "     |      databases, tables, functions, etc.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Catalog`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog\n",
      "     |      <...Catalog object ...>\n",
      "     |      \n",
      "     |      Create a temp view, show the list, and drop it.\n",
      "     |      \n",
      "     |      >>> spark.range(1).createTempView(\"test_view\")\n",
      "     |      >>> spark.catalog.listTables()\n",
      "     |      [Table(name='test_view', catalog=None, namespace=[], description=None, ...\n",
      "     |      >>> _ = spark.catalog.dropTempView(\"test_view\")\n",
      "     |  \n",
      "     |  client\n",
      "     |      Gives access to the Spark Connect client. In normal cases this is not necessary to be used\n",
      "     |      and only relevant for testing.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`SparkConnectClient`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is unstable, and a developer API. It returns non-API instance\n",
      "     |      :class:`SparkConnectClient`.\n",
      "     |      This is an API dedicated to Spark Connect client only. With regular Spark Session, it throws\n",
      "     |      an exception.\n",
      "     |  \n",
      "     |  conf\n",
      "     |      Runtime configuration interface for Spark.\n",
      "     |      \n",
      "     |      This is the interface through which the user can get and set all Spark and Hadoop\n",
      "     |      configurations that are relevant to Spark SQL. When getting the value of a config,\n",
      "     |      this defaults to the value set in the underlying :class:`SparkContext`, if any.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`pyspark.sql.conf.RuntimeConfig`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.conf\n",
      "     |      <pyspark...RuntimeConf...>\n",
      "     |      \n",
      "     |      Set a runtime configuration for the session\n",
      "     |      \n",
      "     |      >>> spark.conf.set(\"key\", \"value\")\n",
      "     |      >>> spark.conf.get(\"key\")\n",
      "     |      'value'\n",
      "     |  \n",
      "     |  read\n",
      "     |      Returns a :class:`DataFrameReader` that can be used to read data\n",
      "     |      in as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrameReader`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.read\n",
      "     |      <...DataFrameReader object ...>\n",
      "     |      \n",
      "     |      Write a DataFrame into a JSON file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a JSON file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the JSON file as a DataFrame.\n",
      "     |      ...     spark.read.format('json').load(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  readStream\n",
      "     |      Returns a :class:`DataStreamReader` that can be used to read data streams\n",
      "     |      as a streaming :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataStreamReader`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.readStream\n",
      "     |      <pyspark...DataStreamReader object ...>\n",
      "     |      \n",
      "     |      The example below uses Rate source that generates rows continuously.\n",
      "     |      After that, we operate a modulo by 3, and then write the stream out to the console.\n",
      "     |      The streaming query stops in 3 seconds.\n",
      "     |      \n",
      "     |      >>> import time\n",
      "     |      >>> df = spark.readStream.format(\"rate\").load()\n",
      "     |      >>> df = df.selectExpr(\"value % 3 as v\")\n",
      "     |      >>> q = df.writeStream.format(\"console\").start()\n",
      "     |      >>> time.sleep(3)\n",
      "     |      >>> q.stop()\n",
      "     |  \n",
      "     |  sparkContext\n",
      "     |      Returns the underlying :class:`SparkContext`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`SparkContext`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.sparkContext\n",
      "     |      <SparkContext master=... appName=...>\n",
      "     |      \n",
      "     |      Create an RDD from the Spark context\n",
      "     |      \n",
      "     |      >>> rdd = spark.sparkContext.parallelize([1, 2, 3])\n",
      "     |      >>> rdd.collect()\n",
      "     |      [1, 2, 3]\n",
      "     |  \n",
      "     |  streams\n",
      "     |      Returns a :class:`StreamingQueryManager` that allows managing all the\n",
      "     |      :class:`StreamingQuery` instances active on `this` context.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`StreamingQueryManager`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.streams\n",
      "     |      <pyspark...StreamingQueryManager object ...>\n",
      "     |      \n",
      "     |      Get the list of active streaming queries\n",
      "     |      \n",
      "     |      >>> sq = spark.readStream.format(\n",
      "     |      ...     \"rate\").load().writeStream.format('memory').queryName('this_query').start()\n",
      "     |      >>> sqm = spark.streams\n",
      "     |      >>> [q.name for q in sqm.active]\n",
      "     |      ['this_query']\n",
      "     |      >>> sq.stop()\n",
      "     |  \n",
      "     |  udf\n",
      "     |      Returns a :class:`UDFRegistration` for UDF registration.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`UDFRegistration`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Register a Python UDF, and use it in SQL.\n",
      "     |      \n",
      "     |      >>> strlen = spark.udf.register(\"strlen\", lambda x: len(x))\n",
      "     |      >>> spark.sql(\"SELECT strlen('test')\").show()\n",
      "     |      +------------+\n",
      "     |      |strlen(test)|\n",
      "     |      +------------+\n",
      "     |      |           4|\n",
      "     |      +------------+\n",
      "     |  \n",
      "     |  udtf\n",
      "     |      Returns a :class:`UDTFRegistration` for UDTF registration.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`UDTFRegistration`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  version\n",
      "     |      The version of Spark on which this application is running.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      str\n",
      "     |          the version of Spark in string.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = spark.version\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Builder = <class 'pyspark.sql.session.SparkSession.Builder'>\n",
      "     |      Builder for :class:`SparkSession`.\n",
      "     |  \n",
      "     |  \n",
      "     |  __annotations__ = {'_activeSession': typing.ClassVar[typing.Optional[F...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pyspark.sql.pandas.conversion.SparkConversionMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class UDFRegistration(builtins.object)\n",
      "     |  UDFRegistration(sparkSession: 'SparkSession')\n",
      "     |  \n",
      "     |  Wrapper for user-defined function registration. This instance can be accessed by\n",
      "     |  :attr:`spark.udf` or :attr:`sqlContext.udf`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.3.1\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sparkSession: 'SparkSession')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  register(self, name: str, f: Union[Callable[..., Any], ForwardRef('UserDefinedFunctionLike')], returnType: Optional[ForwardRef('DataTypeOrString')] = None) -> 'UserDefinedFunctionLike'\n",
      "     |      Register a Python function (including lambda function) or a user-defined function\n",
      "     |      as a SQL function.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str,\n",
      "     |          name of the user-defined function in SQL statements.\n",
      "     |      f : function, :meth:`pyspark.sql.functions.udf` or :meth:`pyspark.sql.functions.pandas_udf`\n",
      "     |          a Python function, or a user-defined function. The user-defined function can\n",
      "     |          be either row-at-a-time or vectorized. See :meth:`pyspark.sql.functions.udf` and\n",
      "     |          :meth:`pyspark.sql.functions.pandas_udf`.\n",
      "     |      returnType : :class:`pyspark.sql.types.DataType` or str, optional\n",
      "     |          the return type of the registered user-defined function. The value can\n",
      "     |          be either a :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "     |          `returnType` can be optionally specified when `f` is a Python function but not\n",
      "     |          when `f` is a user-defined function. Please see the examples below.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      function\n",
      "     |          a user-defined function\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      To register a nondeterministic Python function, users need to first build\n",
      "     |      a nondeterministic user-defined function for the Python function and then register it\n",
      "     |      as a SQL function.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      1. When `f` is a Python function:\n",
      "     |      \n",
      "     |          `returnType` defaults to string type and can be optionally specified. The produced\n",
      "     |          object must match the specified type. In this case, this API works as if\n",
      "     |          `register(name, f, returnType=StringType())`.\n",
      "     |      \n",
      "     |          >>> strlen = spark.udf.register(\"stringLengthString\", lambda x: len(x))\n",
      "     |          >>> spark.sql(\"SELECT stringLengthString('test')\").collect()\n",
      "     |          [Row(stringLengthString(test)='4')]\n",
      "     |      \n",
      "     |          >>> spark.sql(\"SELECT 'foo' AS text\").select(strlen(\"text\")).collect()\n",
      "     |          [Row(stringLengthString(text)='3')]\n",
      "     |      \n",
      "     |          >>> from pyspark.sql.types import IntegerType\n",
      "     |          >>> _ = spark.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n",
      "     |          >>> spark.sql(\"SELECT stringLengthInt('test')\").collect()\n",
      "     |          [Row(stringLengthInt(test)=4)]\n",
      "     |      \n",
      "     |          >>> from pyspark.sql.types import IntegerType\n",
      "     |          >>> _ = spark.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n",
      "     |          >>> spark.sql(\"SELECT stringLengthInt('test')\").collect()\n",
      "     |          [Row(stringLengthInt(test)=4)]\n",
      "     |      \n",
      "     |      2. When `f` is a user-defined function (from Spark 2.3.0):\n",
      "     |      \n",
      "     |          Spark uses the return type of the given user-defined function as the return type of\n",
      "     |          the registered user-defined function. `returnType` should not be specified.\n",
      "     |          In this case, this API works as if `register(name, f)`.\n",
      "     |      \n",
      "     |          >>> from pyspark.sql.types import IntegerType\n",
      "     |          >>> from pyspark.sql.functions import udf\n",
      "     |          >>> slen = udf(lambda s: len(s), IntegerType())\n",
      "     |          >>> _ = spark.udf.register(\"slen\", slen)\n",
      "     |          >>> spark.sql(\"SELECT slen('test')\").collect()\n",
      "     |          [Row(slen(test)=4)]\n",
      "     |      \n",
      "     |          >>> import random\n",
      "     |          >>> from pyspark.sql.functions import udf\n",
      "     |          >>> from pyspark.sql.types import IntegerType\n",
      "     |          >>> random_udf = udf(lambda: random.randint(0, 100), IntegerType()).asNondeterministic()\n",
      "     |          >>> new_random_udf = spark.udf.register(\"random_udf\", random_udf)\n",
      "     |          >>> spark.sql(\"SELECT random_udf()\").collect()  # doctest: +SKIP\n",
      "     |          [Row(random_udf()=82)]\n",
      "     |      \n",
      "     |          >>> import pandas as pd  # doctest: +SKIP\n",
      "     |          >>> from pyspark.sql.functions import pandas_udf\n",
      "     |          >>> @pandas_udf(\"integer\")  # doctest: +SKIP\n",
      "     |          ... def add_one(s: pd.Series) -> pd.Series:\n",
      "     |          ...     return s + 1\n",
      "     |          ...\n",
      "     |          >>> _ = spark.udf.register(\"add_one\", add_one)  # doctest: +SKIP\n",
      "     |          >>> spark.sql(\"SELECT add_one(id) FROM range(3)\").collect()  # doctest: +SKIP\n",
      "     |          [Row(add_one(id)=1), Row(add_one(id)=2), Row(add_one(id)=3)]\n",
      "     |      \n",
      "     |          >>> @pandas_udf(\"integer\")  # doctest: +SKIP\n",
      "     |          ... def sum_udf(v: pd.Series) -> int:\n",
      "     |          ...     return v.sum()\n",
      "     |          ...\n",
      "     |          >>> _ = spark.udf.register(\"sum_udf\", sum_udf)  # doctest: +SKIP\n",
      "     |          >>> q = \"SELECT sum_udf(v1) FROM VALUES (3, 0), (2, 0), (1, 1) tbl(v1, v2) GROUP BY v2\"\n",
      "     |          >>> spark.sql(q).collect()  # doctest: +SKIP\n",
      "     |          [Row(sum_udf(v1)=1), Row(sum_udf(v1)=5)]\n",
      "     |  \n",
      "     |  registerJavaFunction(self, name: str, javaClassName: str, returnType: Optional[ForwardRef('DataTypeOrString')] = None) -> None\n",
      "     |      Register a Java user-defined function as a SQL function.\n",
      "     |      \n",
      "     |      In addition to a name and the function itself, the return type can be optionally specified.\n",
      "     |      When the return type is not specified we would infer it via reflection.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          name of the user-defined function\n",
      "     |      javaClassName : str\n",
      "     |          fully qualified name of java class\n",
      "     |      returnType : :class:`pyspark.sql.types.DataType` or str, optional\n",
      "     |          the return type of the registered Java function. The value can be either\n",
      "     |          a :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.types import IntegerType\n",
      "     |      >>> spark.udf.registerJavaFunction(\n",
      "     |      ...     \"javaStringLength\", \"test.org.apache.spark.sql.JavaStringLength\", IntegerType())\n",
      "     |      ... # doctest: +SKIP\n",
      "     |      >>> spark.sql(\"SELECT javaStringLength('test')\").collect()  # doctest: +SKIP\n",
      "     |      [Row(javaStringLength(test)=4)]\n",
      "     |      \n",
      "     |      >>> spark.udf.registerJavaFunction(\n",
      "     |      ...     \"javaStringLength2\", \"test.org.apache.spark.sql.JavaStringLength\")\n",
      "     |      ... # doctest: +SKIP\n",
      "     |      >>> spark.sql(\"SELECT javaStringLength2('test')\").collect()  # doctest: +SKIP\n",
      "     |      [Row(javaStringLength2(test)=4)]\n",
      "     |      \n",
      "     |      >>> spark.udf.registerJavaFunction(\n",
      "     |      ...     \"javaStringLength3\", \"test.org.apache.spark.sql.JavaStringLength\", \"integer\")\n",
      "     |      ... # doctest: +SKIP\n",
      "     |      >>> spark.sql(\"SELECT javaStringLength3('test')\").collect()  # doctest: +SKIP\n",
      "     |      [Row(javaStringLength3(test)=4)]\n",
      "     |  \n",
      "     |  registerJavaUDAF(self, name: str, javaClassName: str) -> None\n",
      "     |      Register a Java user-defined aggregate function as a SQL function.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      name : str\n",
      "     |          name of the user-defined aggregate function\n",
      "     |      javaClassName : str\n",
      "     |          fully qualified name of java class\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.udf.registerJavaUDAF(\"javaUDAF\", \"test.org.apache.spark.sql.MyDoubleAvg\")\n",
      "     |      ... # doctest: +SKIP\n",
      "     |      >>> df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"a\")],[\"id\", \"name\"])\n",
      "     |      >>> df.createOrReplaceTempView(\"df\")\n",
      "     |      >>> q = \"SELECT name, javaUDAF(id) as avg from df group by name order by name desc\"\n",
      "     |      >>> spark.sql(q).collect()  # doctest: +SKIP\n",
      "     |      [Row(name='b', avg=102.0), Row(name='a', avg=102.0)]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class UDTFRegistration(builtins.object)\n",
      "     |  UDTFRegistration(sparkSession: 'SparkSession')\n",
      "     |  \n",
      "     |  Wrapper for user-defined table function registration. This instance can be accessed by\n",
      "     |  :attr:`spark.udtf` or :attr:`sqlContext.udtf`.\n",
      "     |  \n",
      "     |  .. versionadded:: 3.5.0\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sparkSession: 'SparkSession')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  register(self, name: str, f: 'UserDefinedTableFunction') -> 'UserDefinedTableFunction'\n",
      "     |      Register a Python user-defined table function as a SQL table function.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          The name of the user-defined table function in SQL statements.\n",
      "     |      f : function or :meth:`pyspark.sql.functions.udtf`\n",
      "     |          The user-defined table function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      function\n",
      "     |          The registered user-defined table function.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Spark uses the return type of the given user-defined table function as the return\n",
      "     |      type of the registered user-defined function.\n",
      "     |      \n",
      "     |      To register a nondeterministic Python table function, users need to first build\n",
      "     |      a nondeterministic user-defined table function and then register it as a SQL function.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import udtf\n",
      "     |      >>> @udtf(returnType=\"c1: int, c2: int\")\n",
      "     |      ... class PlusOne:\n",
      "     |      ...     def eval(self, x: int):\n",
      "     |      ...         yield x, x + 1\n",
      "     |      ...\n",
      "     |      >>> _ = spark.udtf.register(name=\"plus_one\", f=PlusOne)\n",
      "     |      >>> spark.sql(\"SELECT * FROM plus_one(1)\").collect()\n",
      "     |      [Row(c1=1, c2=2)]\n",
      "     |      \n",
      "     |      Use it with lateral join\n",
      "     |      \n",
      "     |      >>> spark.sql(\"SELECT * FROM VALUES (0, 1), (1, 2) t(x, y), LATERAL plus_one(x)\").collect()\n",
      "     |      [Row(x=0, y=1, c1=0, c2=1), Row(x=1, y=2, c1=1, c2=2)]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Window(builtins.object)\n",
      "     |  Utility functions for defining window in DataFrames.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  When ordering is not defined, an unbounded window frame (rowFrame,\n",
      "     |  unboundedPreceding, unboundedFollowing) is used by default. When ordering is defined,\n",
      "     |  a growing window frame (rangeFrame, unboundedPreceding, currentRow) is used by default.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> # ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
      "     |  >>> window = Window.orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
      "     |  \n",
      "     |  >>> # PARTITION BY country ORDER BY date RANGE BETWEEN 3 PRECEDING AND 3 FOLLOWING\n",
      "     |  >>> window = Window.orderBy(\"date\").partitionBy(\"country\").rangeBetween(-3, 3)\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  orderBy(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')]]) -> 'WindowSpec'\n",
      "     |      Creates a :class:`WindowSpec` with the ordering defined.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, :class:`Column` or list\n",
      "     |          names of columns or expressions\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class: `WindowSpec`\n",
      "     |          A :class:`WindowSpec` with the ordering defined.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Window\n",
      "     |      >>> from pyspark.sql.functions import row_number\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(1, \"a\"), (1, \"a\"), (2, \"a\"), (1, \"b\"), (2, \"b\"), (3, \"b\")], [\"id\", \"category\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+--------+\n",
      "     |      | id|category|\n",
      "     |      +---+--------+\n",
      "     |      |  1|       a|\n",
      "     |      |  1|       a|\n",
      "     |      |  2|       a|\n",
      "     |      |  1|       b|\n",
      "     |      |  2|       b|\n",
      "     |      |  3|       b|\n",
      "     |      +---+--------+\n",
      "     |      \n",
      "     |      Show row number order by ``category`` in partition ``id``.\n",
      "     |      \n",
      "     |      >>> window = Window.partitionBy(\"id\").orderBy(\"category\")\n",
      "     |      >>> df.withColumn(\"row_number\", row_number().over(window)).show()\n",
      "     |      +---+--------+----------+\n",
      "     |      | id|category|row_number|\n",
      "     |      +---+--------+----------+\n",
      "     |      |  1|       a|         1|\n",
      "     |      |  1|       a|         2|\n",
      "     |      |  1|       b|         3|\n",
      "     |      |  2|       a|         1|\n",
      "     |      |  2|       b|         2|\n",
      "     |      |  3|       b|         1|\n",
      "     |      +---+--------+----------+\n",
      "     |  \n",
      "     |  partitionBy(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')]]) -> 'WindowSpec'\n",
      "     |      Creates a :class:`WindowSpec` with the partitioning defined.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, :class:`Column` or list\n",
      "     |          names of columns or expressions\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class: `WindowSpec`\n",
      "     |          A :class:`WindowSpec` with the partitioning defined.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Window\n",
      "     |      >>> from pyspark.sql.functions import row_number\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(1, \"a\"), (1, \"a\"), (2, \"a\"), (1, \"b\"), (2, \"b\"), (3, \"b\")], [\"id\", \"category\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+--------+\n",
      "     |      | id|category|\n",
      "     |      +---+--------+\n",
      "     |      |  1|       a|\n",
      "     |      |  1|       a|\n",
      "     |      |  2|       a|\n",
      "     |      |  1|       b|\n",
      "     |      |  2|       b|\n",
      "     |      |  3|       b|\n",
      "     |      +---+--------+\n",
      "     |      \n",
      "     |      Show row number order by ``id`` in partition ``category``.\n",
      "     |      \n",
      "     |      >>> window = Window.partitionBy(\"category\").orderBy(\"id\")\n",
      "     |      >>> df.withColumn(\"row_number\", row_number().over(window)).show()\n",
      "     |      +---+--------+----------+\n",
      "     |      | id|category|row_number|\n",
      "     |      +---+--------+----------+\n",
      "     |      |  1|       a|         1|\n",
      "     |      |  1|       a|         2|\n",
      "     |      |  2|       a|         3|\n",
      "     |      |  1|       b|         1|\n",
      "     |      |  2|       b|         2|\n",
      "     |      |  3|       b|         3|\n",
      "     |      +---+--------+----------+\n",
      "     |  \n",
      "     |  rangeBetween(start: int, end: int) -> 'WindowSpec'\n",
      "     |      Creates a :class:`WindowSpec` with the frame boundaries defined,\n",
      "     |      from `start` (inclusive) to `end` (inclusive).\n",
      "     |      \n",
      "     |      Both `start` and `end` are relative from the current row. For example,\n",
      "     |      \"0\" means \"current row\", while \"-1\" means one off before the current row,\n",
      "     |      and \"5\" means the five off after the current row.\n",
      "     |      \n",
      "     |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      "     |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      "     |      values directly.\n",
      "     |      \n",
      "     |      A range-based boundary is based on the actual value of the ORDER BY\n",
      "     |      expression(s). An offset is used to alter the value of the ORDER BY expression, for\n",
      "     |      instance if the current ORDER BY expression has a value of 10 and the lower bound offset\n",
      "     |      is -3, the resulting lower bound for the current row will be 10 - 3 = 7. This however puts a\n",
      "     |      number of constraints on the ORDER BY expressions: there can be only one expression and this\n",
      "     |      expression must have a numerical data type. An exception can be made when the offset is\n",
      "     |      unbounded, because no value modification is needed, in this case multiple and non-numeric\n",
      "     |      ORDER BY expression are allowed.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      start : int\n",
      "     |          boundary start, inclusive.\n",
      "     |          The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      "     |          any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n",
      "     |      end : int\n",
      "     |          boundary end, inclusive.\n",
      "     |          The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      "     |          any value greater than or equal to min(sys.maxsize, 9223372036854775807).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class: `WindowSpec`\n",
      "     |          A :class:`WindowSpec` with the frame boundaries defined,\n",
      "     |          from `start` (inclusive) to `end` (inclusive).\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Window\n",
      "     |      >>> from pyspark.sql import functions as func\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(1, \"a\"), (1, \"a\"), (2, \"a\"), (1, \"b\"), (2, \"b\"), (3, \"b\")], [\"id\", \"category\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+--------+\n",
      "     |      | id|category|\n",
      "     |      +---+--------+\n",
      "     |      |  1|       a|\n",
      "     |      |  1|       a|\n",
      "     |      |  2|       a|\n",
      "     |      |  1|       b|\n",
      "     |      |  2|       b|\n",
      "     |      |  3|       b|\n",
      "     |      +---+--------+\n",
      "     |      \n",
      "     |      Calculate sum of ``id`` in the range from ``id`` of currentRow to ``id`` of currentRow + 1\n",
      "     |      in partition ``category``\n",
      "     |      \n",
      "     |      >>> window = Window.partitionBy(\"category\").orderBy(\"id\").rangeBetween(Window.currentRow, 1)\n",
      "     |      >>> df.withColumn(\"sum\", func.sum(\"id\").over(window)).sort(\"id\", \"category\").show()\n",
      "     |      +---+--------+---+\n",
      "     |      | id|category|sum|\n",
      "     |      +---+--------+---+\n",
      "     |      |  1|       a|  4|\n",
      "     |      |  1|       a|  4|\n",
      "     |      |  1|       b|  3|\n",
      "     |      |  2|       a|  2|\n",
      "     |      |  2|       b|  5|\n",
      "     |      |  3|       b|  3|\n",
      "     |      +---+--------+---+\n",
      "     |  \n",
      "     |  rowsBetween(start: int, end: int) -> 'WindowSpec'\n",
      "     |      Creates a :class:`WindowSpec` with the frame boundaries defined,\n",
      "     |      from `start` (inclusive) to `end` (inclusive).\n",
      "     |      \n",
      "     |      Both `start` and `end` are relative positions from the current row.\n",
      "     |      For example, \"0\" means \"current row\", while \"-1\" means the row before\n",
      "     |      the current row, and \"5\" means the fifth row after the current row.\n",
      "     |      \n",
      "     |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      "     |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      "     |      values directly.\n",
      "     |      \n",
      "     |      A row based boundary is based on the position of the row within the partition.\n",
      "     |      An offset indicates the number of rows above or below the current row, the frame for the\n",
      "     |      current row starts or ends. For instance, given a row based sliding frame with a lower bound\n",
      "     |      offset of -1 and a upper bound offset of +2. The frame for row with index 5 would range from\n",
      "     |      index 4 to index 7.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      start : int\n",
      "     |          boundary start, inclusive.\n",
      "     |          The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      "     |          any value less than or equal to -9223372036854775808.\n",
      "     |      end : int\n",
      "     |          boundary end, inclusive.\n",
      "     |          The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      "     |          any value greater than or equal to 9223372036854775807.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class: `WindowSpec`\n",
      "     |          A :class:`WindowSpec` with the frame boundaries defined,\n",
      "     |          from `start` (inclusive) to `end` (inclusive).\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Window\n",
      "     |      >>> from pyspark.sql import functions as func\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(1, \"a\"), (1, \"a\"), (2, \"a\"), (1, \"b\"), (2, \"b\"), (3, \"b\")], [\"id\", \"category\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+--------+\n",
      "     |      | id|category|\n",
      "     |      +---+--------+\n",
      "     |      |  1|       a|\n",
      "     |      |  1|       a|\n",
      "     |      |  2|       a|\n",
      "     |      |  1|       b|\n",
      "     |      |  2|       b|\n",
      "     |      |  3|       b|\n",
      "     |      +---+--------+\n",
      "     |      \n",
      "     |      Calculate sum of ``id`` in the range from currentRow to currentRow + 1\n",
      "     |      in partition ``category``\n",
      "     |      \n",
      "     |      >>> window = Window.partitionBy(\"category\").orderBy(\"id\").rowsBetween(Window.currentRow, 1)\n",
      "     |      >>> df.withColumn(\"sum\", func.sum(\"id\").over(window)).sort(\"id\", \"category\", \"sum\").show()\n",
      "     |      +---+--------+---+\n",
      "     |      | id|category|sum|\n",
      "     |      +---+--------+---+\n",
      "     |      |  1|       a|  2|\n",
      "     |      |  1|       a|  3|\n",
      "     |      |  1|       b|  3|\n",
      "     |      |  2|       a|  2|\n",
      "     |      |  2|       b|  5|\n",
      "     |      |  3|       b|  3|\n",
      "     |      +---+--------+---+\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'currentRow': <class 'int'>, 'unboundedFollowing': ...\n",
      "     |  \n",
      "     |  currentRow = 0\n",
      "     |  \n",
      "     |  unboundedFollowing = 9223372036854775807\n",
      "     |  \n",
      "     |  unboundedPreceding = -9223372036854775808\n",
      "    \n",
      "    class WindowSpec(builtins.object)\n",
      "     |  WindowSpec(jspec: py4j.java_gateway.JavaObject) -> None\n",
      "     |  \n",
      "     |  A window specification that defines the partitioning, ordering,\n",
      "     |  and frame boundaries.\n",
      "     |  \n",
      "     |  Use the static methods in :class:`Window` to create a :class:`WindowSpec`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, jspec: py4j.java_gateway.JavaObject) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  orderBy(self, *cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')]]) -> 'WindowSpec'\n",
      "     |      Defines the ordering columns in a :class:`WindowSpec`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, :class:`Column` or list\n",
      "     |          names of columns or expressions\n",
      "     |  \n",
      "     |  partitionBy(self, *cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')]]) -> 'WindowSpec'\n",
      "     |      Defines the partitioning columns in a :class:`WindowSpec`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, :class:`Column` or list\n",
      "     |          names of columns or expressions\n",
      "     |  \n",
      "     |  rangeBetween(self, start: int, end: int) -> 'WindowSpec'\n",
      "     |      Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).\n",
      "     |      \n",
      "     |      Both `start` and `end` are relative from the current row. For example,\n",
      "     |      \"0\" means \"current row\", while \"-1\" means one off before the current row,\n",
      "     |      and \"5\" means the five off after the current row.\n",
      "     |      \n",
      "     |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      "     |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      "     |      values directly.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      start : int\n",
      "     |          boundary start, inclusive.\n",
      "     |          The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      "     |          any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n",
      "     |      end : int\n",
      "     |          boundary end, inclusive.\n",
      "     |          The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      "     |          any value greater than or equal to min(sys.maxsize, 9223372036854775807).\n",
      "     |  \n",
      "     |  rowsBetween(self, start: int, end: int) -> 'WindowSpec'\n",
      "     |      Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).\n",
      "     |      \n",
      "     |      Both `start` and `end` are relative positions from the current row.\n",
      "     |      For example, \"0\" means \"current row\", while \"-1\" means the row before\n",
      "     |      the current row, and \"5\" means the fifth row after the current row.\n",
      "     |      \n",
      "     |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      "     |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      "     |      values directly.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      start : int\n",
      "     |          boundary start, inclusive.\n",
      "     |          The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      "     |          any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n",
      "     |      end : int\n",
      "     |          boundary end, inclusive.\n",
      "     |          The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      "     |          any value greater than or equal to min(sys.maxsize, 9223372036854775807).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['SparkSession', 'SQLContext', 'HiveContext', 'UDFRegistrati...\n",
      "\n",
      "FILE\n",
      "    /usr/local/spark/python/pyspark/sql/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pyspark.sql)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05f35b1c-d067-47fb-bb9e-e781b609b8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package pyspark.sql in pyspark:\n",
      "\n",
      "NAME\n",
      "    pyspark.sql - Important classes of Spark SQL and DataFrames:\n",
      "\n",
      "DESCRIPTION\n",
      "        - :class:`pyspark.sql.SparkSession`\n",
      "          Main entry point for :class:`DataFrame` and SQL functionality.\n",
      "        - :class:`pyspark.sql.DataFrame`\n",
      "          A distributed collection of data grouped into named columns.\n",
      "        - :class:`pyspark.sql.Column`\n",
      "          A column expression in a :class:`DataFrame`.\n",
      "        - :class:`pyspark.sql.Row`\n",
      "          A row of data in a :class:`DataFrame`.\n",
      "        - :class:`pyspark.sql.GroupedData`\n",
      "          Aggregation methods, returned by :func:`DataFrame.groupBy`.\n",
      "        - :class:`pyspark.sql.DataFrameNaFunctions`\n",
      "          Methods for handling missing data (null values).\n",
      "        - :class:`pyspark.sql.DataFrameStatFunctions`\n",
      "          Methods for statistics functionality.\n",
      "        - :class:`pyspark.sql.functions`\n",
      "          List of built-in functions available for :class:`DataFrame`.\n",
      "        - :class:`pyspark.sql.types`\n",
      "          List of data types available.\n",
      "        - :class:`pyspark.sql.Window`\n",
      "          For working with window functions.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    avro (package)\n",
      "    catalog\n",
      "    column\n",
      "    conf\n",
      "    connect (package)\n",
      "    context\n",
      "    dataframe\n",
      "    functions\n",
      "    group\n",
      "    observation\n",
      "    pandas (package)\n",
      "    protobuf (package)\n",
      "    readwriter\n",
      "    session\n",
      "    sql_formatter\n",
      "    streaming (package)\n",
      "    tests (package)\n",
      "    types\n",
      "    udf\n",
      "    udtf\n",
      "    utils\n",
      "    window\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        pyspark.sql.catalog.Catalog\n",
      "        pyspark.sql.column.Column\n",
      "        pyspark.sql.context.SQLContext\n",
      "            pyspark.sql.context.HiveContext\n",
      "        pyspark.sql.dataframe.DataFrameNaFunctions\n",
      "        pyspark.sql.dataframe.DataFrameStatFunctions\n",
      "        pyspark.sql.observation.Observation\n",
      "        pyspark.sql.pandas.group_ops.PandasCogroupedOps\n",
      "        pyspark.sql.readwriter.DataFrameWriterV2\n",
      "        pyspark.sql.udf.UDFRegistration\n",
      "        pyspark.sql.udtf.UDTFRegistration\n",
      "        pyspark.sql.window.Window\n",
      "        pyspark.sql.window.WindowSpec\n",
      "    builtins.tuple(builtins.object)\n",
      "        pyspark.sql.types.Row\n",
      "    pyspark.sql.pandas.conversion.PandasConversionMixin(builtins.object)\n",
      "        pyspark.sql.dataframe.DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
      "    pyspark.sql.pandas.conversion.SparkConversionMixin(builtins.object)\n",
      "        pyspark.sql.session.SparkSession\n",
      "    pyspark.sql.pandas.group_ops.PandasGroupedOpsMixin(builtins.object)\n",
      "        pyspark.sql.group.GroupedData\n",
      "    pyspark.sql.pandas.map_ops.PandasMapOpsMixin(builtins.object)\n",
      "        pyspark.sql.dataframe.DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
      "    pyspark.sql.readwriter.OptionUtils(builtins.object)\n",
      "        pyspark.sql.readwriter.DataFrameReader\n",
      "        pyspark.sql.readwriter.DataFrameWriter\n",
      "    \n",
      "    class Catalog(builtins.object)\n",
      "     |  Catalog(sparkSession: pyspark.sql.session.SparkSession) -> None\n",
      "     |  \n",
      "     |  User-facing catalog API, accessible through `SparkSession.catalog`.\n",
      "     |  \n",
      "     |  This is a thin wrapper around its Scala implementation org.apache.spark.sql.catalog.Catalog.\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sparkSession: pyspark.sql.session.SparkSession) -> None\n",
      "     |      Create a new Catalog that wraps the underlying JVM object.\n",
      "     |  \n",
      "     |  cacheTable(self, tableName: str, storageLevel: Optional[pyspark.storagelevel.StorageLevel] = None) -> None\n",
      "     |      Caches the specified table in-memory or with given storage level.\n",
      "     |      Default MEMORY_AND_DISK.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          name of the table to get.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |              Allow ``tableName`` to be qualified with catalog name.\n",
      "     |      \n",
      "     |      storageLevel : :class:`StorageLevel`\n",
      "     |          storage level to set for persistence.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.5.0\n",
      "     |              Allow to specify storage level.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      >>> _ = spark.sql(\"CREATE TABLE tbl1 (name STRING, age INT) USING parquet\")\n",
      "     |      >>> spark.catalog.cacheTable(\"tbl1\")\n",
      "     |      \n",
      "     |      or\n",
      "     |      \n",
      "     |      >>> spark.catalog.cacheTable(\"tbl1\", StorageLevel.OFF_HEAP)\n",
      "     |      \n",
      "     |      Throw an analysis exception when the table does not exist.\n",
      "     |      \n",
      "     |      >>> spark.catalog.cacheTable(\"not_existing_table\")\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      AnalysisException: ...\n",
      "     |      \n",
      "     |      Using the fully qualified name for the table.\n",
      "     |      \n",
      "     |      >>> spark.catalog.cacheTable(\"spark_catalog.default.tbl1\")\n",
      "     |      >>> spark.catalog.uncacheTable(\"tbl1\")\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |  \n",
      "     |  clearCache(self) -> None\n",
      "     |      Removes all cached tables from the in-memory cache.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      >>> _ = spark.sql(\"CREATE TABLE tbl1 (name STRING, age INT) USING parquet\")\n",
      "     |      >>> spark.catalog.clearCache()\n",
      "     |      >>> spark.catalog.isCached(\"tbl1\")\n",
      "     |      False\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |  \n",
      "     |  createExternalTable(self, tableName: str, path: Optional[str] = None, source: Optional[str] = None, schema: Optional[pyspark.sql.types.StructType] = None, **options: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Creates a table based on the dataset in a data source.\n",
      "     |      \n",
      "     |      It returns the DataFrame associated with the external table.\n",
      "     |      \n",
      "     |      The data source is specified by the ``source`` and a set of ``options``.\n",
      "     |      If ``source`` is not specified, the default data source configured by\n",
      "     |      ``spark.sql.sources.default`` will be used.\n",
      "     |      \n",
      "     |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
      "     |      created external table.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |  \n",
      "     |  createTable(self, tableName: str, path: Optional[str] = None, source: Optional[str] = None, schema: Optional[pyspark.sql.types.StructType] = None, description: Optional[str] = None, **options: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Creates a table based on the dataset in a data source.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.2.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          name of the table to create.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |             Allow ``tableName`` to be qualified with catalog name.\n",
      "     |      \n",
      "     |      path : str, optional\n",
      "     |          the path in which the data for this table exists.\n",
      "     |          When ``path`` is specified, an external table is\n",
      "     |          created from the data at the given path. Otherwise a managed table is created.\n",
      "     |      source : str, optional\n",
      "     |          the source of this table such as 'parquet, 'orc', etc.\n",
      "     |          If ``source`` is not specified, the default data source configured by\n",
      "     |          ``spark.sql.sources.default`` will be used.\n",
      "     |      schema : class:`StructType`, optional\n",
      "     |          the schema for this table.\n",
      "     |      description : str, optional\n",
      "     |          the description of this table.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.1.0\n",
      "     |              Added the ``description`` parameter.\n",
      "     |      \n",
      "     |      **options : dict, optional\n",
      "     |          extra options to specify in the table.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          The DataFrame associated with the table.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Creating a managed table.\n",
      "     |      \n",
      "     |      >>> _ = spark.catalog.createTable(\"tbl1\", schema=spark.range(1).schema, source='parquet')\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |      \n",
      "     |      Creating an external table\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     _ = spark.catalog.createTable(\n",
      "     |      ...         \"tbl2\", schema=spark.range(1).schema, path=d, source='parquet')\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl2\")\n",
      "     |  \n",
      "     |  currentCatalog(self) -> str\n",
      "     |      Returns the current default catalog in this session.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog.currentCatalog()\n",
      "     |      'spark_catalog'\n",
      "     |  \n",
      "     |  currentDatabase(self) -> str\n",
      "     |      Returns the current default database in this session.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      str\n",
      "     |          The current default database name.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog.currentDatabase()\n",
      "     |      'default'\n",
      "     |  \n",
      "     |  databaseExists(self, dbName: str) -> bool\n",
      "     |      Check if the database with the specified name exists.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dbName : str\n",
      "     |          name of the database to check existence\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |             Allow ``dbName`` to be qualified with catalog name.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          Indicating whether the database exists\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Check if 'test_new_database' database exists\n",
      "     |      \n",
      "     |      >>> spark.catalog.databaseExists(\"test_new_database\")\n",
      "     |      False\n",
      "     |      >>> _ = spark.sql(\"CREATE DATABASE test_new_database\")\n",
      "     |      >>> spark.catalog.databaseExists(\"test_new_database\")\n",
      "     |      True\n",
      "     |      \n",
      "     |      Using the fully qualified name with the catalog name.\n",
      "     |      \n",
      "     |      >>> spark.catalog.databaseExists(\"spark_catalog.test_new_database\")\n",
      "     |      True\n",
      "     |      >>> _ = spark.sql(\"DROP DATABASE test_new_database\")\n",
      "     |  \n",
      "     |  dropGlobalTempView(self, viewName: str) -> bool\n",
      "     |      Drops the global temporary view with the given view name in the catalog.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      viewName : str\n",
      "     |          name of the global view to drop.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          If the global view was successfully dropped or not.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If the view has been cached before, then it will also be uncached.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.createDataFrame([(1, 1)]).createGlobalTempView(\"my_table\")\n",
      "     |      \n",
      "     |      Dropping the global view.\n",
      "     |      \n",
      "     |      >>> spark.catalog.dropGlobalTempView(\"my_table\")\n",
      "     |      True\n",
      "     |      \n",
      "     |      Throw an exception if the global view does not exists.\n",
      "     |      \n",
      "     |      >>> spark.table(\"global_temp.my_table\")\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      AnalysisException: ...\n",
      "     |  \n",
      "     |  dropTempView(self, viewName: str) -> bool\n",
      "     |      Drops the local temporary view with the given view name in the catalog.\n",
      "     |      If the view has been cached before, then it will also be uncached.\n",
      "     |      Returns true if this view is dropped successfully, false otherwise.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      viewName : str\n",
      "     |          name of the temporary view to drop.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          If the temporary view was successfully dropped or not.\n",
      "     |      \n",
      "     |          .. versionadded:: 2.1.0\n",
      "     |              The return type of this method was ``None`` in Spark 2.0, but changed to ``bool``\n",
      "     |              in Spark 2.1.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.createDataFrame([(1, 1)]).createTempView(\"my_table\")\n",
      "     |      \n",
      "     |      Dropping the temporary view.\n",
      "     |      \n",
      "     |      >>> spark.catalog.dropTempView(\"my_table\")\n",
      "     |      True\n",
      "     |      \n",
      "     |      Throw an exception if the temporary view does not exists.\n",
      "     |      \n",
      "     |      >>> spark.table(\"my_table\")\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      AnalysisException: ...\n",
      "     |  \n",
      "     |  functionExists(self, functionName: str, dbName: Optional[str] = None) -> bool\n",
      "     |      Check if the function with the specified name exists.\n",
      "     |      This can either be a temporary function or a function.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      functionName : str\n",
      "     |          name of the function to check existence\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |             Allow ``functionName`` to be qualified with catalog name\n",
      "     |      \n",
      "     |      dbName : str, optional\n",
      "     |          name of the database to check function existence in.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          Indicating whether the function exists\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If no database is specified, the current database and catalog\n",
      "     |      are used. This API includes all temporary functions.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog.functionExists(\"count\")\n",
      "     |      True\n",
      "     |      \n",
      "     |      Using the fully qualified name for function name.\n",
      "     |      \n",
      "     |      >>> spark.catalog.functionExists(\"default.unexisting_function\")\n",
      "     |      False\n",
      "     |      >>> spark.catalog.functionExists(\"spark_catalog.default.unexisting_function\")\n",
      "     |      False\n",
      "     |  \n",
      "     |  getDatabase(self, dbName: str) -> pyspark.sql.catalog.Database\n",
      "     |      Get the database with the specified name.\n",
      "     |      This throws an :class:`AnalysisException` when the database cannot be found.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dbName : str\n",
      "     |           name of the database to get.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Database`\n",
      "     |          The database found by the name.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog.getDatabase(\"default\")\n",
      "     |      Database(name='default', catalog='spark_catalog', description='default database', ...\n",
      "     |      \n",
      "     |      Using the fully qualified name with the catalog name.\n",
      "     |      \n",
      "     |      >>> spark.catalog.getDatabase(\"spark_catalog.default\")\n",
      "     |      Database(name='default', catalog='spark_catalog', description='default database', ...\n",
      "     |  \n",
      "     |  getFunction(self, functionName: str) -> pyspark.sql.catalog.Function\n",
      "     |      Get the function with the specified name. This function can be a temporary function or a\n",
      "     |      function. This throws an :class:`AnalysisException` when the function cannot be found.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      functionName : str\n",
      "     |          name of the function to check existence.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Function`\n",
      "     |          The function found by the name.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = spark.sql(\n",
      "     |      ...     \"CREATE FUNCTION my_func1 AS 'test.org.apache.spark.sql.MyDoubleAvg'\")\n",
      "     |      >>> spark.catalog.getFunction(\"my_func1\")\n",
      "     |      Function(name='my_func1', catalog='spark_catalog', namespace=['default'], ...\n",
      "     |      \n",
      "     |      Using the fully qualified name for function name.\n",
      "     |      \n",
      "     |      >>> spark.catalog.getFunction(\"default.my_func1\")\n",
      "     |      Function(name='my_func1', catalog='spark_catalog', namespace=['default'], ...\n",
      "     |      >>> spark.catalog.getFunction(\"spark_catalog.default.my_func1\")\n",
      "     |      Function(name='my_func1', catalog='spark_catalog', namespace=['default'], ...\n",
      "     |      \n",
      "     |      Throw an analysis exception when the function does not exists.\n",
      "     |      \n",
      "     |      >>> spark.catalog.getFunction(\"my_func2\")\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      AnalysisException: ...\n",
      "     |  \n",
      "     |  getTable(self, tableName: str) -> pyspark.sql.catalog.Table\n",
      "     |      Get the table or view with the specified name. This table can be a temporary view or a\n",
      "     |      table/view. This throws an :class:`AnalysisException` when no Table can be found.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          name of the table to get.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |             Allow `tableName` to be qualified with catalog name.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Table`\n",
      "     |          The table found by the name.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      >>> _ = spark.sql(\"CREATE TABLE tbl1 (name STRING, age INT) USING parquet\")\n",
      "     |      >>> spark.catalog.getTable(\"tbl1\")\n",
      "     |      Table(name='tbl1', catalog='spark_catalog', namespace=['default'], ...\n",
      "     |      \n",
      "     |      Using the fully qualified name with the catalog name.\n",
      "     |      \n",
      "     |      >>> spark.catalog.getTable(\"default.tbl1\")\n",
      "     |      Table(name='tbl1', catalog='spark_catalog', namespace=['default'], ...\n",
      "     |      >>> spark.catalog.getTable(\"spark_catalog.default.tbl1\")\n",
      "     |      Table(name='tbl1', catalog='spark_catalog', namespace=['default'], ...\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |      \n",
      "     |      Throw an analysis exception when the table does not exist.\n",
      "     |      \n",
      "     |      >>> spark.catalog.getTable(\"tbl1\")\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      AnalysisException: ...\n",
      "     |  \n",
      "     |  isCached(self, tableName: str) -> bool\n",
      "     |      Returns true if the table is currently cached in-memory.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          name of the table to get.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |              Allow ``tableName`` to be qualified with catalog name.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      >>> _ = spark.sql(\"CREATE TABLE tbl1 (name STRING, age INT) USING parquet\")\n",
      "     |      >>> spark.catalog.cacheTable(\"tbl1\")\n",
      "     |      >>> spark.catalog.isCached(\"tbl1\")\n",
      "     |      True\n",
      "     |      \n",
      "     |      Throw an analysis exception when the table does not exist.\n",
      "     |      \n",
      "     |      >>> spark.catalog.isCached(\"not_existing_table\")\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      AnalysisException: ...\n",
      "     |      \n",
      "     |      Using the fully qualified name for the table.\n",
      "     |      \n",
      "     |      >>> spark.catalog.isCached(\"spark_catalog.default.tbl1\")\n",
      "     |      True\n",
      "     |      >>> spark.catalog.uncacheTable(\"tbl1\")\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |  \n",
      "     |  listCatalogs(self, pattern: Optional[str] = None) -> List[pyspark.sql.catalog.CatalogMetadata]\n",
      "     |      Returns a list of catalogs in this session.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      pattern : str\n",
      "     |          The pattern that the catalog name needs to match.\n",
      "     |      \n",
      "     |          .. versionchanged: 3.5.0\n",
      "     |              Added ``pattern`` argument.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          A list of :class:`CatalogMetadata`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog.listCatalogs()\n",
      "     |      [CatalogMetadata(name='spark_catalog', description=None)]\n",
      "     |      \n",
      "     |      >>> spark.catalog.listCatalogs(\"spark*\")\n",
      "     |      [CatalogMetadata(name='spark_catalog', description=None)]\n",
      "     |      \n",
      "     |      >>> spark.catalog.listCatalogs(\"hive*\")\n",
      "     |      []\n",
      "     |  \n",
      "     |  listColumns(self, tableName: str, dbName: Optional[str] = None) -> List[pyspark.sql.catalog.Column]\n",
      "     |      Returns a list of columns for the given table/view in the specified database.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          name of the table to list columns.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |             Allow ``tableName`` to be qualified with catalog name when ``dbName`` is None.\n",
      "     |      \n",
      "     |      dbName : str, optional\n",
      "     |          name of the database to find the table to list columns.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          A list of :class:`Column`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The order of arguments here is different from that of its JVM counterpart\n",
      "     |      because Python does not support method overloading.\n",
      "     |      \n",
      "     |      If no database is specified, the current database and catalog\n",
      "     |      are used. This API includes all temporary views.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      >>> _ = spark.sql(\"CREATE TABLE tblA (name STRING, age INT) USING parquet\")\n",
      "     |      >>> spark.catalog.listColumns(\"tblA\")\n",
      "     |      [Column(name='name', description=None, dataType='string', nullable=True, ...\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n",
      "     |  \n",
      "     |  listDatabases(self, pattern: Optional[str] = None) -> List[pyspark.sql.catalog.Database]\n",
      "     |      Returns a list of databases available across all sessions.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      pattern : str\n",
      "     |          The pattern that the database name needs to match.\n",
      "     |      \n",
      "     |          .. versionchanged: 3.5.0\n",
      "     |              Adds ``pattern`` argument.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          A list of :class:`Database`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog.listDatabases()\n",
      "     |      [Database(name='default', catalog='spark_catalog', description='default database', ...\n",
      "     |      \n",
      "     |      >>> spark.catalog.listDatabases(\"def*\")\n",
      "     |      [Database(name='default', catalog='spark_catalog', description='default database', ...\n",
      "     |      \n",
      "     |      >>> spark.catalog.listDatabases(\"def2*\")\n",
      "     |      []\n",
      "     |  \n",
      "     |  listFunctions(self, dbName: Optional[str] = None, pattern: Optional[str] = None) -> List[pyspark.sql.catalog.Function]\n",
      "     |      Returns a list of functions registered in the specified database.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dbName : str\n",
      "     |          name of the database to list the functions.\n",
      "     |          ``dbName`` can be qualified with catalog name.\n",
      "     |      pattern : str\n",
      "     |          The pattern that the function name needs to match.\n",
      "     |      \n",
      "     |          .. versionchanged: 3.5.0\n",
      "     |              Adds ``pattern`` argument.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          A list of :class:`Function`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If no database is specified, the current database and catalog\n",
      "     |      are used. This API includes all temporary functions.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog.listFunctions()\n",
      "     |      [Function(name=...\n",
      "     |      \n",
      "     |      >>> spark.catalog.listFunctions(pattern=\"to_*\")\n",
      "     |      [Function(name=...\n",
      "     |      \n",
      "     |      >>> spark.catalog.listFunctions(pattern=\"*not_existing_func*\")\n",
      "     |      []\n",
      "     |  \n",
      "     |  listTables(self, dbName: Optional[str] = None, pattern: Optional[str] = None) -> List[pyspark.sql.catalog.Table]\n",
      "     |      Returns a list of tables/views in the specified database.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dbName : str\n",
      "     |          name of the database to list the tables.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |             Allow ``dbName`` to be qualified with catalog name.\n",
      "     |      \n",
      "     |      pattern : str\n",
      "     |          The pattern that the database name needs to match.\n",
      "     |      \n",
      "     |          .. versionchanged: 3.5.0\n",
      "     |              Adds ``pattern`` argument.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          A list of :class:`Table`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If no database is specified, the current database and catalog\n",
      "     |      are used. This API includes all temporary views.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.range(1).createTempView(\"test_view\")\n",
      "     |      >>> spark.catalog.listTables()\n",
      "     |      [Table(name='test_view', catalog=None, namespace=[], description=None, ...\n",
      "     |      \n",
      "     |      >>> spark.catalog.listTables(pattern=\"test*\")\n",
      "     |      [Table(name='test_view', catalog=None, namespace=[], description=None, ...\n",
      "     |      \n",
      "     |      >>> spark.catalog.listTables(pattern=\"table*\")\n",
      "     |      []\n",
      "     |      \n",
      "     |      >>> _ = spark.catalog.dropTempView(\"test_view\")\n",
      "     |      >>> spark.catalog.listTables()\n",
      "     |      []\n",
      "     |  \n",
      "     |  recoverPartitions(self, tableName: str) -> None\n",
      "     |      Recovers all the partitions of the given table and updates the catalog.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.1\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          name of the table to get.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Only works with a partitioned table, and not a view.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      The example below creates a partitioned table against the existing directory of\n",
      "     |      the partitioned table. After that, it recovers the partitions.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      ...     spark.range(1).selectExpr(\n",
      "     |      ...         \"id as key\", \"id as value\").write.partitionBy(\"key\").mode(\"overwrite\").save(d)\n",
      "     |      ...     _ = spark.sql(\n",
      "     |      ...          \"CREATE TABLE tbl1 (key LONG, value LONG)\"\n",
      "     |      ...          \"USING parquet OPTIONS (path '{}') PARTITIONED BY (key)\".format(d))\n",
      "     |      ...     spark.table(\"tbl1\").show()\n",
      "     |      ...     spark.catalog.recoverPartitions(\"tbl1\")\n",
      "     |      ...     spark.table(\"tbl1\").show()\n",
      "     |      +-----+---+\n",
      "     |      |value|key|\n",
      "     |      +-----+---+\n",
      "     |      +-----+---+\n",
      "     |      +-----+---+\n",
      "     |      |value|key|\n",
      "     |      +-----+---+\n",
      "     |      |    0|  0|\n",
      "     |      +-----+---+\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |  \n",
      "     |  refreshByPath(self, path: str) -> None\n",
      "     |      Invalidates and refreshes all the cached data (and the associated metadata) for any\n",
      "     |      DataFrame that contains the given data source path.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.2.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          the path to refresh the cache.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      The example below caches a table, and then removes the data.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      ...     _ = spark.sql(\n",
      "     |      ...         \"CREATE TABLE tbl1 (col STRING) USING TEXT LOCATION '{}'\".format(d))\n",
      "     |      ...     _ = spark.sql(\"INSERT INTO tbl1 SELECT 'abc'\")\n",
      "     |      ...     spark.catalog.cacheTable(\"tbl1\")\n",
      "     |      ...     spark.table(\"tbl1\").show()\n",
      "     |      +---+\n",
      "     |      |col|\n",
      "     |      +---+\n",
      "     |      |abc|\n",
      "     |      +---+\n",
      "     |      \n",
      "     |      Because the table is cached, it computes from the cached data as below.\n",
      "     |      \n",
      "     |      >>> spark.table(\"tbl1\").count()\n",
      "     |      1\n",
      "     |      \n",
      "     |      After refreshing the table by path, it shows 0 because the data does not exist anymore.\n",
      "     |      \n",
      "     |      >>> spark.catalog.refreshByPath(d)\n",
      "     |      >>> spark.table(\"tbl1\").count()\n",
      "     |      0\n",
      "     |      \n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |  \n",
      "     |  refreshTable(self, tableName: str) -> None\n",
      "     |      Invalidates and refreshes all the cached data and metadata of the given table.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          name of the table to get.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |              Allow ``tableName`` to be qualified with catalog name.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      The example below caches a table, and then removes the data.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      ...     _ = spark.sql(\n",
      "     |      ...         \"CREATE TABLE tbl1 (col STRING) USING TEXT LOCATION '{}'\".format(d))\n",
      "     |      ...     _ = spark.sql(\"INSERT INTO tbl1 SELECT 'abc'\")\n",
      "     |      ...     spark.catalog.cacheTable(\"tbl1\")\n",
      "     |      ...     spark.table(\"tbl1\").show()\n",
      "     |      +---+\n",
      "     |      |col|\n",
      "     |      +---+\n",
      "     |      |abc|\n",
      "     |      +---+\n",
      "     |      \n",
      "     |      Because the table is cached, it computes from the cached data as below.\n",
      "     |      \n",
      "     |      >>> spark.table(\"tbl1\").count()\n",
      "     |      1\n",
      "     |      \n",
      "     |      After refreshing the table, it shows 0 because the data does not exist anymore.\n",
      "     |      \n",
      "     |      >>> spark.catalog.refreshTable(\"tbl1\")\n",
      "     |      >>> spark.table(\"tbl1\").count()\n",
      "     |      0\n",
      "     |      \n",
      "     |      Using the fully qualified name for the table.\n",
      "     |      \n",
      "     |      >>> spark.catalog.refreshTable(\"spark_catalog.default.tbl1\")\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |  \n",
      "     |  registerFunction(self, name: str, f: Callable[..., Any], returnType: Optional[ForwardRef('DataType')] = None) -> 'UserDefinedFunctionLike'\n",
      "     |      An alias for :func:`spark.udf.register`.\n",
      "     |      See :meth:`pyspark.sql.UDFRegistration.register`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. deprecated:: 2.3.0\n",
      "     |          Use :func:`spark.udf.register` instead.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |  \n",
      "     |  setCurrentCatalog(self, catalogName: str) -> None\n",
      "     |      Sets the current default catalog in this session.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      catalogName : str\n",
      "     |          name of the catalog to set\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog.setCurrentCatalog(\"spark_catalog\")\n",
      "     |  \n",
      "     |  setCurrentDatabase(self, dbName: str) -> None\n",
      "     |      Sets the current default database in this session.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog.setCurrentDatabase(\"default\")\n",
      "     |  \n",
      "     |  tableExists(self, tableName: str, dbName: Optional[str] = None) -> bool\n",
      "     |      Check if the table or view with the specified name exists.\n",
      "     |      This can either be a temporary view or a table/view.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          name of the table to check existence.\n",
      "     |          If no database is specified, first try to treat ``tableName`` as a\n",
      "     |          multi-layer-namespace identifier, then try ``tableName`` as a normal table\n",
      "     |          name in the current database if necessary.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |             Allow ``tableName`` to be qualified with catalog name when ``dbName`` is None.\n",
      "     |      \n",
      "     |      dbName : str, optional\n",
      "     |          name of the database to check table existence in.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          Indicating whether the table/view exists\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      This function can check if a table is defined or not:\n",
      "     |      \n",
      "     |      >>> spark.catalog.tableExists(\"unexisting_table\")\n",
      "     |      False\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      >>> _ = spark.sql(\"CREATE TABLE tbl1 (name STRING, age INT) USING parquet\")\n",
      "     |      >>> spark.catalog.tableExists(\"tbl1\")\n",
      "     |      True\n",
      "     |      \n",
      "     |      Using the fully qualified names for tables.\n",
      "     |      \n",
      "     |      >>> spark.catalog.tableExists(\"default.tbl1\")\n",
      "     |      True\n",
      "     |      >>> spark.catalog.tableExists(\"spark_catalog.default.tbl1\")\n",
      "     |      True\n",
      "     |      >>> spark.catalog.tableExists(\"tbl1\", \"default\")\n",
      "     |      True\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |      \n",
      "     |      Check if views exist:\n",
      "     |      \n",
      "     |      >>> spark.catalog.tableExists(\"view1\")\n",
      "     |      False\n",
      "     |      >>> _ = spark.sql(\"CREATE VIEW view1 AS SELECT 1\")\n",
      "     |      >>> spark.catalog.tableExists(\"view1\")\n",
      "     |      True\n",
      "     |      \n",
      "     |      Using the fully qualified names for views.\n",
      "     |      \n",
      "     |      >>> spark.catalog.tableExists(\"default.view1\")\n",
      "     |      True\n",
      "     |      >>> spark.catalog.tableExists(\"spark_catalog.default.view1\")\n",
      "     |      True\n",
      "     |      >>> spark.catalog.tableExists(\"view1\", \"default\")\n",
      "     |      True\n",
      "     |      >>> _ = spark.sql(\"DROP VIEW view1\")\n",
      "     |      \n",
      "     |      Check if temporary views exist:\n",
      "     |      \n",
      "     |      >>> _ = spark.sql(\"CREATE TEMPORARY VIEW view1 AS SELECT 1\")\n",
      "     |      >>> spark.catalog.tableExists(\"view1\")\n",
      "     |      True\n",
      "     |      >>> df = spark.sql(\"DROP VIEW view1\")\n",
      "     |      >>> spark.catalog.tableExists(\"view1\")\n",
      "     |      False\n",
      "     |  \n",
      "     |  uncacheTable(self, tableName: str) -> None\n",
      "     |      Removes the specified table from the in-memory cache.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          name of the table to get.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |              Allow ``tableName`` to be qualified with catalog name.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n",
      "     |      >>> _ = spark.sql(\"CREATE TABLE tbl1 (name STRING, age INT) USING parquet\")\n",
      "     |      >>> spark.catalog.cacheTable(\"tbl1\")\n",
      "     |      >>> spark.catalog.uncacheTable(\"tbl1\")\n",
      "     |      >>> spark.catalog.isCached(\"tbl1\")\n",
      "     |      False\n",
      "     |      \n",
      "     |      Throw an analysis exception when the table does not exist.\n",
      "     |      \n",
      "     |      >>> spark.catalog.uncacheTable(\"not_existing_table\")\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      AnalysisException: ...\n",
      "     |      \n",
      "     |      Using the fully qualified name for the table.\n",
      "     |      \n",
      "     |      >>> spark.catalog.uncacheTable(\"spark_catalog.default.tbl1\")\n",
      "     |      >>> spark.catalog.isCached(\"tbl1\")\n",
      "     |      False\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tbl1\")\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Column(builtins.object)\n",
      "     |  Column(jc: py4j.java_gateway.JavaObject) -> None\n",
      "     |  \n",
      "     |  A column in a DataFrame.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.3.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Column instances can be created by\n",
      "     |  \n",
      "     |  >>> df = spark.createDataFrame(\n",
      "     |  ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |  \n",
      "     |  Select a column out of a DataFrame\n",
      "     |  >>> df.name\n",
      "     |  Column<'name'>\n",
      "     |  >>> df[\"name\"]\n",
      "     |  Column<'name'>\n",
      "     |  \n",
      "     |  Create from an expression\n",
      "     |  \n",
      "     |  >>> df.age + 1\n",
      "     |  Column<...>\n",
      "     |  >>> 1 / df.age\n",
      "     |  Column<...>\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __add__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __and__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __bool__ = __nonzero__(self) -> None\n",
      "     |  \n",
      "     |  __contains__(self, item: Any) -> None\n",
      "     |      # container operators\n",
      "     |  \n",
      "     |  __div__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __eq__(self, other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary function\n",
      "     |  \n",
      "     |  __ge__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __getattr__(self, item: Any) -> 'Column'\n",
      "     |      An expression that gets an item at position ``ordinal`` out of a list,\n",
      "     |      or gets an item by key out of a dict.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      item\n",
      "     |          a literal value.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing the item got by key out of a dict.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([('abcedfg', {\"key\": \"value\"})], [\"l\", \"d\"])\n",
      "     |      >>> df.select(df.d.key).show()\n",
      "     |      +------+\n",
      "     |      |d[key]|\n",
      "     |      +------+\n",
      "     |      | value|\n",
      "     |      +------+\n",
      "     |  \n",
      "     |  __getitem__(self, k: Any) -> 'Column'\n",
      "     |      An expression that gets an item at position ``ordinal`` out of a list,\n",
      "     |      or gets an item by key out of a dict.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      k\n",
      "     |          a literal value, or a slice object without step.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing the item got by key out of a dict, or substrings sliced by\n",
      "     |          the given slice object.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([('abcedfg', {\"key\": \"value\"})], [\"l\", \"d\"])\n",
      "     |      >>> df.select(df.l[slice(1, 3)], df.d['key']).show()\n",
      "     |      +------------------+------+\n",
      "     |      |substring(l, 1, 3)|d[key]|\n",
      "     |      +------------------+------+\n",
      "     |      |               abc| value|\n",
      "     |      +------------------+------+\n",
      "     |  \n",
      "     |  __gt__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __init__(self, jc: py4j.java_gateway.JavaObject) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __invert__ = _(self: 'Column') -> 'Column'\n",
      "     |  \n",
      "     |  __iter__(self) -> None\n",
      "     |  \n",
      "     |  __le__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __lt__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __mod__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __mul__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __ne__(self, other: Any) -> 'Column'\n",
      "     |      binary function\n",
      "     |  \n",
      "     |  __neg__ = _(self: 'Column') -> 'Column'\n",
      "     |  \n",
      "     |  __nonzero__(self) -> None\n",
      "     |  \n",
      "     |  __or__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __pow__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      "     |      binary function\n",
      "     |  \n",
      "     |  __radd__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __rand__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __rdiv__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __rmod__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __rmul__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __ror__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __rpow__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      "     |      binary function\n",
      "     |  \n",
      "     |  __rsub__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __rtruediv__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __sub__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  __truediv__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      binary operator\n",
      "     |  \n",
      "     |  alias(self, *alias: str, **kwargs: Any) -> 'Column'\n",
      "     |      Returns this column aliased with a new name or names (in the case of expressions that\n",
      "     |      return more than one column, such as explode).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      alias : str\n",
      "     |          desired column names (collects all positional arguments passed)\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      metadata: dict\n",
      "     |          a dict of information to be stored in ``metadata`` attribute of the\n",
      "     |          corresponding :class:`StructField <pyspark.sql.types.StructField>` (optional, keyword\n",
      "     |          only argument)\n",
      "     |      \n",
      "     |          .. versionchanged:: 2.2.0\n",
      "     |             Added optional ``metadata`` argument.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing whether each element of Column is aliased with new name or names.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.select(df.age.alias(\"age2\")).collect()\n",
      "     |      [Row(age2=2), Row(age2=5)]\n",
      "     |      >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n",
      "     |      99\n",
      "     |  \n",
      "     |  asc = _(self: 'Column') -> 'Column'\n",
      "     |      Returns a sort expression based on the ascending order of the column.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
      "     |      >>> df.select(df.name).orderBy(df.name.asc()).collect()\n",
      "     |      [Row(name='Alice'), Row(name='Tom')]\n",
      "     |  \n",
      "     |  asc_nulls_first = _(self: 'Column') -> 'Column'\n",
      "     |      Returns a sort expression based on ascending order of the column, and null values\n",
      "     |      return before non-null values.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      "     |      >>> df.select(df.name).orderBy(df.name.asc_nulls_first()).collect()\n",
      "     |      [Row(name=None), Row(name='Alice'), Row(name='Tom')]\n",
      "     |  \n",
      "     |  asc_nulls_last = _(self: 'Column') -> 'Column'\n",
      "     |      Returns a sort expression based on ascending order of the column, and null values\n",
      "     |      appear after non-null values.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      "     |      >>> df.select(df.name).orderBy(df.name.asc_nulls_last()).collect()\n",
      "     |      [Row(name='Alice'), Row(name='Tom'), Row(name=None)]\n",
      "     |  \n",
      "     |  astype = cast(self, dataType)\n",
      "     |      :func:`astype` is an alias for :func:`cast`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  between(self, lowerBound: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DateTimeLiteral'), ForwardRef('DecimalLiteral')], upperBound: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DateTimeLiteral'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      "     |      True if the current column is between the lower bound and upper bound, inclusive.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      lowerBound : :class:`Column`, int, float, string, bool, datetime, date or Decimal\n",
      "     |          a boolean expression that boundary start, inclusive.\n",
      "     |      upperBound : :class:`Column`, int, float, string, bool, datetime, date or Decimal\n",
      "     |          a boolean expression that boundary end, inclusive.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column of booleans showing whether each element of Column\n",
      "     |          is between left and right (inclusive).\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.select(df.name, df.age.between(2, 4)).show()\n",
      "     |      +-----+---------------------------+\n",
      "     |      | name|((age >= 2) AND (age <= 4))|\n",
      "     |      +-----+---------------------------+\n",
      "     |      |Alice|                       true|\n",
      "     |      |  Bob|                      false|\n",
      "     |      +-----+---------------------------+\n",
      "     |  \n",
      "     |  bitwiseAND = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      Compute bitwise AND of this expression with another expression.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other\n",
      "     |          a value or :class:`Column` to calculate bitwise and(&) with\n",
      "     |          this :class:`Column`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      "     |      >>> df.select(df.a.bitwiseAND(df.b)).collect()\n",
      "     |      [Row((a & b)=10)]\n",
      "     |  \n",
      "     |  bitwiseOR = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      Compute bitwise OR of this expression with another expression.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other\n",
      "     |          a value or :class:`Column` to calculate bitwise or(|) with\n",
      "     |          this :class:`Column`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      "     |      >>> df.select(df.a.bitwiseOR(df.b)).collect()\n",
      "     |      [Row((a | b)=235)]\n",
      "     |  \n",
      "     |  bitwiseXOR = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      Compute bitwise XOR of this expression with another expression.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other\n",
      "     |          a value or :class:`Column` to calculate bitwise xor(^) with\n",
      "     |          this :class:`Column`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      "     |      >>> df.select(df.a.bitwiseXOR(df.b)).collect()\n",
      "     |      [Row((a ^ b)=225)]\n",
      "     |  \n",
      "     |  cast(self, dataType: Union[pyspark.sql.types.DataType, str]) -> 'Column'\n",
      "     |      Casts the column into type ``dataType``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dataType : :class:`DataType` or str\n",
      "     |          a DataType or Python string literal with a DDL-formatted string\n",
      "     |          to use when parsing the column to the same type.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing whether each element of Column is cast into new type.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.types import StringType\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n",
      "     |      [Row(ages='2'), Row(ages='5')]\n",
      "     |      >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n",
      "     |      [Row(ages='2'), Row(ages='5')]\n",
      "     |  \n",
      "     |  contains = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      Contains the other element. Returns a boolean :class:`Column` based on a string match.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other\n",
      "     |          string in line. A value as a literal or a :class:`Column`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.filter(df.name.contains('o')).collect()\n",
      "     |      [Row(age=5, name='Bob')]\n",
      "     |  \n",
      "     |  desc = _(self: 'Column') -> 'Column'\n",
      "     |      Returns a sort expression based on the descending order of the column.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
      "     |      >>> df.select(df.name).orderBy(df.name.desc()).collect()\n",
      "     |      [Row(name='Tom'), Row(name='Alice')]\n",
      "     |  \n",
      "     |  desc_nulls_first = _(self: 'Column') -> 'Column'\n",
      "     |      Returns a sort expression based on the descending order of the column, and null values\n",
      "     |      appear before non-null values.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      "     |      >>> df.select(df.name).orderBy(df.name.desc_nulls_first()).collect()\n",
      "     |      [Row(name=None), Row(name='Tom'), Row(name='Alice')]\n",
      "     |  \n",
      "     |  desc_nulls_last = _(self: 'Column') -> 'Column'\n",
      "     |      Returns a sort expression based on the descending order of the column, and null values\n",
      "     |      appear after non-null values.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      "     |      >>> df.select(df.name).orderBy(df.name.desc_nulls_last()).collect()\n",
      "     |      [Row(name='Tom'), Row(name='Alice'), Row(name=None)]\n",
      "     |  \n",
      "     |  dropFields(self, *fieldNames: str) -> 'Column'\n",
      "     |      An expression that drops fields in :class:`StructType` by name.\n",
      "     |      This is a no-op if the schema doesn't contain field name(s).\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fieldNames : str\n",
      "     |          Desired field names (collects all positional arguments passed)\n",
      "     |          The result will drop at a location if any field matches in the Column.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing whether each element of Column with field dropped by fieldName.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> from pyspark.sql.functions import col, lit\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n",
      "     |      >>> df.withColumn('a', df['a'].dropFields('b')).show()\n",
      "     |      +-----------------+\n",
      "     |      |                a|\n",
      "     |      +-----------------+\n",
      "     |      |{2, 3, {4, 5, 6}}|\n",
      "     |      +-----------------+\n",
      "     |      \n",
      "     |      >>> df.withColumn('a', df['a'].dropFields('b', 'c')).show()\n",
      "     |      +--------------+\n",
      "     |      |             a|\n",
      "     |      +--------------+\n",
      "     |      |{3, {4, 5, 6}}|\n",
      "     |      +--------------+\n",
      "     |      \n",
      "     |      This method supports dropping multiple nested fields directly e.g.\n",
      "     |      \n",
      "     |      >>> df.withColumn(\"a\", col(\"a\").dropFields(\"e.g\", \"e.h\")).show()\n",
      "     |      +--------------+\n",
      "     |      |             a|\n",
      "     |      +--------------+\n",
      "     |      |{1, 2, 3, {4}}|\n",
      "     |      +--------------+\n",
      "     |      \n",
      "     |      However, if you are going to add/replace multiple nested fields,\n",
      "     |      it is preferred to extract out the nested struct before\n",
      "     |      adding/replacing multiple fields e.g.\n",
      "     |      \n",
      "     |      >>> df.select(col(\"a\").withField(\n",
      "     |      ...     \"e\", col(\"a.e\").dropFields(\"g\", \"h\")).alias(\"a\")\n",
      "     |      ... ).show()\n",
      "     |      +--------------+\n",
      "     |      |             a|\n",
      "     |      +--------------+\n",
      "     |      |{1, 2, 3, {4}}|\n",
      "     |      +--------------+\n",
      "     |  \n",
      "     |  endswith = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      String ends with. Returns a boolean :class:`Column` based on a string match.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`Column` or str\n",
      "     |          string at end of line (do not use a regex `$`)\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.filter(df.name.endswith('ice')).collect()\n",
      "     |      [Row(age=2, name='Alice')]\n",
      "     |      >>> df.filter(df.name.endswith('ice$')).collect()\n",
      "     |      []\n",
      "     |  \n",
      "     |  eqNullSafe = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      Equality test that is safe for null values.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other\n",
      "     |          a value or :class:`Column`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df1 = spark.createDataFrame([\n",
      "     |      ...     Row(id=1, value='foo'),\n",
      "     |      ...     Row(id=2, value=None)\n",
      "     |      ... ])\n",
      "     |      >>> df1.select(\n",
      "     |      ...     df1['value'] == 'foo',\n",
      "     |      ...     df1['value'].eqNullSafe('foo'),\n",
      "     |      ...     df1['value'].eqNullSafe(None)\n",
      "     |      ... ).show()\n",
      "     |      +-------------+---------------+----------------+\n",
      "     |      |(value = foo)|(value <=> foo)|(value <=> NULL)|\n",
      "     |      +-------------+---------------+----------------+\n",
      "     |      |         true|           true|           false|\n",
      "     |      |         NULL|          false|            true|\n",
      "     |      +-------------+---------------+----------------+\n",
      "     |      >>> df2 = spark.createDataFrame([\n",
      "     |      ...     Row(value = 'bar'),\n",
      "     |      ...     Row(value = None)\n",
      "     |      ... ])\n",
      "     |      >>> df1.join(df2, df1[\"value\"] == df2[\"value\"]).count()\n",
      "     |      0\n",
      "     |      >>> df1.join(df2, df1[\"value\"].eqNullSafe(df2[\"value\"])).count()\n",
      "     |      1\n",
      "     |      >>> df2 = spark.createDataFrame([\n",
      "     |      ...     Row(id=1, value=float('NaN')),\n",
      "     |      ...     Row(id=2, value=42.0),\n",
      "     |      ...     Row(id=3, value=None)\n",
      "     |      ... ])\n",
      "     |      >>> df2.select(\n",
      "     |      ...     df2['value'].eqNullSafe(None),\n",
      "     |      ...     df2['value'].eqNullSafe(float('NaN')),\n",
      "     |      ...     df2['value'].eqNullSafe(42.0)\n",
      "     |      ... ).show()\n",
      "     |      +----------------+---------------+----------------+\n",
      "     |      |(value <=> NULL)|(value <=> NaN)|(value <=> 42.0)|\n",
      "     |      +----------------+---------------+----------------+\n",
      "     |      |           false|           true|           false|\n",
      "     |      |           false|          false|            true|\n",
      "     |      |            true|          false|           false|\n",
      "     |      +----------------+---------------+----------------+\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Unlike Pandas, PySpark doesn't consider NaN values to be NULL. See the\n",
      "     |      `NaN Semantics <https://spark.apache.org/docs/latest/sql-ref-datatypes.html#nan-semantics>`_\n",
      "     |      for details.\n",
      "     |  \n",
      "     |  getField(self, name: Any) -> 'Column'\n",
      "     |      An expression that gets a field by name in a :class:`StructType`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name\n",
      "     |          a literal value, or a :class:`Column` expression.\n",
      "     |          The result will only be true at a location if the field matches in the Column.\n",
      "     |      \n",
      "     |           .. deprecated:: 3.0.0\n",
      "     |               :class:`Column` as a parameter is deprecated.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing whether each element of Column got by name.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n",
      "     |      >>> df.select(df.r.getField(\"b\")).show()\n",
      "     |      +---+\n",
      "     |      |r.b|\n",
      "     |      +---+\n",
      "     |      |  b|\n",
      "     |      +---+\n",
      "     |      >>> df.select(df.r.a).show()\n",
      "     |      +---+\n",
      "     |      |r.a|\n",
      "     |      +---+\n",
      "     |      |  1|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  getItem(self, key: Any) -> 'Column'\n",
      "     |      An expression that gets an item at position ``ordinal`` out of a list,\n",
      "     |      or gets an item by key out of a dict.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key\n",
      "     |          a literal value, or a :class:`Column` expression.\n",
      "     |          The result will only be true at a location if the item matches in the column.\n",
      "     |      \n",
      "     |           .. deprecated:: 3.0.0\n",
      "     |               :class:`Column` as a parameter is deprecated.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing the item(s) got at position out of a list or by key out of a dict.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n",
      "     |      >>> df.select(df.l.getItem(0), df.d.getItem(\"key\")).show()\n",
      "     |      +----+------+\n",
      "     |      |l[0]|d[key]|\n",
      "     |      +----+------+\n",
      "     |      |   1| value|\n",
      "     |      +----+------+\n",
      "     |  \n",
      "     |  ilike(self: 'Column', other: str) -> 'Column'\n",
      "     |      SQL ILIKE expression (case insensitive LIKE). Returns a boolean :class:`Column`\n",
      "     |      based on a case insensitive match.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : str\n",
      "     |          a SQL LIKE pattern\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.Column.rlike\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column of booleans showing whether each element\n",
      "     |          in the Column is matched by SQL LIKE pattern.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.filter(df.name.ilike('%Ice')).collect()\n",
      "     |      [Row(age=2, name='Alice')]\n",
      "     |  \n",
      "     |  isNotNull = _(self: 'Column') -> 'Column'\n",
      "     |      True if the current expression is NOT null.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      "     |      >>> df.filter(df.height.isNotNull()).collect()\n",
      "     |      [Row(name='Tom', height=80)]\n",
      "     |  \n",
      "     |  isNull = _(self: 'Column') -> 'Column'\n",
      "     |      True if the current expression is null.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      "     |      >>> df.filter(df.height.isNull()).collect()\n",
      "     |      [Row(name='Alice', height=None)]\n",
      "     |  \n",
      "     |  isin(self, *cols: Any) -> 'Column'\n",
      "     |      A boolean expression that is evaluated to true if the value of this\n",
      "     |      expression is contained by the evaluated values of the arguments.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols\n",
      "     |          The result will only be true at a location if any value matches in the Column.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column of booleans showing whether each element in the Column is contained in cols.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n",
      "     |      [Row(age=5, name='Bob')]\n",
      "     |      >>> df[df.age.isin([1, 2, 3])].collect()\n",
      "     |      [Row(age=2, name='Alice')]\n",
      "     |  \n",
      "     |  like(self: 'Column', other: str) -> 'Column'\n",
      "     |      SQL like expression. Returns a boolean :class:`Column` based on a SQL LIKE match.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : str\n",
      "     |          a SQL LIKE pattern\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.Column.rlike\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column of booleans showing whether each element\n",
      "     |          in the Column is matched by SQL LIKE pattern.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.filter(df.name.like('Al%')).collect()\n",
      "     |      [Row(age=2, name='Alice')]\n",
      "     |  \n",
      "     |  name = alias(self, *alias, **kwargs)\n",
      "     |      :func:`name` is an alias for :func:`alias`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0\n",
      "     |  \n",
      "     |  otherwise(self, value: Any) -> 'Column'\n",
      "     |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "     |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      value\n",
      "     |          a literal value, or a :class:`Column` expression.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing whether each element of Column is unmatched conditions.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import functions as sf\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.select(df.name, sf.when(df.age > 3, 1).otherwise(0)).show()\n",
      "     |      +-----+-------------------------------------+\n",
      "     |      | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n",
      "     |      +-----+-------------------------------------+\n",
      "     |      |Alice|                                    0|\n",
      "     |      |  Bob|                                    1|\n",
      "     |      +-----+-------------------------------------+\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.functions.when\n",
      "     |  \n",
      "     |  over(self, window: 'WindowSpec') -> 'Column'\n",
      "     |      Define a windowing column.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      window : :class:`WindowSpec`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Window\n",
      "     |      >>> window = (\n",
      "     |      ...     Window.partitionBy(\"name\")\n",
      "     |      ...     .orderBy(\"age\")\n",
      "     |      ...     .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
      "     |      ... )\n",
      "     |      >>> from pyspark.sql.functions import rank, min, desc\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.withColumn(\n",
      "     |      ...      \"rank\", rank().over(window)\n",
      "     |      ... ).withColumn(\n",
      "     |      ...      \"min\", min('age').over(window)\n",
      "     |      ... ).sort(desc(\"age\")).show()\n",
      "     |      +---+-----+----+---+\n",
      "     |      |age| name|rank|min|\n",
      "     |      +---+-----+----+---+\n",
      "     |      |  5|  Bob|   1|  5|\n",
      "     |      |  2|Alice|   1|  2|\n",
      "     |      +---+-----+----+---+\n",
      "     |  \n",
      "     |  rlike(self: 'Column', other: str) -> 'Column'\n",
      "     |      SQL RLIKE expression (LIKE with Regex). Returns a boolean :class:`Column` based on a regex\n",
      "     |      match.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : str\n",
      "     |          an extended regex expression\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column of booleans showing whether each element\n",
      "     |          in the Column is matched by extended regex expression.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.filter(df.name.rlike('ice$')).collect()\n",
      "     |      [Row(age=2, name='Alice')]\n",
      "     |  \n",
      "     |  startswith = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      "     |      String starts with. Returns a boolean :class:`Column` based on a string match.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`Column` or str\n",
      "     |          string at start of line (do not use a regex `^`)\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.filter(df.name.startswith('Al')).collect()\n",
      "     |      [Row(age=2, name='Alice')]\n",
      "     |      >>> df.filter(df.name.startswith('^Al')).collect()\n",
      "     |      []\n",
      "     |  \n",
      "     |  substr(self, startPos: Union[int, ForwardRef('Column')], length: Union[int, ForwardRef('Column')]) -> 'Column'\n",
      "     |      Return a :class:`Column` which is a substring of the column.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      startPos : :class:`Column` or int\n",
      "     |          start position\n",
      "     |      length : :class:`Column` or int\n",
      "     |          length of the substring\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing whether each element of Column is substr of origin Column.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n",
      "     |      [Row(col='Ali'), Row(col='Bob')]\n",
      "     |  \n",
      "     |  when(self, condition: 'Column', value: Any) -> 'Column'\n",
      "     |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "     |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      condition : :class:`Column`\n",
      "     |          a boolean :class:`Column` expression.\n",
      "     |      value\n",
      "     |          a literal value, or a :class:`Column` expression.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing whether each element of Column is in conditions.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import functions as sf\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.select(df.name, sf.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()\n",
      "     |      +-----+------------------------------------------------------------+\n",
      "     |      | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\n",
      "     |      +-----+------------------------------------------------------------+\n",
      "     |      |Alice|                                                          -1|\n",
      "     |      |  Bob|                                                           1|\n",
      "     |      +-----+------------------------------------------------------------+\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.functions.when\n",
      "     |  \n",
      "     |  withField(self, fieldName: str, col: 'Column') -> 'Column'\n",
      "     |      An expression that adds/replaces a field in :class:`StructType` by name.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fieldName : str\n",
      "     |          a literal value.\n",
      "     |          The result will only be true at a location if any field matches in the Column.\n",
      "     |      col : :class:`Column`\n",
      "     |          A :class:`Column` expression for the column with `fieldName`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Column representing whether each element of Column\n",
      "     |          which field was added/replaced by fieldName.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> from pyspark.sql.functions import lit\n",
      "     |      >>> df = spark.createDataFrame([Row(a=Row(b=1, c=2))])\n",
      "     |      >>> df.withColumn('a', df['a'].withField('b', lit(3))).select('a.b').show()\n",
      "     |      +---+\n",
      "     |      |  b|\n",
      "     |      +---+\n",
      "     |      |  3|\n",
      "     |      +---+\n",
      "     |      >>> df.withColumn('a', df['a'].withField('d', lit(4))).select('a.d').show()\n",
      "     |      +---+\n",
      "     |      |  d|\n",
      "     |      +---+\n",
      "     |      |  4|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
      "     |  DataFrame(jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[ForwardRef('SQLContext'), ForwardRef('SparkSession')])\n",
      "     |  \n",
      "     |  A distributed collection of data grouped into named columns.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.3.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
      "     |  and can be created using various functions in :class:`SparkSession`:\n",
      "     |  \n",
      "     |  >>> people = spark.createDataFrame([\n",
      "     |  ...     {\"deptId\": 1, \"age\": 40, \"name\": \"Hyukjin Kwon\", \"gender\": \"M\", \"salary\": 50},\n",
      "     |  ...     {\"deptId\": 1, \"age\": 50, \"name\": \"Takuya Ueshin\", \"gender\": \"M\", \"salary\": 100},\n",
      "     |  ...     {\"deptId\": 2, \"age\": 60, \"name\": \"Xinrong Meng\", \"gender\": \"F\", \"salary\": 150},\n",
      "     |  ...     {\"deptId\": 3, \"age\": 20, \"name\": \"Haejoon Lee\", \"gender\": \"M\", \"salary\": 200}\n",
      "     |  ... ])\n",
      "     |  \n",
      "     |  Once created, it can be manipulated using the various domain-specific-language\n",
      "     |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
      "     |  \n",
      "     |  To select a column from the :class:`DataFrame`, use the apply method:\n",
      "     |  \n",
      "     |  >>> age_col = people.age\n",
      "     |  \n",
      "     |  A more concrete example:\n",
      "     |  \n",
      "     |  >>> # To create DataFrame using SparkSession\n",
      "     |  ... department = spark.createDataFrame([\n",
      "     |  ...     {\"id\": 1, \"name\": \"PySpark\"},\n",
      "     |  ...     {\"id\": 2, \"name\": \"ML\"},\n",
      "     |  ...     {\"id\": 3, \"name\": \"Spark SQL\"}\n",
      "     |  ... ])\n",
      "     |  \n",
      "     |  >>> people.filter(people.age > 30).join(\n",
      "     |  ...     department, people.deptId == department.id).groupBy(\n",
      "     |  ...     department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"}).show()\n",
      "     |  +-------+------+-----------+--------+\n",
      "     |  |   name|gender|avg(salary)|max(age)|\n",
      "     |  +-------+------+-----------+--------+\n",
      "     |  |     ML|     F|      150.0|      60|\n",
      "     |  |PySpark|     M|       75.0|      50|\n",
      "     |  +-------+------+-----------+--------+\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  A DataFrame should only be created as described above. It should not be directly\n",
      "     |  created via using the constructor.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DataFrame\n",
      "     |      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n",
      "     |      pyspark.sql.pandas.conversion.PandasConversionMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __dir__(self) -> List[str]\n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import lit\n",
      "     |      \n",
      "     |      Create a dataframe with a column named 'id'.\n",
      "     |      \n",
      "     |      >>> df = spark.range(3)\n",
      "     |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Includes column id\n",
      "     |      ['id', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal', 'isStreaming']\n",
      "     |      \n",
      "     |      Add a column named 'i_like_pancakes'.\n",
      "     |      \n",
      "     |      >>> df = df.withColumn('i_like_pancakes', lit(1))\n",
      "     |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Includes columns i_like_pancakes, id\n",
      "     |      ['i_like_pancakes', 'id', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal']\n",
      "     |      \n",
      "     |      Try to add an existed column 'inputFiles'.\n",
      "     |      \n",
      "     |      >>> df = df.withColumn('inputFiles', lit(2))\n",
      "     |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Doesn't duplicate inputFiles\n",
      "     |      ['i_like_pancakes', 'id', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal']\n",
      "     |      \n",
      "     |      Try to add a column named 'id2'.\n",
      "     |      \n",
      "     |      >>> df = df.withColumn('id2', lit(3))\n",
      "     |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # result includes id2 and sorted\n",
      "     |      ['i_like_pancakes', 'id', 'id2', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty']\n",
      "     |      \n",
      "     |      Don't include columns that are not valid python identifiers.\n",
      "     |      \n",
      "     |      >>> df = df.withColumn('1', lit(4))\n",
      "     |      >>> df = df.withColumn('name 1', lit(5))\n",
      "     |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Doesn't include 1 or name 1\n",
      "     |      ['i_like_pancakes', 'id', 'id2', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty']\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> pyspark.sql.column.Column\n",
      "     |      Returns the :class:`Column` denoted by ``name``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          Column name to return as :class:`Column`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |          Requested column.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Retrieve a column instance.\n",
      "     |      \n",
      "     |      >>> df.select(df.age).show()\n",
      "     |      +---+\n",
      "     |      |age|\n",
      "     |      +---+\n",
      "     |      |  2|\n",
      "     |      |  5|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  __getitem__(self, item: Union[int, str, pyspark.sql.column.Column, List, Tuple]) -> Union[pyspark.sql.column.Column, ForwardRef('DataFrame')]\n",
      "     |      Returns the column as a :class:`Column`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      item : int, str, :class:`Column`, list or tuple\n",
      "     |          column index, column name, column, or a list or tuple of columns\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column` or :class:`DataFrame`\n",
      "     |          a specified column, or a filtered or projected dataframe.\n",
      "     |      \n",
      "     |          * If the input `item` is an int or str, the output is a :class:`Column`.\n",
      "     |      \n",
      "     |          * If the input `item` is a :class:`Column`, the output is a :class:`DataFrame`\n",
      "     |              filtered by this given :class:`Column`.\n",
      "     |      \n",
      "     |          * If the input `item` is a list or tuple, the output is a :class:`DataFrame`\n",
      "     |              projected by this given list or tuple.\n",
      "     |      \n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Retrieve a column instance.\n",
      "     |      \n",
      "     |      >>> df.select(df['age']).show()\n",
      "     |      +---+\n",
      "     |      |age|\n",
      "     |      +---+\n",
      "     |      |  2|\n",
      "     |      |  5|\n",
      "     |      +---+\n",
      "     |      \n",
      "     |      >>> df.select(df[1]).show()\n",
      "     |      +-----+\n",
      "     |      | name|\n",
      "     |      +-----+\n",
      "     |      |Alice|\n",
      "     |      |  Bob|\n",
      "     |      +-----+\n",
      "     |      \n",
      "     |      Select multiple string columns as index.\n",
      "     |      \n",
      "     |      >>> df[[\"name\", \"age\"]].show()\n",
      "     |      +-----+---+\n",
      "     |      | name|age|\n",
      "     |      +-----+---+\n",
      "     |      |Alice|  2|\n",
      "     |      |  Bob|  5|\n",
      "     |      +-----+---+\n",
      "     |      >>> df[df.age > 3].show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |  5| Bob|\n",
      "     |      +---+----+\n",
      "     |      >>> df[df[0] > 3].show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |  5| Bob|\n",
      "     |      +---+----+\n",
      "     |  \n",
      "     |  __init__(self, jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[ForwardRef('SQLContext'), ForwardRef('SparkSession')])\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  agg(self, *exprs: Union[pyspark.sql.column.Column, Dict[str, str]]) -> 'DataFrame'\n",
      "     |      Aggregate on the entire :class:`DataFrame` without groups\n",
      "     |      (shorthand for ``df.groupBy().agg()``).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      exprs : :class:`Column` or dict of key and value strings\n",
      "     |          Columns or expressions to aggregate DataFrame by.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Aggregated DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import functions as sf\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.agg({\"age\": \"max\"}).show()\n",
      "     |      +--------+\n",
      "     |      |max(age)|\n",
      "     |      +--------+\n",
      "     |      |       5|\n",
      "     |      +--------+\n",
      "     |      >>> df.agg(sf.min(df.age)).show()\n",
      "     |      +--------+\n",
      "     |      |min(age)|\n",
      "     |      +--------+\n",
      "     |      |       2|\n",
      "     |      +--------+\n",
      "     |  \n",
      "     |  alias(self, alias: str) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` with an alias set.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      alias : str\n",
      "     |          an alias name to be set for the :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Aliased DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import col, desc\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df_as1 = df.alias(\"df_as1\")\n",
      "     |      >>> df_as2 = df.alias(\"df_as2\")\n",
      "     |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
      "     |      >>> joined_df.select(\n",
      "     |      ...     \"df_as1.name\", \"df_as2.name\", \"df_as2.age\").sort(desc(\"df_as1.name\")).show()\n",
      "     |      +-----+-----+---+\n",
      "     |      | name| name|age|\n",
      "     |      +-----+-----+---+\n",
      "     |      |  Tom|  Tom| 14|\n",
      "     |      |  Bob|  Bob| 16|\n",
      "     |      |Alice|Alice| 23|\n",
      "     |      +-----+-----+---+\n",
      "     |  \n",
      "     |  approxQuantile(self, col: Union[str, List[str], Tuple[str]], probabilities: Union[List[float], Tuple[float]], relativeError: float) -> Union[List[float], List[List[float]]]\n",
      "     |      Calculates the approximate quantiles of numerical columns of a\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The result of this algorithm has the following deterministic bound:\n",
      "     |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
      "     |      probability `p` up to error `err`, then the algorithm will return\n",
      "     |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
      "     |      close to (p * N). More precisely,\n",
      "     |      \n",
      "     |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
      "     |      \n",
      "     |      This method implements a variation of the Greenwald-Khanna\n",
      "     |      algorithm (with some speed optimizations). The algorithm was first\n",
      "     |      present in [[https://doi.org/10.1145/375663.375670\n",
      "     |      Space-efficient Online Computation of Quantile Summaries]]\n",
      "     |      by Greenwald and Khanna.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col: str, tuple or list\n",
      "     |          Can be a single column name, or a list of names for multiple columns.\n",
      "     |      \n",
      "     |          .. versionchanged:: 2.2.0\n",
      "     |             Added support for multiple columns.\n",
      "     |      probabilities : list or tuple\n",
      "     |          a list of quantile probabilities\n",
      "     |          Each number must belong to [0, 1].\n",
      "     |          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
      "     |      relativeError : float\n",
      "     |          The relative target precision to achieve\n",
      "     |          (>= 0). If set to zero, the exact quantiles are computed, which\n",
      "     |          could be very expensive. Note that values greater than 1 are\n",
      "     |          accepted but gives the same result as 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          the approximate quantiles at the given probabilities.\n",
      "     |      \n",
      "     |          * If the input `col` is a string, the output is a list of floats.\n",
      "     |      \n",
      "     |          * If the input `col` is a list or tuple of strings, the output is also a\n",
      "     |              list, but each element in it is a list of floats, i.e., the output\n",
      "     |              is a list of list of floats.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Null values will be ignored in numerical columns before calculation.\n",
      "     |      For columns only containing null values, an empty list is returned.\n",
      "     |  \n",
      "     |  cache(self) -> 'DataFrame'\n",
      "     |      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Cached DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.range(1)\n",
      "     |      >>> df.cache()\n",
      "     |      DataFrame[id: bigint]\n",
      "     |      \n",
      "     |      >>> df.explain()\n",
      "     |      == Physical Plan ==\n",
      "     |      AdaptiveSparkPlan isFinalPlan=false\n",
      "     |      +- InMemoryTableScan ...\n",
      "     |  \n",
      "     |  checkpoint(self, eager: bool = True) -> 'DataFrame'\n",
      "     |      Returns a checkpointed version of this :class:`DataFrame`. Checkpointing can be used to\n",
      "     |      truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      "     |      iterative algorithms where the plan may grow exponentially. It will be saved to files\n",
      "     |      inside the checkpoint directory set with :meth:`SparkContext.setCheckpointDir`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      eager : bool, optional, default True\n",
      "     |          Whether to checkpoint this :class:`DataFrame` immediately.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Checkpointed DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is experimental.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import tempfile\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     spark.sparkContext.setCheckpointDir(\"/tmp/bb\")\n",
      "     |      ...     df.checkpoint(False)\n",
      "     |      DataFrame[age: bigint, name: string]\n",
      "     |  \n",
      "     |  coalesce(self, numPartitions: int) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
      "     |      \n",
      "     |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n",
      "     |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n",
      "     |      there will not be a shuffle, instead each of the 100 new partitions will\n",
      "     |      claim 10 of the current partitions. If a larger number of partitions is requested,\n",
      "     |      it will stay at the current number of partitions.\n",
      "     |      \n",
      "     |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n",
      "     |      this may result in your computation taking place on fewer nodes than\n",
      "     |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n",
      "     |      you can call repartition(). This will add a shuffle step, but means the\n",
      "     |      current upstream partitions will be executed in parallel (per whatever\n",
      "     |      the current partitioning is).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      numPartitions : int\n",
      "     |          specify the target number of partitions\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.range(10)\n",
      "     |      >>> df.coalesce(1).rdd.getNumPartitions()\n",
      "     |      1\n",
      "     |  \n",
      "     |  colRegex(self, colName: str) -> pyspark.sql.column.Column\n",
      "     |      Selects column based on the column name specified as a regex and returns it\n",
      "     |      as :class:`Column`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      colName : str\n",
      "     |          string, column name specified as a regex.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Column`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
      "     |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n",
      "     |      +----+\n",
      "     |      |Col2|\n",
      "     |      +----+\n",
      "     |      |   1|\n",
      "     |      |   2|\n",
      "     |      |   3|\n",
      "     |      +----+\n",
      "     |  \n",
      "     |  collect(self) -> List[pyspark.sql.types.Row]\n",
      "     |      Returns all the records as a list of :class:`Row`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          List of rows.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.collect()\n",
      "     |      [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
      "     |  \n",
      "     |  corr(self, col1: str, col2: str, method: Optional[str] = None) -> float\n",
      "     |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
      "     |      Currently only supports the Pearson Correlation Coefficient.\n",
      "     |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col1 : str\n",
      "     |          The name of the first column\n",
      "     |      col2 : str\n",
      "     |          The name of the second column\n",
      "     |      method : str, optional\n",
      "     |          The correlation method. Currently only supports \"pearson\"\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float\n",
      "     |          Pearson Correlation Coefficient of two columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
      "     |      >>> df.corr(\"c1\", \"c2\")\n",
      "     |      -0.3592106040535498\n",
      "     |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
      "     |      >>> df.corr(\"small\", \"bigger\")\n",
      "     |      1.0\n",
      "     |  \n",
      "     |  count(self) -> int\n",
      "     |      Returns the number of rows in this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          Number of rows.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Return the number of rows in the :class:`DataFrame`.\n",
      "     |      \n",
      "     |      >>> df.count()\n",
      "     |      3\n",
      "     |  \n",
      "     |  cov(self, col1: str, col2: str) -> float\n",
      "     |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
      "     |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col1 : str\n",
      "     |          The name of the first column\n",
      "     |      col2 : str\n",
      "     |          The name of the second column\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float\n",
      "     |          Covariance of two columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
      "     |      >>> df.cov(\"c1\", \"c2\")\n",
      "     |      -18.0\n",
      "     |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
      "     |      >>> df.cov(\"small\", \"bigger\")\n",
      "     |      1.0\n",
      "     |  \n",
      "     |  createGlobalTempView(self, name: str) -> None\n",
      "     |      Creates a global temporary view with this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The lifetime of this temporary view is tied to this Spark application.\n",
      "     |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      "     |      catalog.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          Name of the view.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Create a global temporary view.\n",
      "     |      \n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.createGlobalTempView(\"people\")\n",
      "     |      >>> df2 = spark.sql(\"SELECT * FROM global_temp.people\")\n",
      "     |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |      \n",
      "     |      Throws an exception if the global temporary view already exists.\n",
      "     |      \n",
      "     |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "     |      Traceback (most recent call last):\n",
      "     |      ...\n",
      "     |      AnalysisException: \"Temporary table 'people' already exists;\"\n",
      "     |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      "     |      True\n",
      "     |  \n",
      "     |  createOrReplaceGlobalTempView(self, name: str) -> None\n",
      "     |      Creates or replaces a global temporary view using the given name.\n",
      "     |      \n",
      "     |      The lifetime of this temporary view is tied to this Spark application.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.2.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          Name of the view.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Create a global temporary view.\n",
      "     |      \n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.createOrReplaceGlobalTempView(\"people\")\n",
      "     |      \n",
      "     |      Replace the global temporary view.\n",
      "     |      \n",
      "     |      >>> df2 = df.filter(df.age > 3)\n",
      "     |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n",
      "     |      >>> df3 = spark.sql(\"SELECT * FROM global_temp.people\")\n",
      "     |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      "     |      True\n",
      "     |  \n",
      "     |  createOrReplaceTempView(self, name: str) -> None\n",
      "     |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      "     |      that was used to create this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          Name of the view.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Create a local temporary view named 'people'.\n",
      "     |      \n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.createOrReplaceTempView(\"people\")\n",
      "     |      \n",
      "     |      Replace the local temporary view.\n",
      "     |      \n",
      "     |      >>> df2 = df.filter(df.age > 3)\n",
      "     |      >>> df2.createOrReplaceTempView(\"people\")\n",
      "     |      >>> df3 = spark.sql(\"SELECT * FROM people\")\n",
      "     |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |      >>> spark.catalog.dropTempView(\"people\")\n",
      "     |      True\n",
      "     |  \n",
      "     |  createTempView(self, name: str) -> None\n",
      "     |      Creates a local temporary view with this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      "     |      that was used to create this :class:`DataFrame`.\n",
      "     |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      "     |      catalog.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          Name of the view.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Create a local temporary view.\n",
      "     |      \n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.createTempView(\"people\")\n",
      "     |      >>> df2 = spark.sql(\"SELECT * FROM people\")\n",
      "     |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |      \n",
      "     |      Throw an exception if the table already exists.\n",
      "     |      \n",
      "     |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "     |      Traceback (most recent call last):\n",
      "     |      ...\n",
      "     |      AnalysisException: \"Temporary table 'people' already exists;\"\n",
      "     |      >>> spark.catalog.dropTempView(\"people\")\n",
      "     |      True\n",
      "     |  \n",
      "     |  crossJoin(self, other: 'DataFrame') -> 'DataFrame'\n",
      "     |      Returns the cartesian product with another :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          Right side of the cartesian product.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Joined DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df2 = spark.createDataFrame(\n",
      "     |      ...     [Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      "     |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").show()\n",
      "     |      +---+-----+------+\n",
      "     |      |age| name|height|\n",
      "     |      +---+-----+------+\n",
      "     |      | 14|  Tom|    80|\n",
      "     |      | 14|  Tom|    85|\n",
      "     |      | 23|Alice|    80|\n",
      "     |      | 23|Alice|    85|\n",
      "     |      | 16|  Bob|    80|\n",
      "     |      | 16|  Bob|    85|\n",
      "     |      +---+-----+------+\n",
      "     |  \n",
      "     |  crosstab(self, col1: str, col2: str) -> 'DataFrame'\n",
      "     |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
      "     |      table.\n",
      "     |      The first column of each row will be the distinct values of `col1` and the column names\n",
      "     |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
      "     |      Pairs that have no occurrences will have zero as their counts.\n",
      "     |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col1 : str\n",
      "     |          The name of the first column. Distinct items will make the first item of\n",
      "     |          each row.\n",
      "     |      col2 : str\n",
      "     |          The name of the second column. Distinct items will make the column names\n",
      "     |          of the :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Frequency matrix of two columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
      "     |      >>> df.crosstab(\"c1\", \"c2\").sort(\"c1_c2\").show()\n",
      "     |      +-----+---+---+---+\n",
      "     |      |c1_c2| 10| 11|  8|\n",
      "     |      +-----+---+---+---+\n",
      "     |      |    1|  0|  2|  0|\n",
      "     |      |    3|  1|  0|  0|\n",
      "     |      |    4|  0|  0|  2|\n",
      "     |      +-----+---+---+---+\n",
      "     |  \n",
      "     |  cube(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
      "     |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
      "     |      the specified columns, so we can run aggregations on them.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : list, str or :class:`Column`\n",
      "     |          columns to create cube by.\n",
      "     |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
      "     |          or list of them.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`GroupedData`\n",
      "     |          Cube of the data by given columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      "     |      +-----+----+-----+\n",
      "     |      | name| age|count|\n",
      "     |      +-----+----+-----+\n",
      "     |      | NULL|NULL|    2|\n",
      "     |      | NULL|   2|    1|\n",
      "     |      | NULL|   5|    1|\n",
      "     |      |Alice|NULL|    1|\n",
      "     |      |Alice|   2|    1|\n",
      "     |      |  Bob|NULL|    1|\n",
      "     |      |  Bob|   5|    1|\n",
      "     |      +-----+----+-----+\n",
      "     |  \n",
      "     |  describe(self, *cols: Union[str, List[str]]) -> 'DataFrame'\n",
      "     |      Computes basic statistics for numeric and string columns.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      This includes count, mean, stddev, min, and max. If no columns are\n",
      "     |      given, this function computes statistics for all numerical or string columns.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This function is meant for exploratory data analysis, as we make no\n",
      "     |      guarantee about the backward compatibility of the schema of the resulting\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Use summary for expanded statistics and control over which statistics to compute.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, list, optional\n",
      "     |           Column name or list of column names to describe by (default All columns).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          A new DataFrame that describes (provides statistics) given DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
      "     |      ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
      "     |      ... )\n",
      "     |      >>> df.describe(['age']).show()\n",
      "     |      +-------+----+\n",
      "     |      |summary| age|\n",
      "     |      +-------+----+\n",
      "     |      |  count|   3|\n",
      "     |      |   mean|12.0|\n",
      "     |      | stddev| 1.0|\n",
      "     |      |    min|  11|\n",
      "     |      |    max|  13|\n",
      "     |      +-------+----+\n",
      "     |      \n",
      "     |      >>> df.describe(['age', 'weight', 'height']).show()\n",
      "     |      +-------+----+------------------+-----------------+\n",
      "     |      |summary| age|            weight|           height|\n",
      "     |      +-------+----+------------------+-----------------+\n",
      "     |      |  count|   3|                 3|                3|\n",
      "     |      |   mean|12.0| 40.73333333333333|            145.0|\n",
      "     |      | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
      "     |      |    min|  11|              37.8|            142.2|\n",
      "     |      |    max|  13|              44.1|            150.5|\n",
      "     |      +-------+----+------------------+-----------------+\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      DataFrame.summary\n",
      "     |  \n",
      "     |  distinct(self) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with distinct records.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (23, \"Alice\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Return the number of distinct rows in the :class:`DataFrame`\n",
      "     |      \n",
      "     |      >>> df.distinct().count()\n",
      "     |      2\n",
      "     |  \n",
      "     |  drop(self, *cols: 'ColumnOrName') -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` without specified columns.\n",
      "     |      This is a no-op if the schema doesn't contain the given column name(s).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols: str or :class:`Column`\n",
      "     |          a name of the column, or the :class:`Column` to drop\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame without given columns.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      When an input is a column name, it is treated literally without further interpretation.\n",
      "     |      Otherwise, will try to match the equivalent expression.\n",
      "     |      So that dropping column by its name `drop(colName)` has different semantic with directly\n",
      "     |      dropping the column `drop(col(colName))`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> from pyspark.sql.functions import col, lit\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      "     |      \n",
      "     |      >>> df.drop('age').show()\n",
      "     |      +-----+\n",
      "     |      | name|\n",
      "     |      +-----+\n",
      "     |      |  Tom|\n",
      "     |      |Alice|\n",
      "     |      |  Bob|\n",
      "     |      +-----+\n",
      "     |      >>> df.drop(df.age).show()\n",
      "     |      +-----+\n",
      "     |      | name|\n",
      "     |      +-----+\n",
      "     |      |  Tom|\n",
      "     |      |Alice|\n",
      "     |      |  Bob|\n",
      "     |      +-----+\n",
      "     |      \n",
      "     |      Drop the column that joined both DataFrames on.\n",
      "     |      \n",
      "     |      >>> df.join(df2, df.name == df2.name, 'inner').drop('name').sort('age').show()\n",
      "     |      +---+------+\n",
      "     |      |age|height|\n",
      "     |      +---+------+\n",
      "     |      | 14|    80|\n",
      "     |      | 16|    85|\n",
      "     |      +---+------+\n",
      "     |      \n",
      "     |      >>> df3 = df.join(df2)\n",
      "     |      >>> df3.show()\n",
      "     |      +---+-----+------+----+\n",
      "     |      |age| name|height|name|\n",
      "     |      +---+-----+------+----+\n",
      "     |      | 14|  Tom|    80| Tom|\n",
      "     |      | 14|  Tom|    85| Bob|\n",
      "     |      | 23|Alice|    80| Tom|\n",
      "     |      | 23|Alice|    85| Bob|\n",
      "     |      | 16|  Bob|    80| Tom|\n",
      "     |      | 16|  Bob|    85| Bob|\n",
      "     |      +---+-----+------+----+\n",
      "     |      \n",
      "     |      Drop two column by the same name.\n",
      "     |      \n",
      "     |      >>> df3.drop(\"name\").show()\n",
      "     |      +---+------+\n",
      "     |      |age|height|\n",
      "     |      +---+------+\n",
      "     |      | 14|    80|\n",
      "     |      | 14|    85|\n",
      "     |      | 23|    80|\n",
      "     |      | 23|    85|\n",
      "     |      | 16|    80|\n",
      "     |      | 16|    85|\n",
      "     |      +---+------+\n",
      "     |      \n",
      "     |      Can not drop col('name') due to ambiguous reference.\n",
      "     |      \n",
      "     |      >>> df3.drop(col(\"name\")).show()\n",
      "     |      Traceback (most recent call last):\n",
      "     |      ...\n",
      "     |      pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference...\n",
      "     |      \n",
      "     |      >>> df4 = df.withColumn(\"a.b.c\", lit(1))\n",
      "     |      >>> df4.show()\n",
      "     |      +---+-----+-----+\n",
      "     |      |age| name|a.b.c|\n",
      "     |      +---+-----+-----+\n",
      "     |      | 14|  Tom|    1|\n",
      "     |      | 23|Alice|    1|\n",
      "     |      | 16|  Bob|    1|\n",
      "     |      +---+-----+-----+\n",
      "     |      \n",
      "     |      >>> df4.drop(\"a.b.c\").show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      | 14|  Tom|\n",
      "     |      | 23|Alice|\n",
      "     |      | 16|  Bob|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Can not find a column matching the expression \"a.b.c\".\n",
      "     |      \n",
      "     |      >>> df4.drop(col(\"a.b.c\")).show()\n",
      "     |      +---+-----+-----+\n",
      "     |      |age| name|a.b.c|\n",
      "     |      +---+-----+-----+\n",
      "     |      | 14|  Tom|    1|\n",
      "     |      | 23|Alice|    1|\n",
      "     |      | 16|  Bob|    1|\n",
      "     |      +---+-----+-----+\n",
      "     |  \n",
      "     |  dropDuplicates(self, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
      "     |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
      "     |      optionally only considering certain columns.\n",
      "     |      \n",
      "     |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      "     |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      "     |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      "     |      be and the system will accordingly limit the state. In addition, data older than\n",
      "     |      watermark will be dropped to avoid any possibility of duplicates.\n",
      "     |      \n",
      "     |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      subset : List of column names, optional\n",
      "     |          List of columns to use for duplicate comparison (default All columns).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame without duplicates.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     Row(name='Alice', age=5, height=80),\n",
      "     |      ...     Row(name='Alice', age=5, height=80),\n",
      "     |      ...     Row(name='Alice', age=10, height=80)\n",
      "     |      ... ])\n",
      "     |      \n",
      "     |      Deduplicate the same rows.\n",
      "     |      \n",
      "     |      >>> df.dropDuplicates().show()\n",
      "     |      +-----+---+------+\n",
      "     |      | name|age|height|\n",
      "     |      +-----+---+------+\n",
      "     |      |Alice|  5|    80|\n",
      "     |      |Alice| 10|    80|\n",
      "     |      +-----+---+------+\n",
      "     |      \n",
      "     |      Deduplicate values on 'name' and 'height' columns.\n",
      "     |      \n",
      "     |      >>> df.dropDuplicates(['name', 'height']).show()\n",
      "     |      +-----+---+------+\n",
      "     |      | name|age|height|\n",
      "     |      +-----+---+------+\n",
      "     |      |Alice|  5|    80|\n",
      "     |      +-----+---+------+\n",
      "     |  \n",
      "     |  dropDuplicatesWithinWatermark(self, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
      "     |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
      "     |       optionally only considering certain columns, within watermark.\n",
      "     |      \n",
      "     |       This only works with streaming :class:`DataFrame`, and watermark for the input\n",
      "     |       :class:`DataFrame` must be set via :func:`withWatermark`.\n",
      "     |      \n",
      "     |      For a streaming :class:`DataFrame`, this will keep all data across triggers as intermediate\n",
      "     |      state to drop duplicated rows. The state will be kept to guarantee the semantic, \"Events\n",
      "     |      are deduplicated as long as the time distance of earliest and latest events are smaller\n",
      "     |      than the delay threshold of watermark.\" Users are encouraged to set the delay threshold of\n",
      "     |      watermark longer than max timestamp differences among duplicated events.\n",
      "     |      \n",
      "     |      Note: too late data older than watermark will be dropped.\n",
      "     |      \n",
      "     |       .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |       Parameters\n",
      "     |       ----------\n",
      "     |       subset : List of column names, optional\n",
      "     |           List of columns to use for duplicate comparison (default All columns).\n",
      "     |      \n",
      "     |       Returns\n",
      "     |       -------\n",
      "     |       :class:`DataFrame`\n",
      "     |           DataFrame without duplicates.\n",
      "     |      \n",
      "     |       Notes\n",
      "     |       -----\n",
      "     |       Supports Spark Connect.\n",
      "     |      \n",
      "     |       Examples\n",
      "     |       --------\n",
      "     |       >>> from pyspark.sql import Row\n",
      "     |       >>> from pyspark.sql.functions import timestamp_seconds\n",
      "     |       >>> df = spark.readStream.format(\"rate\").load().selectExpr(\n",
      "     |       ...     \"value % 5 AS value\", \"timestamp\")\n",
      "     |       >>> df.select(\"value\", df.timestamp.alias(\"time\")).withWatermark(\"time\", '10 minutes')\n",
      "     |       DataFrame[value: bigint, time: timestamp]\n",
      "     |      \n",
      "     |       Deduplicate the same rows.\n",
      "     |      \n",
      "     |       >>> df.dropDuplicatesWithinWatermark() # doctest: +SKIP\n",
      "     |      \n",
      "     |       Deduplicate values on 'value' columns.\n",
      "     |      \n",
      "     |       >>> df.dropDuplicatesWithinWatermark(['value'])  # doctest: +SKIP\n",
      "     |  \n",
      "     |  drop_duplicates = dropDuplicates(self, subset=None)\n",
      "     |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  dropna(self, how: str = 'any', thresh: Optional[int] = None, subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      "     |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      how : str, optional\n",
      "     |          'any' or 'all'.\n",
      "     |          If 'any', drop a row if it contains any nulls.\n",
      "     |          If 'all', drop a row only if all its values are null.\n",
      "     |      thresh: int, optional\n",
      "     |          default None\n",
      "     |          If specified, drop rows that have less than `thresh` non-null values.\n",
      "     |          This overwrites the `how` parameter.\n",
      "     |      subset : str, tuple or list, optional\n",
      "     |          optional list of column names to consider.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with null only rows excluded.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
      "     |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
      "     |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
      "     |      ...     Row(age=None, height=None, name=None),\n",
      "     |      ... ])\n",
      "     |      >>> df.na.drop().show()\n",
      "     |      +---+------+-----+\n",
      "     |      |age|height| name|\n",
      "     |      +---+------+-----+\n",
      "     |      | 10|    80|Alice|\n",
      "     |      +---+------+-----+\n",
      "     |  \n",
      "     |  exceptAll(self, other: 'DataFrame') -> 'DataFrame'\n",
      "     |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
      "     |      not in another :class:`DataFrame` while preserving duplicates.\n",
      "     |      \n",
      "     |      This is equivalent to `EXCEPT ALL` in SQL.\n",
      "     |      As standard in SQL, this function resolves columns by position (not by name).\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          The other :class:`DataFrame` to compare to.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df1 = spark.createDataFrame(\n",
      "     |      ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      "     |      >>> df1.exceptAll(df2).show()\n",
      "     |      +---+---+\n",
      "     |      | C1| C2|\n",
      "     |      +---+---+\n",
      "     |      |  a|  1|\n",
      "     |      |  a|  1|\n",
      "     |      |  a|  2|\n",
      "     |      |  c|  4|\n",
      "     |      +---+---+\n",
      "     |  \n",
      "     |  explain(self, extended: Union[bool, str, NoneType] = None, mode: Optional[str] = None) -> None\n",
      "     |      Prints the (logical and physical) plans to the console for debugging purposes.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      extended : bool, optional\n",
      "     |          default ``False``. If ``False``, prints only the physical plan.\n",
      "     |          When this is a string without specifying the ``mode``, it works as the mode is\n",
      "     |          specified.\n",
      "     |      mode : str, optional\n",
      "     |          specifies the expected output format of plans.\n",
      "     |      \n",
      "     |          * ``simple``: Print only a physical plan.\n",
      "     |          * ``extended``: Print both logical and physical plans.\n",
      "     |          * ``codegen``: Print a physical plan and generated codes if they are available.\n",
      "     |          * ``cost``: Print a logical plan and statistics if they are available.\n",
      "     |          * ``formatted``: Split explain output into two sections: a physical plan outline                 and node details.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.0.0\n",
      "     |             Added optional argument `mode` to specify the expected output format of plans.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Print out the physical plan only (default).\n",
      "     |      \n",
      "     |      >>> df.explain()  # doctest: +SKIP\n",
      "     |      == Physical Plan ==\n",
      "     |      *(1) Scan ExistingRDD[age...,name...]\n",
      "     |      \n",
      "     |      Print out all of the parsed, analyzed, optimized and physical plans.\n",
      "     |      \n",
      "     |      >>> df.explain(True)\n",
      "     |      == Parsed Logical Plan ==\n",
      "     |      ...\n",
      "     |      == Analyzed Logical Plan ==\n",
      "     |      ...\n",
      "     |      == Optimized Logical Plan ==\n",
      "     |      ...\n",
      "     |      == Physical Plan ==\n",
      "     |      ...\n",
      "     |      \n",
      "     |      Print out the plans with two sections: a physical plan outline and node details\n",
      "     |      \n",
      "     |      >>> df.explain(mode=\"formatted\")  # doctest: +SKIP\n",
      "     |      == Physical Plan ==\n",
      "     |      * Scan ExistingRDD (...)\n",
      "     |      (1) Scan ExistingRDD [codegen id : ...]\n",
      "     |      Output [2]: [age..., name...]\n",
      "     |      ...\n",
      "     |      \n",
      "     |      Print a logical plan and statistics if they are available.\n",
      "     |      \n",
      "     |      >>> df.explain(\"cost\")\n",
      "     |      == Optimized Logical Plan ==\n",
      "     |      ...Statistics...\n",
      "     |      ...\n",
      "     |  \n",
      "     |  fillna(self, value: Union[ForwardRef('LiteralType'), Dict[str, ForwardRef('LiteralType')]], subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> 'DataFrame'\n",
      "     |      Replace null values, alias for ``na.fill()``.\n",
      "     |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      value : int, float, string, bool or dict\n",
      "     |          Value to replace null values with.\n",
      "     |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      "     |          from column name (string) to replacement value. The replacement value must be\n",
      "     |          an int, float, boolean, or string.\n",
      "     |      subset : str, tuple or list, optional\n",
      "     |          optional list of column names to consider.\n",
      "     |          Columns specified in subset that do not have matching data types are ignored.\n",
      "     |          For example, if `value` is a string, and subset contains a non-string column,\n",
      "     |          then the non-string column is simply ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with replaced null values.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (10, 80.5, \"Alice\", None),\n",
      "     |      ...     (5, None, \"Bob\", None),\n",
      "     |      ...     (None, None, \"Tom\", None),\n",
      "     |      ...     (None, None, None, True)],\n",
      "     |      ...     schema=[\"age\", \"height\", \"name\", \"bool\"])\n",
      "     |      \n",
      "     |      Fill all null values with 50 for numeric columns.\n",
      "     |      \n",
      "     |      >>> df.na.fill(50).show()\n",
      "     |      +---+------+-----+----+\n",
      "     |      |age|height| name|bool|\n",
      "     |      +---+------+-----+----+\n",
      "     |      | 10|  80.5|Alice|NULL|\n",
      "     |      |  5|  50.0|  Bob|NULL|\n",
      "     |      | 50|  50.0|  Tom|NULL|\n",
      "     |      | 50|  50.0| NULL|true|\n",
      "     |      +---+------+-----+----+\n",
      "     |      \n",
      "     |      Fill all null values with ``False`` for boolean columns.\n",
      "     |      \n",
      "     |      >>> df.na.fill(False).show()\n",
      "     |      +----+------+-----+-----+\n",
      "     |      | age|height| name| bool|\n",
      "     |      +----+------+-----+-----+\n",
      "     |      |  10|  80.5|Alice|false|\n",
      "     |      |   5|  NULL|  Bob|false|\n",
      "     |      |NULL|  NULL|  Tom|false|\n",
      "     |      |NULL|  NULL| NULL| true|\n",
      "     |      +----+------+-----+-----+\n",
      "     |      \n",
      "     |      Fill all null values with to 50 and \"unknown\" for 'age' and 'name' column respectively.\n",
      "     |      \n",
      "     |      >>> df.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      "     |      +---+------+-------+----+\n",
      "     |      |age|height|   name|bool|\n",
      "     |      +---+------+-------+----+\n",
      "     |      | 10|  80.5|  Alice|NULL|\n",
      "     |      |  5|  NULL|    Bob|NULL|\n",
      "     |      | 50|  NULL|    Tom|NULL|\n",
      "     |      | 50|  NULL|unknown|true|\n",
      "     |      +---+------+-------+----+\n",
      "     |  \n",
      "     |  filter(self, condition: 'ColumnOrName') -> 'DataFrame'\n",
      "     |      Filters rows using the given condition.\n",
      "     |      \n",
      "     |      :func:`where` is an alias for :func:`filter`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      condition : :class:`Column` or str\n",
      "     |          a :class:`Column` of :class:`types.BooleanType`\n",
      "     |          or a string of SQL expressions.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Filtered DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Filter by :class:`Column` instances.\n",
      "     |      \n",
      "     |      >>> df.filter(df.age > 3).show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |  5| Bob|\n",
      "     |      +---+----+\n",
      "     |      >>> df.where(df.age == 2).show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  2|Alice|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Filter by SQL expression in a string.\n",
      "     |      \n",
      "     |      >>> df.filter(\"age > 3\").show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |  5| Bob|\n",
      "     |      +---+----+\n",
      "     |      >>> df.where(\"age = 2\").show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  2|Alice|\n",
      "     |      +---+-----+\n",
      "     |  \n",
      "     |  first(self) -> Optional[pyspark.sql.types.Row]\n",
      "     |      Returns the first row as a :class:`Row`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Row`\n",
      "     |          First row if :class:`DataFrame` is not empty, otherwise ``None``.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.first()\n",
      "     |      Row(age=2, name='Alice')\n",
      "     |  \n",
      "     |  foreach(self, f: Callable[[pyspark.sql.types.Row], NoneType]) -> None\n",
      "     |      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      This is a shorthand for ``df.rdd.foreach()``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |          A function that accepts one parameter which will\n",
      "     |          receive each row to process.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> def func(person):\n",
      "     |      ...     print(person.name)\n",
      "     |      ...\n",
      "     |      >>> df.foreach(func)\n",
      "     |  \n",
      "     |  foreachPartition(self, f: Callable[[Iterator[pyspark.sql.types.Row]], NoneType]) -> None\n",
      "     |      Applies the ``f`` function to each partition of this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      This a shorthand for ``df.rdd.foreachPartition()``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      f : function\n",
      "     |          A function that accepts one parameter which will receive\n",
      "     |          each partition to process.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> def func(itr):\n",
      "     |      ...     for person in itr:\n",
      "     |      ...         print(person.name)\n",
      "     |      ...\n",
      "     |      >>> df.foreachPartition(func)\n",
      "     |  \n",
      "     |  freqItems(self, cols: Union[List[str], Tuple[str]], support: Optional[float] = None) -> 'DataFrame'\n",
      "     |      Finding frequent items for columns, possibly with false positives. Using the\n",
      "     |      frequent element count algorithm described in\n",
      "     |      \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
      "     |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : list or tuple\n",
      "     |          Names of the columns to calculate frequent items for as a list or tuple of\n",
      "     |          strings.\n",
      "     |      support : float, optional\n",
      "     |          The frequency with which to consider an item 'frequent'. Default is 1%.\n",
      "     |          The support must be greater than 1e-4.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with frequent items.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This function is meant for exploratory data analysis, as we make no\n",
      "     |      guarantee about the backward compatibility of the schema of the resulting\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
      "     |      >>> df.freqItems([\"c1\", \"c2\"]).show()  # doctest: +SKIP\n",
      "     |      +------------+------------+\n",
      "     |      |c1_freqItems|c2_freqItems|\n",
      "     |      +------------+------------+\n",
      "     |      |   [4, 1, 3]| [8, 11, 10]|\n",
      "     |      +------------+------------+\n",
      "     |  \n",
      "     |  groupBy(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
      "     |      Groups the :class:`DataFrame` using the specified columns,\n",
      "     |      so we can run aggregation on them. See :class:`GroupedData`\n",
      "     |      for all the available aggregate functions.\n",
      "     |      \n",
      "     |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : list, str or :class:`Column`\n",
      "     |          columns to group by.\n",
      "     |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
      "     |          or list of them.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`GroupedData`\n",
      "     |          Grouped data by given columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (2, \"Bob\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Empty grouping columns triggers a global aggregation.\n",
      "     |      \n",
      "     |      >>> df.groupBy().avg().show()\n",
      "     |      +--------+\n",
      "     |      |avg(age)|\n",
      "     |      +--------+\n",
      "     |      |    2.75|\n",
      "     |      +--------+\n",
      "     |      \n",
      "     |      Group-by 'name', and specify a dictionary to calculate the summation of 'age'.\n",
      "     |      \n",
      "     |      >>> df.groupBy(\"name\").agg({\"age\": \"sum\"}).sort(\"name\").show()\n",
      "     |      +-----+--------+\n",
      "     |      | name|sum(age)|\n",
      "     |      +-----+--------+\n",
      "     |      |Alice|       2|\n",
      "     |      |  Bob|       9|\n",
      "     |      +-----+--------+\n",
      "     |      \n",
      "     |      Group-by 'name', and calculate maximum values.\n",
      "     |      \n",
      "     |      >>> df.groupBy(df.name).max().sort(\"name\").show()\n",
      "     |      +-----+--------+\n",
      "     |      | name|max(age)|\n",
      "     |      +-----+--------+\n",
      "     |      |Alice|       2|\n",
      "     |      |  Bob|       5|\n",
      "     |      +-----+--------+\n",
      "     |      \n",
      "     |      Group-by 'name' and 'age', and calculate the number of rows in each group.\n",
      "     |      \n",
      "     |      >>> df.groupBy([\"name\", df.age]).count().sort(\"name\", \"age\").show()\n",
      "     |      +-----+---+-----+\n",
      "     |      | name|age|count|\n",
      "     |      +-----+---+-----+\n",
      "     |      |Alice|  2|    1|\n",
      "     |      |  Bob|  2|    2|\n",
      "     |      |  Bob|  5|    1|\n",
      "     |      +-----+---+-----+\n",
      "     |  \n",
      "     |  groupby = groupBy(self, *cols)\n",
      "     |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  head(self, n: Optional[int] = None) -> Union[pyspark.sql.types.Row, NoneType, List[pyspark.sql.types.Row]]\n",
      "     |      Returns the first ``n`` rows.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method should only be used if the resulting array is expected\n",
      "     |      to be small, as all the data is loaded into the driver's memory.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int, optional\n",
      "     |          default 1. Number of rows to return.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      If n is greater than 1, return a list of :class:`Row`.\n",
      "     |      If n is 1, return a single Row.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.head()\n",
      "     |      Row(age=2, name='Alice')\n",
      "     |      >>> df.head(1)\n",
      "     |      [Row(age=2, name='Alice')]\n",
      "     |  \n",
      "     |  hint(self, name: str, *parameters: Union[ForwardRef('PrimitiveType'), List[ForwardRef('PrimitiveType')]]) -> 'DataFrame'\n",
      "     |      Specifies some hint on the current :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.2.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          A name of the hint.\n",
      "     |      parameters : str, list, float or int\n",
      "     |          Optional parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Hinted DataFrame\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      "     |      >>> df.join(df2, \"name\").explain()  # doctest: +SKIP\n",
      "     |      == Physical Plan ==\n",
      "     |      ...\n",
      "     |      ... +- SortMergeJoin ...\n",
      "     |      ...\n",
      "     |      \n",
      "     |      Explicitly trigger the broadcast hashjoin by providing the hint in ``df2``.\n",
      "     |      \n",
      "     |      >>> df.join(df2.hint(\"broadcast\"), \"name\").explain()\n",
      "     |      == Physical Plan ==\n",
      "     |      ...\n",
      "     |      ... +- BroadcastHashJoin ...\n",
      "     |      ...\n",
      "     |  \n",
      "     |  inputFiles(self) -> List[str]\n",
      "     |      Returns a best-effort snapshot of the files that compose this :class:`DataFrame`.\n",
      "     |      This method simply asks each constituent BaseRelation for its respective files and\n",
      "     |      takes the union of all results. Depending on the source relations, this may not find\n",
      "     |      all input files. Duplicates are removed.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          List of file paths.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a single-row DataFrame into a JSON file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).repartition(1).write.json(d, mode=\"overwrite\")\n",
      "     |      ...\n",
      "     |      ...     # Read the JSON file as a DataFrame.\n",
      "     |      ...     df = spark.read.format(\"json\").load(d)\n",
      "     |      ...\n",
      "     |      ...     # Returns the number of input files.\n",
      "     |      ...     len(df.inputFiles())\n",
      "     |      1\n",
      "     |  \n",
      "     |  intersect(self, other: 'DataFrame') -> 'DataFrame'\n",
      "     |      Return a new :class:`DataFrame` containing rows only in\n",
      "     |      both this :class:`DataFrame` and another :class:`DataFrame`.\n",
      "     |      Note that any duplicates are removed. To preserve duplicates\n",
      "     |      use :func:`intersectAll`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          Another :class:`DataFrame` that needs to be combined.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Combined DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This is equivalent to `INTERSECT` in SQL.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      "     |      >>> df1.intersect(df2).sort(df1.C1.desc()).show()\n",
      "     |      +---+---+\n",
      "     |      | C1| C2|\n",
      "     |      +---+---+\n",
      "     |      |  b|  3|\n",
      "     |      |  a|  1|\n",
      "     |      +---+---+\n",
      "     |  \n",
      "     |  intersectAll(self, other: 'DataFrame') -> 'DataFrame'\n",
      "     |      Return a new :class:`DataFrame` containing rows in both this :class:`DataFrame`\n",
      "     |      and another :class:`DataFrame` while preserving duplicates.\n",
      "     |      \n",
      "     |      This is equivalent to `INTERSECT ALL` in SQL. As standard in SQL, this function\n",
      "     |      resolves columns by position (not by name).\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          Another :class:`DataFrame` that needs to be combined.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Combined DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      "     |      >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n",
      "     |      +---+---+\n",
      "     |      | C1| C2|\n",
      "     |      +---+---+\n",
      "     |      |  a|  1|\n",
      "     |      |  a|  1|\n",
      "     |      |  b|  3|\n",
      "     |      +---+---+\n",
      "     |  \n",
      "     |  isEmpty(self) -> bool\n",
      "     |      Checks if the :class:`DataFrame` is empty and returns a boolean value.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          Returns ``True`` if the DataFrame is empty, ``False`` otherwise.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      DataFrame.count : Counts the number of rows in DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      - Unlike `count()`, this method does not trigger any computation.\n",
      "     |      - An empty DataFrame has no rows. It may have columns, but no data.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Example 1: Checking if an empty DataFrame is empty\n",
      "     |      \n",
      "     |      >>> df_empty = spark.createDataFrame([], 'a STRING')\n",
      "     |      >>> df_empty.isEmpty()\n",
      "     |      True\n",
      "     |      \n",
      "     |      Example 2: Checking if a non-empty DataFrame is empty\n",
      "     |      \n",
      "     |      >>> df_non_empty = spark.createDataFrame([\"a\"], 'STRING')\n",
      "     |      >>> df_non_empty.isEmpty()\n",
      "     |      False\n",
      "     |      \n",
      "     |      Example 3: Checking if a DataFrame with null values is empty\n",
      "     |      \n",
      "     |      >>> df_nulls = spark.createDataFrame([(None, None)], 'a STRING, b INT')\n",
      "     |      >>> df_nulls.isEmpty()\n",
      "     |      False\n",
      "     |      \n",
      "     |      Example 4: Checking if a DataFrame with no rows but with columns is empty\n",
      "     |      \n",
      "     |      >>> df_no_rows = spark.createDataFrame([], 'id INT, value STRING')\n",
      "     |      >>> df_no_rows.isEmpty()\n",
      "     |      True\n",
      "     |  \n",
      "     |  isLocal(self) -> bool\n",
      "     |      Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally\n",
      "     |      (without any Spark executors).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.sql(\"SHOW TABLES\")\n",
      "     |      >>> df.isLocal()\n",
      "     |      True\n",
      "     |  \n",
      "     |  join(self, other: 'DataFrame', on: Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], NoneType] = None, how: Optional[str] = None) -> 'DataFrame'\n",
      "     |      Joins with another :class:`DataFrame`, using the given join expression.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          Right side of the join\n",
      "     |      on : str, list or :class:`Column`, optional\n",
      "     |          a string for the join column name, a list of column names,\n",
      "     |          a join expression (Column), or a list of Columns.\n",
      "     |          If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      "     |          the column(s) must exist on both sides, and this performs an equi-join.\n",
      "     |      how : str, optional\n",
      "     |          default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      "     |          ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      "     |          ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      "     |          ``anti``, ``leftanti`` and ``left_anti``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Joined DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      The following performs a full outer join between ``df1`` and ``df2``.\n",
      "     |      \n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> from pyspark.sql.functions import desc\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")]).toDF(\"age\", \"name\")\n",
      "     |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      "     |      >>> df3 = spark.createDataFrame([Row(age=2, name=\"Alice\"), Row(age=5, name=\"Bob\")])\n",
      "     |      >>> df4 = spark.createDataFrame([\n",
      "     |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
      "     |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
      "     |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
      "     |      ...     Row(age=None, height=None, name=None),\n",
      "     |      ... ])\n",
      "     |      \n",
      "     |      Inner join on columns (default)\n",
      "     |      \n",
      "     |      >>> df.join(df2, 'name').select(df.name, df2.height).show()\n",
      "     |      +----+------+\n",
      "     |      |name|height|\n",
      "     |      +----+------+\n",
      "     |      | Bob|    85|\n",
      "     |      +----+------+\n",
      "     |      >>> df.join(df4, ['name', 'age']).select(df.name, df.age).show()\n",
      "     |      +----+---+\n",
      "     |      |name|age|\n",
      "     |      +----+---+\n",
      "     |      | Bob|  5|\n",
      "     |      +----+---+\n",
      "     |      \n",
      "     |      Outer join for both DataFrames on the 'name' column.\n",
      "     |      \n",
      "     |      >>> df.join(df2, df.name == df2.name, 'outer').select(\n",
      "     |      ...     df.name, df2.height).sort(desc(\"name\")).show()\n",
      "     |      +-----+------+\n",
      "     |      | name|height|\n",
      "     |      +-----+------+\n",
      "     |      |  Bob|    85|\n",
      "     |      |Alice|  NULL|\n",
      "     |      | NULL|    80|\n",
      "     |      +-----+------+\n",
      "     |      >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).show()\n",
      "     |      +-----+------+\n",
      "     |      | name|height|\n",
      "     |      +-----+------+\n",
      "     |      |  Tom|    80|\n",
      "     |      |  Bob|    85|\n",
      "     |      |Alice|  NULL|\n",
      "     |      +-----+------+\n",
      "     |      \n",
      "     |      Outer join for both DataFrams with multiple columns.\n",
      "     |      \n",
      "     |      >>> df.join(\n",
      "     |      ...     df3,\n",
      "     |      ...     [df.name == df3.name, df.age == df3.age],\n",
      "     |      ...     'outer'\n",
      "     |      ... ).select(df.name, df3.age).show()\n",
      "     |      +-----+---+\n",
      "     |      | name|age|\n",
      "     |      +-----+---+\n",
      "     |      |Alice|  2|\n",
      "     |      |  Bob|  5|\n",
      "     |      +-----+---+\n",
      "     |  \n",
      "     |  limit(self, num: int) -> 'DataFrame'\n",
      "     |      Limits the result count to the number specified.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      num : int\n",
      "     |          Number of records to return. Will return this number of records\n",
      "     |          or all records if the DataFrame contains less than this number of records.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Subset of the records\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.limit(1).show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      | 14| Tom|\n",
      "     |      +---+----+\n",
      "     |      >>> df.limit(0).show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      +---+----+\n",
      "     |  \n",
      "     |  localCheckpoint(self, eager: bool = True) -> 'DataFrame'\n",
      "     |      Returns a locally checkpointed version of this :class:`DataFrame`. Checkpointing can be\n",
      "     |      used to truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      "     |      iterative algorithms where the plan may grow exponentially. Local checkpoints are\n",
      "     |      stored in the executors using the caching subsystem and therefore they are not reliable.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      eager : bool, optional, default True\n",
      "     |          Whether to checkpoint this :class:`DataFrame` immediately.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Checkpointed DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is experimental.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.localCheckpoint(False)\n",
      "     |      DataFrame[age: bigint, name: string]\n",
      "     |  \n",
      "     |  melt(self, ids: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...]], values: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...], NoneType], variableColumnName: str, valueColumnName: str) -> 'DataFrame'\n",
      "     |      Unpivot a DataFrame from wide format to long format, optionally leaving\n",
      "     |      identifier columns set. This is the reverse to `groupBy(...).pivot(...).agg(...)`,\n",
      "     |      except for the aggregation, which cannot be reversed.\n",
      "     |      \n",
      "     |      :func:`melt` is an alias for :func:`unpivot`.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ids : str, Column, tuple, list, optional\n",
      "     |          Column(s) to use as identifiers. Can be a single column or column name,\n",
      "     |          or a list or tuple for multiple columns.\n",
      "     |      values : str, Column, tuple, list, optional\n",
      "     |          Column(s) to unpivot. Can be a single column or column name, or a list or tuple\n",
      "     |          for multiple columns. If not specified or empty, use all columns that\n",
      "     |          are not set as `ids`.\n",
      "     |      variableColumnName : str\n",
      "     |          Name of the variable column.\n",
      "     |      valueColumnName : str\n",
      "     |          Name of the value column.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Unpivoted DataFrame.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      DataFrame.unpivot\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  observe(self, observation: Union[ForwardRef('Observation'), str], *exprs: pyspark.sql.column.Column) -> 'DataFrame'\n",
      "     |      Define (named) metrics to observe on the DataFrame. This method returns an 'observed'\n",
      "     |      DataFrame that returns the same result as the input, with the following guarantees:\n",
      "     |      \n",
      "     |      * It will compute the defined aggregates (metrics) on all the data that is flowing through\n",
      "     |          the Dataset at that point.\n",
      "     |      \n",
      "     |      * It will report the value of the defined aggregate columns as soon as we reach a completion\n",
      "     |          point. A completion point is either the end of a query (batch mode) or the end of a\n",
      "     |          streaming epoch. The value of the aggregates only reflects the data processed since\n",
      "     |          the previous completion point.\n",
      "     |      \n",
      "     |      The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or\n",
      "     |      more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that\n",
      "     |      contain references to the input Dataset's columns must always be wrapped in an aggregate\n",
      "     |      function.\n",
      "     |      \n",
      "     |      A user can observe these metrics by adding\n",
      "     |      Python's :class:`~pyspark.sql.streaming.StreamingQueryListener`,\n",
      "     |      Scala/Java's ``org.apache.spark.sql.streaming.StreamingQueryListener`` or Scala/Java's\n",
      "     |      ``org.apache.spark.sql.util.QueryExecutionListener`` to the spark session.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      observation : :class:`Observation` or str\n",
      "     |          `str` to specify the name, or an :class:`Observation` instance to obtain the metric.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |             Added support for `str` in this parameter.\n",
      "     |      exprs : :class:`Column`\n",
      "     |          column expressions (:class:`Column`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          the observed :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      When ``observation`` is :class:`Observation`, this method only supports batch queries.\n",
      "     |      When ``observation`` is a string, this method works for both batch and streaming queries.\n",
      "     |      Continuous execution is currently not supported yet.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      When ``observation`` is :class:`Observation`, only batch queries work as below.\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.functions import col, count, lit, max\n",
      "     |      >>> from pyspark.sql import Observation\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> observation = Observation(\"my metrics\")\n",
      "     |      >>> observed_df = df.observe(observation, count(lit(1)).alias(\"count\"), max(col(\"age\")))\n",
      "     |      >>> observed_df.count()\n",
      "     |      2\n",
      "     |      >>> observation.get\n",
      "     |      {'count': 2, 'max(age)': 5}\n",
      "     |      \n",
      "     |      When ``observation`` is a string, streaming queries also work as below.\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.streaming import StreamingQueryListener\n",
      "     |      >>> class MyErrorListener(StreamingQueryListener):\n",
      "     |      ...    def onQueryStarted(self, event):\n",
      "     |      ...        pass\n",
      "     |      ...\n",
      "     |      ...    def onQueryProgress(self, event):\n",
      "     |      ...        row = event.progress.observedMetrics.get(\"my_event\")\n",
      "     |      ...        # Trigger if the number of errors exceeds 5 percent\n",
      "     |      ...        num_rows = row.rc\n",
      "     |      ...        num_error_rows = row.erc\n",
      "     |      ...        ratio = num_error_rows / num_rows\n",
      "     |      ...        if ratio > 0.05:\n",
      "     |      ...            # Trigger alert\n",
      "     |      ...            pass\n",
      "     |      ...\n",
      "     |      ...    def onQueryIdle(self, event):\n",
      "     |      ...        pass\n",
      "     |      ...\n",
      "     |      ...    def onQueryTerminated(self, event):\n",
      "     |      ...        pass\n",
      "     |      ...\n",
      "     |      >>> spark.streams.addListener(MyErrorListener())\n",
      "     |      >>> # Observe row count (rc) and error row count (erc) in the streaming Dataset\n",
      "     |      ... observed_ds = df.observe(\n",
      "     |      ...     \"my_event\",\n",
      "     |      ...     count(lit(1)).alias(\"rc\"),\n",
      "     |      ...     count(col(\"error\")).alias(\"erc\"))  # doctest: +SKIP\n",
      "     |      >>> observed_ds.writeStream.format(\"console\").start()  # doctest: +SKIP\n",
      "     |  \n",
      "     |  offset(self, num: int) -> 'DataFrame'\n",
      "     |      Returns a new :class: `DataFrame` by skipping the first `n` rows.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports vanilla PySpark.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      num : int\n",
      "     |          Number of records to skip.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Subset of the records\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.offset(1).show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      | 23|Alice|\n",
      "     |      | 16|  Bob|\n",
      "     |      +---+-----+\n",
      "     |      >>> df.offset(10).show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      +---+----+\n",
      "     |  \n",
      "     |  orderBy = sort(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
      "     |  \n",
      "     |  pandas_api(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
      "     |      Converts the existing DataFrame into a pandas-on-Spark DataFrame.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.2.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      If a pandas-on-Spark DataFrame is converted to a Spark DataFrame and then back\n",
      "     |      to pandas-on-Spark, it will lose the index information and the original index\n",
      "     |      will be turned into a normal column.\n",
      "     |      \n",
      "     |      This is only available if Pandas is installed and available.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      index_col: str or list of str, optional, default: None\n",
      "     |          Index column of table in Spark.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`PandasOnSparkDataFrame`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.pandas.frame.DataFrame.to_spark\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      >>> df.pandas_api()  # doctest: +SKIP\n",
      "     |         age   name\n",
      "     |      0   14    Tom\n",
      "     |      1   23  Alice\n",
      "     |      2   16    Bob\n",
      "     |      \n",
      "     |      We can specify the index columns.\n",
      "     |      \n",
      "     |      >>> df.pandas_api(index_col=\"age\")  # doctest: +SKIP\n",
      "     |            name\n",
      "     |      age\n",
      "     |      14     Tom\n",
      "     |      23   Alice\n",
      "     |      16     Bob\n",
      "     |  \n",
      "     |  persist(self, storageLevel: pyspark.storagelevel.StorageLevel = StorageLevel(True, True, False, True, 1)) -> 'DataFrame'\n",
      "     |      Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
      "     |      operations after the first time it is computed. This can only be used to assign\n",
      "     |      a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
      "     |      If no storage level is specified defaults to (`MEMORY_AND_DISK_DESER`)\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      storageLevel : :class:`StorageLevel`\n",
      "     |          Storage level to set for persistence. Default is MEMORY_AND_DISK_DESER.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Persisted DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.range(1)\n",
      "     |      >>> df.persist()\n",
      "     |      DataFrame[id: bigint]\n",
      "     |      \n",
      "     |      >>> df.explain()\n",
      "     |      == Physical Plan ==\n",
      "     |      AdaptiveSparkPlan isFinalPlan=false\n",
      "     |      +- InMemoryTableScan ...\n",
      "     |      \n",
      "     |      Persists the data in the disk by specifying the storage level.\n",
      "     |      \n",
      "     |      >>> from pyspark.storagelevel import StorageLevel\n",
      "     |      >>> df.persist(StorageLevel.DISK_ONLY)\n",
      "     |      DataFrame[id: bigint]\n",
      "     |  \n",
      "     |  printSchema(self, level: Optional[int] = None) -> None\n",
      "     |      Prints out the schema in the tree format.\n",
      "     |      Optionally allows to specify how many levels to print if schema is nested.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      level : int, optional, default None\n",
      "     |          How many levels to print for nested schemas.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.5.0\n",
      "     |              Added Level parameter.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.printSchema()\n",
      "     |      root\n",
      "     |       |-- age: long (nullable = true)\n",
      "     |       |-- name: string (nullable = true)\n",
      "     |      \n",
      "     |      >>> df = spark.createDataFrame([(1, (2,2))], [\"a\", \"b\"])\n",
      "     |      >>> df.printSchema(1)\n",
      "     |      root\n",
      "     |       |-- a: long (nullable = true)\n",
      "     |       |-- b: struct (nullable = true)\n",
      "     |      \n",
      "     |      >>> df.printSchema(2)\n",
      "     |      root\n",
      "     |       |-- a: long (nullable = true)\n",
      "     |       |-- b: struct (nullable = true)\n",
      "     |       |    |-- _1: long (nullable = true)\n",
      "     |       |    |-- _2: long (nullable = true)\n",
      "     |  \n",
      "     |  randomSplit(self, weights: List[float], seed: Optional[int] = None) -> List[ForwardRef('DataFrame')]\n",
      "     |      Randomly splits this :class:`DataFrame` with the provided weights.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      weights : list\n",
      "     |          list of doubles as weights with which to split the :class:`DataFrame`.\n",
      "     |          Weights will be normalized if they don't sum up to 1.0.\n",
      "     |      seed : int, optional\n",
      "     |          The seed for sampling.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          List of DataFrames.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
      "     |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
      "     |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
      "     |      ...     Row(age=None, height=None, name=None),\n",
      "     |      ... ])\n",
      "     |      \n",
      "     |      >>> splits = df.randomSplit([1.0, 2.0], 24)\n",
      "     |      >>> splits[0].count()\n",
      "     |      2\n",
      "     |      >>> splits[1].count()\n",
      "     |      2\n",
      "     |  \n",
      "     |  registerTempTable(self, name: str) -> None\n",
      "     |      Registers this :class:`DataFrame` as a temporary table using the given name.\n",
      "     |      \n",
      "     |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      "     |      that was used to create this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      .. deprecated:: 2.0.0\n",
      "     |          Use :meth:`DataFrame.createOrReplaceTempView` instead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          Name of the temporary table to register.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.registerTempTable(\"people\")\n",
      "     |      >>> df2 = spark.sql(\"SELECT * FROM people\")\n",
      "     |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |      >>> spark.catalog.dropTempView(\"people\")\n",
      "     |      True\n",
      "     |  \n",
      "     |  repartition(self, numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      "     |      resulting :class:`DataFrame` is hash partitioned.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      numPartitions : int\n",
      "     |          can be an int to specify the target number of partitions or a Column.\n",
      "     |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      "     |          the default number of partitions is used.\n",
      "     |      cols : str or :class:`Column`\n",
      "     |          partitioning columns.\n",
      "     |      \n",
      "     |          .. versionchanged:: 1.6.0\n",
      "     |             Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
      "     |             optional if partitioning columns are specified.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Repartitioned DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Repartition the data into 10 partitions.\n",
      "     |      \n",
      "     |      >>> df.repartition(10).rdd.getNumPartitions()\n",
      "     |      10\n",
      "     |      \n",
      "     |      Repartition the data into 7 partitions by 'age' column.\n",
      "     |      \n",
      "     |      >>> df.repartition(7, \"age\").rdd.getNumPartitions()\n",
      "     |      7\n",
      "     |      \n",
      "     |      Repartition the data into 7 partitions by 'age' and 'name columns.\n",
      "     |      \n",
      "     |      >>> df.repartition(3, \"name\", \"age\").rdd.getNumPartitions()\n",
      "     |      3\n",
      "     |  \n",
      "     |  repartitionByRange(self, numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      "     |      resulting :class:`DataFrame` is range partitioned.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      numPartitions : int\n",
      "     |          can be an int to specify the target number of partitions or a Column.\n",
      "     |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      "     |          the default number of partitions is used.\n",
      "     |      cols : str or :class:`Column`\n",
      "     |          partitioning columns.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Repartitioned DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      At least one partition-by expression must be specified.\n",
      "     |      When no explicit sort order is specified, \"ascending nulls first\" is assumed.\n",
      "     |      \n",
      "     |      Due to performance reasons this method uses sampling to estimate the ranges.\n",
      "     |      Hence, the output may not be consistent, since sampling can return different values.\n",
      "     |      The sample size can be controlled by the config\n",
      "     |      `spark.sql.execution.rangeExchange.sampleSizePerPartition`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Repartition the data into 2 partitions by range in 'age' column.\n",
      "     |      For example, the first partition can have ``(14, \"Tom\")``, and the second\n",
      "     |      partition would have ``(16, \"Bob\")`` and ``(23, \"Alice\")``.\n",
      "     |      \n",
      "     |      >>> df.repartitionByRange(2, \"age\").rdd.getNumPartitions()\n",
      "     |      2\n",
      "     |  \n",
      "     |  replace(self, to_replace: Union[ForwardRef('LiteralType'), List[ForwardRef('LiteralType')], Dict[ForwardRef('LiteralType'), ForwardRef('OptionalPrimitiveType')]], value: Union[ForwardRef('OptionalPrimitiveType'), List[ForwardRef('OptionalPrimitiveType')], pyspark._globals._NoValueType, NoneType] = <no value>, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      "     |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      "     |      aliases of each other.\n",
      "     |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      "     |      or strings. Value can have None. When replacing, the new value will be cast\n",
      "     |      to the type of the existing column.\n",
      "     |      For numeric replacements all values to be replaced should have unique\n",
      "     |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      "     |      and arbitrary replacement will be used.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      to_replace : bool, int, float, string, list or dict\n",
      "     |          Value to be replaced.\n",
      "     |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      "     |          must be a mapping between a value and a replacement.\n",
      "     |      value : bool, int, float, string or None, optional\n",
      "     |          The replacement value must be a bool, int, float, string or None. If `value` is a\n",
      "     |          list, `value` should be of the same length and type as `to_replace`.\n",
      "     |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      "     |          used as a replacement for each item in `to_replace`.\n",
      "     |      subset : list, optional\n",
      "     |          optional list of column names to consider.\n",
      "     |          Columns specified in subset that do not have matching data types are ignored.\n",
      "     |          For example, if `value` is a string, and subset contains a non-string column,\n",
      "     |          then the non-string column is simply ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with replaced values.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (10, 80, \"Alice\"),\n",
      "     |      ...     (5, None, \"Bob\"),\n",
      "     |      ...     (None, 10, \"Tom\"),\n",
      "     |      ...     (None, None, None)],\n",
      "     |      ...     schema=[\"age\", \"height\", \"name\"])\n",
      "     |      \n",
      "     |      Replace 10 to 20 in all columns.\n",
      "     |      \n",
      "     |      >>> df.na.replace(10, 20).show()\n",
      "     |      +----+------+-----+\n",
      "     |      | age|height| name|\n",
      "     |      +----+------+-----+\n",
      "     |      |  20|    80|Alice|\n",
      "     |      |   5|  NULL|  Bob|\n",
      "     |      |NULL|    20|  Tom|\n",
      "     |      |NULL|  NULL| NULL|\n",
      "     |      +----+------+-----+\n",
      "     |      \n",
      "     |      Replace 'Alice' to null in all columns.\n",
      "     |      \n",
      "     |      >>> df.na.replace('Alice', None).show()\n",
      "     |      +----+------+----+\n",
      "     |      | age|height|name|\n",
      "     |      +----+------+----+\n",
      "     |      |  10|    80|NULL|\n",
      "     |      |   5|  NULL| Bob|\n",
      "     |      |NULL|    10| Tom|\n",
      "     |      |NULL|  NULL|NULL|\n",
      "     |      +----+------+----+\n",
      "     |      \n",
      "     |      Replace 'Alice' to 'A', and 'Bob' to 'B' in the 'name' column.\n",
      "     |      \n",
      "     |      >>> df.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      "     |      +----+------+----+\n",
      "     |      | age|height|name|\n",
      "     |      +----+------+----+\n",
      "     |      |  10|    80|   A|\n",
      "     |      |   5|  NULL|   B|\n",
      "     |      |NULL|    10| Tom|\n",
      "     |      |NULL|  NULL|NULL|\n",
      "     |      +----+------+----+\n",
      "     |  \n",
      "     |  rollup(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
      "     |      Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
      "     |      the specified columns, so we can run aggregation on them.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : list, str or :class:`Column`\n",
      "     |          Columns to roll-up by.\n",
      "     |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
      "     |          or list of them.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`GroupedData`\n",
      "     |          Rolled-up data by given columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      "     |      +-----+----+-----+\n",
      "     |      | name| age|count|\n",
      "     |      +-----+----+-----+\n",
      "     |      | NULL|NULL|    2|\n",
      "     |      |Alice|NULL|    1|\n",
      "     |      |Alice|   2|    1|\n",
      "     |      |  Bob|NULL|    1|\n",
      "     |      |  Bob|   5|    1|\n",
      "     |      +-----+----+-----+\n",
      "     |  \n",
      "     |  sameSemantics(self, other: 'DataFrame') -> bool\n",
      "     |      Returns `True` when the logical query plans inside both :class:`DataFrame`\\s are equal and\n",
      "     |      therefore return the same results.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The equality comparison here is simplified by tolerating the cosmetic differences\n",
      "     |      such as attribute names.\n",
      "     |      \n",
      "     |      This API can compare both :class:`DataFrame`\\s very fast but can still return\n",
      "     |      `False` on the :class:`DataFrame` that return the same results, for instance, from\n",
      "     |      different plans. Such false negative semantic can be useful when caching as an example.\n",
      "     |      \n",
      "     |      This API is a developer API.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          The other DataFrame to compare against.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          Whether these two DataFrames are similar.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df1 = spark.range(10)\n",
      "     |      >>> df2 = spark.range(10)\n",
      "     |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id * 2))\n",
      "     |      True\n",
      "     |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id + 2))\n",
      "     |      False\n",
      "     |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col0\", df2.id * 2))\n",
      "     |      True\n",
      "     |  \n",
      "     |  sample(self, withReplacement: Union[float, bool, NoneType] = None, fraction: Union[int, float, NoneType] = None, seed: Optional[int] = None) -> 'DataFrame'\n",
      "     |      Returns a sampled subset of this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      withReplacement : bool, optional\n",
      "     |          Sample with replacement or not (default ``False``).\n",
      "     |      fraction : float, optional\n",
      "     |          Fraction of rows to generate, range [0.0, 1.0].\n",
      "     |      seed : int, optional\n",
      "     |          Seed for sampling (default a random seed).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Sampled rows from given DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This is not guaranteed to provide exactly the fraction specified of the total\n",
      "     |      count of the given :class:`DataFrame`.\n",
      "     |      \n",
      "     |      `fraction` is required and, `withReplacement` and `seed` are optional.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.range(10)\n",
      "     |      >>> df.sample(0.5, 3).count() # doctest: +SKIP\n",
      "     |      7\n",
      "     |      >>> df.sample(fraction=0.5, seed=3).count() # doctest: +SKIP\n",
      "     |      7\n",
      "     |      >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count() # doctest: +SKIP\n",
      "     |      1\n",
      "     |      >>> df.sample(1.0).count()\n",
      "     |      10\n",
      "     |      >>> df.sample(fraction=1.0).count()\n",
      "     |      10\n",
      "     |      >>> df.sample(False, fraction=1.0).count()\n",
      "     |      10\n",
      "     |  \n",
      "     |  sampleBy(self, col: 'ColumnOrName', fractions: Dict[Any, float], seed: Optional[int] = None) -> 'DataFrame'\n",
      "     |      Returns a stratified sample without replacement based on the\n",
      "     |      fraction given on each stratum.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col : :class:`Column` or str\n",
      "     |          column that defines strata\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.0.0\n",
      "     |             Added sampling by a column of :class:`Column`\n",
      "     |      fractions : dict\n",
      "     |          sampling fraction for each stratum. If a stratum is not\n",
      "     |          specified, we treat its fraction as zero.\n",
      "     |      seed : int, optional\n",
      "     |          random seed\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a new :class:`DataFrame` that represents the stratified sample\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import col\n",
      "     |      >>> dataset = spark.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
      "     |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
      "     |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
      "     |      +---+-----+\n",
      "     |      |key|count|\n",
      "     |      +---+-----+\n",
      "     |      |  0|    3|\n",
      "     |      |  1|    6|\n",
      "     |      +---+-----+\n",
      "     |      >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
      "     |      33\n",
      "     |  \n",
      "     |  select(self, *cols: 'ColumnOrName') -> 'DataFrame'\n",
      "     |      Projects a set of expressions and returns a new :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, :class:`Column`, or list\n",
      "     |          column names (string) or expressions (:class:`Column`).\n",
      "     |          If one of the column names is '*', that column is expanded to include all columns\n",
      "     |          in the current :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          A DataFrame with subset (or all) of columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Select all columns in the DataFrame.\n",
      "     |      \n",
      "     |      >>> df.select('*').show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  2|Alice|\n",
      "     |      |  5|  Bob|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Select a column with other expressions in the DataFrame.\n",
      "     |      \n",
      "     |      >>> df.select(df.name, (df.age + 10).alias('age')).show()\n",
      "     |      +-----+---+\n",
      "     |      | name|age|\n",
      "     |      +-----+---+\n",
      "     |      |Alice| 12|\n",
      "     |      |  Bob| 15|\n",
      "     |      +-----+---+\n",
      "     |  \n",
      "     |  selectExpr(self, *expr: Union[str, List[str]]) -> 'DataFrame'\n",
      "     |      Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
      "     |      \n",
      "     |      This is a variant of :func:`select` that accepts SQL expressions.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          A DataFrame with new/old columns transformed by expressions.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.selectExpr(\"age * 2\", \"abs(age)\").show()\n",
      "     |      +---------+--------+\n",
      "     |      |(age * 2)|abs(age)|\n",
      "     |      +---------+--------+\n",
      "     |      |        4|       2|\n",
      "     |      |       10|       5|\n",
      "     |      +---------+--------+\n",
      "     |  \n",
      "     |  semanticHash(self) -> int\n",
      "     |      Returns a hash code of the logical query plan against this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Unlike the standard hash code, the hash is calculated against the query plan\n",
      "     |      simplified by tolerating the cosmetic differences such as attribute names.\n",
      "     |      \n",
      "     |      This API is a developer API.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          Hash value.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.range(10).selectExpr(\"id as col0\").semanticHash()  # doctest: +SKIP\n",
      "     |      1855039936\n",
      "     |      >>> spark.range(10).selectExpr(\"id as col1\").semanticHash()  # doctest: +SKIP\n",
      "     |      1855039936\n",
      "     |  \n",
      "     |  show(self, n: int = 20, truncate: Union[bool, int] = True, vertical: bool = False) -> None\n",
      "     |      Prints the first ``n`` rows to the console.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int, optional\n",
      "     |          Number of rows to show.\n",
      "     |      truncate : bool or int, optional\n",
      "     |          If set to ``True``, truncate strings longer than 20 chars by default.\n",
      "     |          If set to a number greater than one, truncates long strings to length ``truncate``\n",
      "     |          and align cells right.\n",
      "     |      vertical : bool, optional\n",
      "     |          If set to ``True``, print output rows vertically (one line\n",
      "     |          per column value).\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Show only top 2 rows.\n",
      "     |      \n",
      "     |      >>> df.show(2)\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      | 14|  Tom|\n",
      "     |      | 23|Alice|\n",
      "     |      +---+-----+\n",
      "     |      only showing top 2 rows\n",
      "     |      \n",
      "     |      Show :class:`DataFrame` where the maximum number of characters is 3.\n",
      "     |      \n",
      "     |      >>> df.show(truncate=3)\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      | 14| Tom|\n",
      "     |      | 23| Ali|\n",
      "     |      | 16| Bob|\n",
      "     |      +---+----+\n",
      "     |      \n",
      "     |      Show :class:`DataFrame` vertically.\n",
      "     |      \n",
      "     |      >>> df.show(vertical=True)\n",
      "     |      -RECORD 0-----\n",
      "     |      age  | 14\n",
      "     |      name | Tom\n",
      "     |      -RECORD 1-----\n",
      "     |      age  | 23\n",
      "     |      name | Alice\n",
      "     |      -RECORD 2-----\n",
      "     |      age  | 16\n",
      "     |      name | Bob\n",
      "     |  \n",
      "     |  sort(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, list, or :class:`Column`, optional\n",
      "     |           list of :class:`Column` or column names to sort by.\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      ascending : bool or list, optional, default True\n",
      "     |          boolean or list of boolean.\n",
      "     |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      "     |          If a list is specified, the length of the list must equal the length of the `cols`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Sorted DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import desc, asc\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Sort the DataFrame in ascending order.\n",
      "     |      \n",
      "     |      >>> df.sort(asc(\"age\")).show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  2|Alice|\n",
      "     |      |  5|  Bob|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Sort the DataFrame in descending order.\n",
      "     |      \n",
      "     |      >>> df.sort(df.age.desc()).show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  5|  Bob|\n",
      "     |      |  2|Alice|\n",
      "     |      +---+-----+\n",
      "     |      >>> df.orderBy(df.age.desc()).show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  5|  Bob|\n",
      "     |      |  2|Alice|\n",
      "     |      +---+-----+\n",
      "     |      >>> df.sort(\"age\", ascending=False).show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  5|  Bob|\n",
      "     |      |  2|Alice|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Specify multiple columns\n",
      "     |      \n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.orderBy(desc(\"age\"), \"name\").show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  5|  Bob|\n",
      "     |      |  2|Alice|\n",
      "     |      |  2|  Bob|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Specify multiple columns for sorting order at `ascending`.\n",
      "     |      \n",
      "     |      >>> df.orderBy([\"age\", \"name\"], ascending=[False, False]).show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  5|  Bob|\n",
      "     |      |  2|  Bob|\n",
      "     |      |  2|Alice|\n",
      "     |      +---+-----+\n",
      "     |  \n",
      "     |  sortWithinPartitions(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, list or :class:`Column`, optional\n",
      "     |          list of :class:`Column` or column names to sort by.\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      ascending : bool or list, optional, default True\n",
      "     |          boolean or list of boolean.\n",
      "     |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      "     |          If a list is specified, the length of the list must equal the length of the `cols`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame sorted by partitions.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.sortWithinPartitions(\"age\", ascending=False)\n",
      "     |      DataFrame[age: bigint, name: string]\n",
      "     |  \n",
      "     |  subtract(self, other: 'DataFrame') -> 'DataFrame'\n",
      "     |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n",
      "     |      but not in another :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          Another :class:`DataFrame` that needs to be subtracted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Subtracted DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      "     |      >>> df1.subtract(df2).show()\n",
      "     |      +---+---+\n",
      "     |      | C1| C2|\n",
      "     |      +---+---+\n",
      "     |      |  c|  4|\n",
      "     |      +---+---+\n",
      "     |  \n",
      "     |  summary(self, *statistics: str) -> 'DataFrame'\n",
      "     |      Computes specified statistics for numeric and string columns. Available statistics are:\n",
      "     |      - count\n",
      "     |      - mean\n",
      "     |      - stddev\n",
      "     |      - min\n",
      "     |      - max\n",
      "     |      - arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n",
      "     |      \n",
      "     |      If no statistics are given, this function computes count, mean, stddev, min,\n",
      "     |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      statistics : str, optional\n",
      "     |           Column names to calculate statistics by (default All columns).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          A new DataFrame that provides statistics for the given DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This function is meant for exploratory data analysis, as we make no\n",
      "     |      guarantee about the backward compatibility of the schema of the resulting\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
      "     |      ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
      "     |      ... )\n",
      "     |      >>> df.select(\"age\", \"weight\", \"height\").summary().show()\n",
      "     |      +-------+----+------------------+-----------------+\n",
      "     |      |summary| age|            weight|           height|\n",
      "     |      +-------+----+------------------+-----------------+\n",
      "     |      |  count|   3|                 3|                3|\n",
      "     |      |   mean|12.0| 40.73333333333333|            145.0|\n",
      "     |      | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
      "     |      |    min|  11|              37.8|            142.2|\n",
      "     |      |    25%|  11|              37.8|            142.2|\n",
      "     |      |    50%|  12|              40.3|            142.3|\n",
      "     |      |    75%|  13|              44.1|            150.5|\n",
      "     |      |    max|  13|              44.1|            150.5|\n",
      "     |      +-------+----+------------------+-----------------+\n",
      "     |      \n",
      "     |      >>> df.select(\"age\", \"weight\", \"height\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
      "     |      +-------+---+------+------+\n",
      "     |      |summary|age|weight|height|\n",
      "     |      +-------+---+------+------+\n",
      "     |      |  count|  3|     3|     3|\n",
      "     |      |    min| 11|  37.8| 142.2|\n",
      "     |      |    25%| 11|  37.8| 142.2|\n",
      "     |      |    75%| 13|  44.1| 150.5|\n",
      "     |      |    max| 13|  44.1| 150.5|\n",
      "     |      +-------+---+------+------+\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      DataFrame.display\n",
      "     |  \n",
      "     |  tail(self, num: int) -> List[pyspark.sql.types.Row]\n",
      "     |      Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n",
      "     |      \n",
      "     |      Running tail requires moving data into the application's driver process, and doing so with\n",
      "     |      a very large ``num`` can crash the driver process with OutOfMemoryError.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      num : int\n",
      "     |          Number of records to return. Will return this number of records\n",
      "     |          or all records if the DataFrame contains less than this number of records.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          List of rows\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      >>> df.tail(2)\n",
      "     |      [Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
      "     |  \n",
      "     |  take(self, num: int) -> List[pyspark.sql.types.Row]\n",
      "     |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      num : int\n",
      "     |          Number of records to return. Will return this number of records\n",
      "     |          or all records if the DataFrame contains less than this number of records..\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          List of rows\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Return the first 2 rows of the :class:`DataFrame`.\n",
      "     |      \n",
      "     |      >>> df.take(2)\n",
      "     |      [Row(age=14, name='Tom'), Row(age=23, name='Alice')]\n",
      "     |  \n",
      "     |  to(self, schema: pyspark.sql.types.StructType) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` where each row is reconciled to match the specified\n",
      "     |      schema.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      schema : :class:`StructType`\n",
      "     |          Specified schema.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Reconciled DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      * Reorder columns and/or inner fields by name to match the specified schema.\n",
      "     |      \n",
      "     |      * Project away columns and/or inner fields that are not needed by the specified schema.\n",
      "     |          Missing columns and/or inner fields (present in the specified schema but not input\n",
      "     |          DataFrame) lead to failures.\n",
      "     |      \n",
      "     |      * Cast the columns and/or inner fields to match the data types in the specified schema,\n",
      "     |          if the types are compatible, e.g., numeric to numeric (error if overflows), but\n",
      "     |          not string to int.\n",
      "     |      \n",
      "     |      * Carry over the metadata from the specified schema, while the columns and/or inner fields\n",
      "     |          still keep their own metadata if not overwritten by the specified schema.\n",
      "     |      \n",
      "     |      * Fail if the nullability is not compatible. For example, the column and/or inner field\n",
      "     |          is nullable but the specified schema requires them to be not nullable.\n",
      "     |      \n",
      "     |      Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.types import StructField, StringType\n",
      "     |      >>> df = spark.createDataFrame([(\"a\", 1)], [\"i\", \"j\"])\n",
      "     |      >>> df.schema\n",
      "     |      StructType([StructField('i', StringType(), True), StructField('j', LongType(), True)])\n",
      "     |      \n",
      "     |      >>> schema = StructType([StructField(\"j\", StringType()), StructField(\"i\", StringType())])\n",
      "     |      >>> df2 = df.to(schema)\n",
      "     |      >>> df2.schema\n",
      "     |      StructType([StructField('j', StringType(), True), StructField('i', StringType(), True)])\n",
      "     |      >>> df2.show()\n",
      "     |      +---+---+\n",
      "     |      |  j|  i|\n",
      "     |      +---+---+\n",
      "     |      |  1|  a|\n",
      "     |      +---+---+\n",
      "     |  \n",
      "     |  toDF(self, *cols: str) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` that with new specified column names\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      *cols : tuple\n",
      "     |          a tuple of string new column name. The length of the\n",
      "     |          list needs to be the same as the number of columns in the initial\n",
      "     |          :class:`DataFrame`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with new column names.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(14, \"Tom\"), (23, \"Alice\"),\n",
      "     |      ...     (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.toDF('f1', 'f2').show()\n",
      "     |      +---+-----+\n",
      "     |      | f1|   f2|\n",
      "     |      +---+-----+\n",
      "     |      | 14|  Tom|\n",
      "     |      | 23|Alice|\n",
      "     |      | 16|  Bob|\n",
      "     |      +---+-----+\n",
      "     |  \n",
      "     |  toJSON(self, use_unicode: bool = True) -> pyspark.rdd.RDD[str]\n",
      "     |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n",
      "     |      \n",
      "     |      Each row is turned into a JSON document as one element in the returned RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      use_unicode : bool, optional, default True\n",
      "     |          Whether to convert to unicode or not.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.toJSON().first()\n",
      "     |      '{\"age\":2,\"name\":\"Alice\"}'\n",
      "     |  \n",
      "     |  toLocalIterator(self, prefetchPartitions: bool = False) -> Iterator[pyspark.sql.types.Row]\n",
      "     |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n",
      "     |      The iterator will consume as much memory as the largest partition in this\n",
      "     |      :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n",
      "     |      partitions.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      prefetchPartitions : bool, optional\n",
      "     |          If Spark should pre-fetch the next partition before it is needed.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.4.0\n",
      "     |              This argument does not take effect for Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Iterator\n",
      "     |          Iterator of rows.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> list(df.toLocalIterator())\n",
      "     |      [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
      "     |  \n",
      "     |  to_koalas(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
      "     |      # Keep to_koalas for backward compatibility for now.\n",
      "     |  \n",
      "     |  to_pandas_on_spark(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
      "     |      # Keep to_pandas_on_spark for backward compatibility for now.\n",
      "     |  \n",
      "     |  transform(self, func: Callable[..., ForwardRef('DataFrame')], *args: Any, **kwargs: Any) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : function\n",
      "     |          a function that takes and returns a :class:`DataFrame`.\n",
      "     |      *args\n",
      "     |          Positional arguments to pass to func.\n",
      "     |      \n",
      "     |          .. versionadded:: 3.3.0\n",
      "     |      **kwargs\n",
      "     |          Keyword arguments to pass to func.\n",
      "     |      \n",
      "     |          .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Transformed DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import col\n",
      "     |      >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
      "     |      >>> def cast_all_to_int(input_df):\n",
      "     |      ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
      "     |      ...\n",
      "     |      >>> def sort_columns_asc(input_df):\n",
      "     |      ...     return input_df.select(*sorted(input_df.columns))\n",
      "     |      ...\n",
      "     |      >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n",
      "     |      +-----+---+\n",
      "     |      |float|int|\n",
      "     |      +-----+---+\n",
      "     |      |    1|  1|\n",
      "     |      |    2|  2|\n",
      "     |      +-----+---+\n",
      "     |      \n",
      "     |      >>> def add_n(input_df, n):\n",
      "     |      ...     return input_df.select([(col(col_name) + n).alias(col_name)\n",
      "     |      ...                             for col_name in input_df.columns])\n",
      "     |      >>> df.transform(add_n, 1).transform(add_n, n=10).show()\n",
      "     |      +---+-----+\n",
      "     |      |int|float|\n",
      "     |      +---+-----+\n",
      "     |      | 12| 12.0|\n",
      "     |      | 13| 13.0|\n",
      "     |      +---+-----+\n",
      "     |  \n",
      "     |  union(self, other: 'DataFrame') -> 'DataFrame'\n",
      "     |      Return a new :class:`DataFrame` containing the union of rows in this and another\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          Another :class:`DataFrame` that needs to be unioned.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          A new :class:`DataFrame` containing the combined rows with corresponding columns.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      DataFrame.unionAll\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method performs a SQL-style set union of the rows from both `DataFrame` objects,\n",
      "     |      with no automatic deduplication of elements.\n",
      "     |      \n",
      "     |      Use the `distinct()` method to perform deduplication of rows.\n",
      "     |      \n",
      "     |      The method resolves columns by position (not by name), following the standard behavior\n",
      "     |      in SQL.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Example 1: Combining two DataFrames with the same schema\n",
      "     |      \n",
      "     |      >>> df1 = spark.createDataFrame([(1, 'A'), (2, 'B')], ['id', 'value'])\n",
      "     |      >>> df2 = spark.createDataFrame([(3, 'C'), (4, 'D')], ['id', 'value'])\n",
      "     |      >>> df3 = df1.union(df2)\n",
      "     |      >>> df3.show()\n",
      "     |      +---+-----+\n",
      "     |      | id|value|\n",
      "     |      +---+-----+\n",
      "     |      |  1|    A|\n",
      "     |      |  2|    B|\n",
      "     |      |  3|    C|\n",
      "     |      |  4|    D|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Example 2: Combining two DataFrames with different schemas\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.functions import lit\n",
      "     |      >>> df1 = spark.createDataFrame([(\"Alice\", 1), (\"Bob\", 2)], [\"name\", \"id\"])\n",
      "     |      >>> df2 = spark.createDataFrame([(3, \"Charlie\"), (4, \"Dave\")], [\"id\", \"name\"])\n",
      "     |      >>> df1 = df1.withColumn(\"age\", lit(30))\n",
      "     |      >>> df2 = df2.withColumn(\"age\", lit(40))\n",
      "     |      >>> df3 = df1.union(df2)\n",
      "     |      >>> df3.show()\n",
      "     |      +-----+-------+---+\n",
      "     |      | name|     id|age|\n",
      "     |      +-----+-------+---+\n",
      "     |      |Alice|      1| 30|\n",
      "     |      |  Bob|      2| 30|\n",
      "     |      |    3|Charlie| 40|\n",
      "     |      |    4|   Dave| 40|\n",
      "     |      +-----+-------+---+\n",
      "     |      \n",
      "     |      Example 3: Combining two DataFrames with mismatched columns\n",
      "     |      \n",
      "     |      >>> df1 = spark.createDataFrame([(1, 2)], [\"A\", \"B\"])\n",
      "     |      >>> df2 = spark.createDataFrame([(3, 4)], [\"C\", \"D\"])\n",
      "     |      >>> df3 = df1.union(df2)\n",
      "     |      >>> df3.show()\n",
      "     |      +---+---+\n",
      "     |      |  A|  B|\n",
      "     |      +---+---+\n",
      "     |      |  1|  2|\n",
      "     |      |  3|  4|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      Example 4: Combining duplicate rows from two different DataFrames\n",
      "     |      \n",
      "     |      >>> df1 = spark.createDataFrame([(1, 'A'), (2, 'B'), (3, 'C')], ['id', 'value'])\n",
      "     |      >>> df2 = spark.createDataFrame([(3, 'C'), (4, 'D')], ['id', 'value'])\n",
      "     |      >>> df3 = df1.union(df2).distinct().sort(\"id\")\n",
      "     |      >>> df3.show()\n",
      "     |      +---+-----+\n",
      "     |      | id|value|\n",
      "     |      +---+-----+\n",
      "     |      |  1|    A|\n",
      "     |      |  2|    B|\n",
      "     |      |  3|    C|\n",
      "     |      |  4|    D|\n",
      "     |      +---+-----+\n",
      "     |  \n",
      "     |  unionAll(self, other: 'DataFrame') -> 'DataFrame'\n",
      "     |      Return a new :class:`DataFrame` containing the union of rows in this and another\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          Another :class:`DataFrame` that needs to be combined\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          A new :class:`DataFrame` containing combined rows from both dataframes.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method combines all rows from both `DataFrame` objects with no automatic\n",
      "     |      deduplication of elements.\n",
      "     |      \n",
      "     |      Use the `distinct()` method to perform deduplication of rows.\n",
      "     |      \n",
      "     |      :func:`unionAll` is an alias to :func:`union`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      DataFrame.union\n",
      "     |  \n",
      "     |  unionByName(self, other: 'DataFrame', allowMissingColumns: bool = False) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` containing union of rows in this and another\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      This method performs a union operation on both input DataFrames, resolving columns by\n",
      "     |      name (rather than position). When `allowMissingColumns` is True, missing columns will\n",
      "     |      be filled with null.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          Another :class:`DataFrame` that needs to be combined.\n",
      "     |      allowMissingColumns : bool, optional, default False\n",
      "     |         Specify whether to allow missing columns.\n",
      "     |      \n",
      "     |         .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          A new :class:`DataFrame` containing the combined rows with corresponding\n",
      "     |          columns of the two given DataFrames.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Example 1: Union of two DataFrames with same columns in different order.\n",
      "     |      \n",
      "     |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
      "     |      >>> df1.unionByName(df2).show()\n",
      "     |      +----+----+----+\n",
      "     |      |col0|col1|col2|\n",
      "     |      +----+----+----+\n",
      "     |      |   1|   2|   3|\n",
      "     |      |   6|   4|   5|\n",
      "     |      +----+----+----+\n",
      "     |      \n",
      "     |      Example 2: Union with missing columns and setting `allowMissingColumns=True`.\n",
      "     |      \n",
      "     |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n",
      "     |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
      "     |      +----+----+----+----+\n",
      "     |      |col0|col1|col2|col3|\n",
      "     |      +----+----+----+----+\n",
      "     |      |   1|   2|   3|NULL|\n",
      "     |      |NULL|   4|   5|   6|\n",
      "     |      +----+----+----+----+\n",
      "     |      \n",
      "     |      Example 3: Union of two DataFrames with few common columns.\n",
      "     |      \n",
      "     |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([[4, 5, 6, 7]], [\"col1\", \"col2\", \"col3\", \"col4\"])\n",
      "     |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
      "     |      +----+----+----+----+----+\n",
      "     |      |col0|col1|col2|col3|col4|\n",
      "     |      +----+----+----+----+----+\n",
      "     |      |   1|   2|   3|NULL|NULL|\n",
      "     |      |NULL|   4|   5|   6|   7|\n",
      "     |      +----+----+----+----+----+\n",
      "     |      \n",
      "     |      Example 4: Union of two DataFrames with completely different columns.\n",
      "     |      \n",
      "     |      >>> df1 = spark.createDataFrame([[0, 1, 2]], [\"col0\", \"col1\", \"col2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([[3, 4, 5]], [\"col3\", \"col4\", \"col5\"])\n",
      "     |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
      "     |      +----+----+----+----+----+----+\n",
      "     |      |col0|col1|col2|col3|col4|col5|\n",
      "     |      +----+----+----+----+----+----+\n",
      "     |      |   0|   1|   2|NULL|NULL|NULL|\n",
      "     |      |NULL|NULL|NULL|   3|   4|   5|\n",
      "     |      +----+----+----+----+----+----+\n",
      "     |  \n",
      "     |  unpersist(self, blocking: bool = False) -> 'DataFrame'\n",
      "     |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n",
      "     |      memory and disk.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      `blocking` default has changed to ``False`` to match Scala in 2.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      blocking : bool\n",
      "     |          Whether to block until all blocks are deleted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Unpersisted DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.range(1)\n",
      "     |      >>> df.persist()\n",
      "     |      DataFrame[id: bigint]\n",
      "     |      >>> df.unpersist()\n",
      "     |      DataFrame[id: bigint]\n",
      "     |      >>> df = spark.range(1)\n",
      "     |      >>> df.unpersist(True)\n",
      "     |      DataFrame[id: bigint]\n",
      "     |  \n",
      "     |  unpivot(self, ids: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...]], values: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...], NoneType], variableColumnName: str, valueColumnName: str) -> 'DataFrame'\n",
      "     |      Unpivot a DataFrame from wide format to long format, optionally leaving\n",
      "     |      identifier columns set. This is the reverse to `groupBy(...).pivot(...).agg(...)`,\n",
      "     |      except for the aggregation, which cannot be reversed.\n",
      "     |      \n",
      "     |      This function is useful to massage a DataFrame into a format where some\n",
      "     |      columns are identifier columns (\"ids\"), while all other columns (\"values\")\n",
      "     |      are \"unpivoted\" to the rows, leaving just two non-id columns, named as given\n",
      "     |      by `variableColumnName` and `valueColumnName`.\n",
      "     |      \n",
      "     |      When no \"id\" columns are given, the unpivoted DataFrame consists of only the\n",
      "     |      \"variable\" and \"value\" columns.\n",
      "     |      \n",
      "     |      The `values` columns must not be empty so at least one value must be given to be unpivoted.\n",
      "     |      When `values` is `None`, all non-id columns will be unpivoted.\n",
      "     |      \n",
      "     |      All \"value\" columns must share a least common data type. Unless they are the same data type,\n",
      "     |      all \"value\" columns are cast to the nearest common data type. For instance, types\n",
      "     |      `IntegerType` and `LongType` are cast to `LongType`, while `IntegerType` and `StringType`\n",
      "     |      do not have a common data type and `unpivot` fails.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ids : str, Column, tuple, list\n",
      "     |          Column(s) to use as identifiers. Can be a single column or column name,\n",
      "     |          or a list or tuple for multiple columns.\n",
      "     |      values : str, Column, tuple, list, optional\n",
      "     |          Column(s) to unpivot. Can be a single column or column name, or a list or tuple\n",
      "     |          for multiple columns. If specified, must not be empty. If not specified, uses all\n",
      "     |          columns that are not set as `ids`.\n",
      "     |      variableColumnName : str\n",
      "     |          Name of the variable column.\n",
      "     |      valueColumnName : str\n",
      "     |          Name of the value column.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Unpivoted DataFrame.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(1, 11, 1.1), (2, 12, 1.2)],\n",
      "     |      ...     [\"id\", \"int\", \"double\"],\n",
      "     |      ... )\n",
      "     |      >>> df.show()\n",
      "     |      +---+---+------+\n",
      "     |      | id|int|double|\n",
      "     |      +---+---+------+\n",
      "     |      |  1| 11|   1.1|\n",
      "     |      |  2| 12|   1.2|\n",
      "     |      +---+---+------+\n",
      "     |      \n",
      "     |      >>> df.unpivot(\"id\", [\"int\", \"double\"], \"var\", \"val\").show()\n",
      "     |      +---+------+----+\n",
      "     |      | id|   var| val|\n",
      "     |      +---+------+----+\n",
      "     |      |  1|   int|11.0|\n",
      "     |      |  1|double| 1.1|\n",
      "     |      |  2|   int|12.0|\n",
      "     |      |  2|double| 1.2|\n",
      "     |      +---+------+----+\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      DataFrame.melt\n",
      "     |  \n",
      "     |  where = filter(self, condition)\n",
      "     |      :func:`where` is an alias for :func:`filter`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  withColumn(self, colName: str, col: pyspark.sql.column.Column) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      "     |      existing column that has the same name.\n",
      "     |      \n",
      "     |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      "     |      a column from some other :class:`DataFrame` will raise an error.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      colName : str\n",
      "     |          string, name of the new column.\n",
      "     |      col : :class:`Column`\n",
      "     |          a :class:`Column` expression for the new column.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with new or replaced column.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method introduces a projection internally. Therefore, calling it multiple\n",
      "     |      times, for instance, via loops in order to add multiple columns can generate big\n",
      "     |      plans which can cause performance issues and even `StackOverflowException`.\n",
      "     |      To avoid this, use :func:`select` with multiple columns at once.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.withColumn('age2', df.age + 2).show()\n",
      "     |      +---+-----+----+\n",
      "     |      |age| name|age2|\n",
      "     |      +---+-----+----+\n",
      "     |      |  2|Alice|   4|\n",
      "     |      |  5|  Bob|   7|\n",
      "     |      +---+-----+----+\n",
      "     |  \n",
      "     |  withColumnRenamed(self, existing: str, new: str) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` by renaming an existing column.\n",
      "     |      This is a no-op if the schema doesn't contain the given column name.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      existing : str\n",
      "     |          string, name of the existing column to rename.\n",
      "     |      new : str\n",
      "     |          string, new name of the column.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with renamed column.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.withColumnRenamed('age', 'age2').show()\n",
      "     |      +----+-----+\n",
      "     |      |age2| name|\n",
      "     |      +----+-----+\n",
      "     |      |   2|Alice|\n",
      "     |      |   5|  Bob|\n",
      "     |      +----+-----+\n",
      "     |  \n",
      "     |  withColumns(self, *colsMap: Dict[str, pyspark.sql.column.Column]) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` by adding multiple columns or replacing the\n",
      "     |      existing columns that have the same names.\n",
      "     |      \n",
      "     |      The colsMap is a map of column name and column, the column must only refer to attributes\n",
      "     |      supplied by this Dataset. It is an error to add columns that refer to some other Dataset.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |         Added support for multiple columns adding\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      colsMap : dict\n",
      "     |          a dict of column name and :class:`Column`. Currently, only a single map is supported.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with new or replaced columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df.withColumns({'age2': df.age + 2, 'age3': df.age + 3}).show()\n",
      "     |      +---+-----+----+----+\n",
      "     |      |age| name|age2|age3|\n",
      "     |      +---+-----+----+----+\n",
      "     |      |  2|Alice|   4|   5|\n",
      "     |      |  5|  Bob|   7|   8|\n",
      "     |      +---+-----+----+----+\n",
      "     |  \n",
      "     |  withColumnsRenamed(self, colsMap: Dict[str, str]) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` by renaming multiple columns.\n",
      "     |      This is a no-op if the schema doesn't contain the given column names.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |         Added support for multiple columns renaming\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      colsMap : dict\n",
      "     |          a dict of existing column names and corresponding desired column names.\n",
      "     |          Currently, only a single map is supported.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with renamed columns.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`withColumnRenamed`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Support Spark Connect\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df = df.withColumns({'age2': df.age + 2, 'age3': df.age + 3})\n",
      "     |      >>> df.withColumnsRenamed({'age2': 'age4', 'age3': 'age5'}).show()\n",
      "     |      +---+-----+----+----+\n",
      "     |      |age| name|age4|age5|\n",
      "     |      +---+-----+----+----+\n",
      "     |      |  2|Alice|   4|   5|\n",
      "     |      |  5|  Bob|   7|   8|\n",
      "     |      +---+-----+----+----+\n",
      "     |  \n",
      "     |  withMetadata(self, columnName: str, metadata: Dict[str, Any]) -> 'DataFrame'\n",
      "     |      Returns a new :class:`DataFrame` by updating an existing column with metadata.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      columnName : str\n",
      "     |          string, name of the existing column to update the metadata.\n",
      "     |      metadata : dict\n",
      "     |          dict, new metadata to be assigned to df.schema[columnName].metadata\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with updated metadata column.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> df_meta = df.withMetadata('age', {'foo': 'bar'})\n",
      "     |      >>> df_meta.schema['age'].metadata\n",
      "     |      {'foo': 'bar'}\n",
      "     |  \n",
      "     |  withWatermark(self, eventTime: str, delayThreshold: str) -> 'DataFrame'\n",
      "     |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n",
      "     |      in time before which we assume no more late data is going to arrive.\n",
      "     |      \n",
      "     |      Spark will use this watermark for several purposes:\n",
      "     |        - To know when a given time window aggregation can be finalized and thus can be emitted\n",
      "     |          when using output modes that do not allow updates.\n",
      "     |      \n",
      "     |        - To minimize the amount of state that we need to keep for on-going aggregations.\n",
      "     |      \n",
      "     |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n",
      "     |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n",
      "     |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n",
      "     |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n",
      "     |      process records that arrive more than `delayThreshold` late.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      eventTime : str\n",
      "     |          the name of the column that contains the event time of the row.\n",
      "     |      delayThreshold : str\n",
      "     |          the minimum delay to wait to data to arrive late, relative to the\n",
      "     |          latest record that has been processed in the form of an interval\n",
      "     |          (e.g. \"1 minute\" or \"5 hours\").\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Watermarked DataFrame\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This is a feature only for Structured Streaming.\n",
      "     |      \n",
      "     |      This API is evolving.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> from pyspark.sql.functions import timestamp_seconds\n",
      "     |      >>> df = spark.readStream.format(\"rate\").load().selectExpr(\n",
      "     |      ...     \"value % 5 AS value\", \"timestamp\")\n",
      "     |      >>> df.select(\"value\", df.timestamp.alias(\"time\")).withWatermark(\"time\", '10 minutes')\n",
      "     |      DataFrame[value: bigint, time: timestamp]\n",
      "     |      \n",
      "     |      Group the data by window and value (0 - 4), and compute the count of each group.\n",
      "     |      \n",
      "     |      >>> import time\n",
      "     |      >>> from pyspark.sql.functions import window\n",
      "     |      >>> query = (df\n",
      "     |      ...     .withWatermark(\"timestamp\", \"10 minutes\")\n",
      "     |      ...     .groupBy(\n",
      "     |      ...         window(df.timestamp, \"10 minutes\", \"5 minutes\"),\n",
      "     |      ...         df.value)\n",
      "     |      ...     ).count().writeStream.outputMode(\"complete\").format(\"console\").start()\n",
      "     |      >>> time.sleep(3)\n",
      "     |      >>> query.stop()\n",
      "     |  \n",
      "     |  writeTo(self, table: str) -> pyspark.sql.readwriter.DataFrameWriterV2\n",
      "     |      Create a write configuration builder for v2 sources.\n",
      "     |      \n",
      "     |      This builder is used to configure and execute write operations.\n",
      "     |      \n",
      "     |      For example, to append or create or replace existing tables.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      table : str\n",
      "     |          Target table name to write to.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrameWriterV2`\n",
      "     |          DataFrameWriterV2 to use further to specify how to save the data\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.writeTo(\"catalog.db.table\").append()  # doctest: +SKIP\n",
      "     |      >>> df.writeTo(                              # doctest: +SKIP\n",
      "     |      ...     \"catalog.db.table\"\n",
      "     |      ... ).partitionedBy(\"col\").createOrReplace()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  columns\n",
      "     |      Retrieves the names of all columns in the :class:`DataFrame` as a list.\n",
      "     |      \n",
      "     |      The order of the column names in the list reflects their order in the DataFrame.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          List of column names in the DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Example 1: Retrieve column names of a DataFrame\n",
      "     |      \n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\", \"CA\"), (23, \"Alice\", \"NY\"), (16, \"Bob\", \"TX\")],\n",
      "     |      ...     [\"age\", \"name\", \"state\"]\n",
      "     |      ... )\n",
      "     |      >>> df.columns\n",
      "     |      ['age', 'name', 'state']\n",
      "     |      \n",
      "     |      Example 2: Using column names to project specific columns\n",
      "     |      \n",
      "     |      >>> selected_cols = [col for col in df.columns if col != \"age\"]\n",
      "     |      >>> df.select(selected_cols).show()\n",
      "     |      +-----+-----+\n",
      "     |      | name|state|\n",
      "     |      +-----+-----+\n",
      "     |      |  Tom|   CA|\n",
      "     |      |Alice|   NY|\n",
      "     |      |  Bob|   TX|\n",
      "     |      +-----+-----+\n",
      "     |      \n",
      "     |      Example 3: Checking if a specific column exists in a DataFrame\n",
      "     |      \n",
      "     |      >>> \"state\" in df.columns\n",
      "     |      True\n",
      "     |      >>> \"salary\" in df.columns\n",
      "     |      False\n",
      "     |      \n",
      "     |      Example 4: Iterating over columns to apply a transformation\n",
      "     |      \n",
      "     |      >>> import pyspark.sql.functions as f\n",
      "     |      >>> for col_name in df.columns:\n",
      "     |      ...     df = df.withColumn(col_name, f.upper(f.col(col_name)))\n",
      "     |      >>> df.show()\n",
      "     |      +---+-----+-----+\n",
      "     |      |age| name|state|\n",
      "     |      +---+-----+-----+\n",
      "     |      | 14|  TOM|   CA|\n",
      "     |      | 23|ALICE|   NY|\n",
      "     |      | 16|  BOB|   TX|\n",
      "     |      +---+-----+-----+\n",
      "     |      \n",
      "     |      Example 5: Renaming columns and checking the updated column names\n",
      "     |      \n",
      "     |      >>> df = df.withColumnRenamed(\"name\", \"first_name\")\n",
      "     |      >>> df.columns\n",
      "     |      ['age', 'first_name', 'state']\n",
      "     |      \n",
      "     |      Example 6: Using the `columns` property to ensure two DataFrames have the\n",
      "     |      same columns before a union\n",
      "     |      \n",
      "     |      >>> df2 = spark.createDataFrame(\n",
      "     |      ...     [(30, \"Eve\", \"FL\"), (40, \"Sam\", \"WA\")], [\"age\", \"name\", \"location\"])\n",
      "     |      >>> df.columns == df2.columns\n",
      "     |      False\n",
      "     |  \n",
      "     |  dtypes\n",
      "     |      Returns all column names and their data types as a list.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          List of columns as tuple pairs.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.dtypes\n",
      "     |      [('age', 'bigint'), ('name', 'string')]\n",
      "     |  \n",
      "     |  isStreaming\n",
      "     |      Returns ``True`` if this :class:`DataFrame` contains one or more sources that\n",
      "     |      continuously return data as it arrives. A :class:`DataFrame` that reads data from a\n",
      "     |      streaming source must be executed as a :class:`StreamingQuery` using the :func:`start`\n",
      "     |      method in :class:`DataStreamWriter`.  Methods that return a single answer, (e.g.,\n",
      "     |      :func:`count` or :func:`collect`) will throw an :class:`AnalysisException` when there\n",
      "     |      is a streaming source present.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          Whether it's streaming DataFrame or not.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.readStream.format(\"rate\").load()\n",
      "     |      >>> df.isStreaming\n",
      "     |      True\n",
      "     |  \n",
      "     |  na\n",
      "     |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrameNaFunctions`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.sql(\"SELECT 1 AS c1, int(NULL) AS c2\")\n",
      "     |      >>> type(df.na)\n",
      "     |      <class '...dataframe.DataFrameNaFunctions'>\n",
      "     |      \n",
      "     |      Replace the missing values as 2.\n",
      "     |      \n",
      "     |      >>> df.na.fill(2).show()\n",
      "     |      +---+---+\n",
      "     |      | c1| c2|\n",
      "     |      +---+---+\n",
      "     |      |  1|  2|\n",
      "     |      +---+---+\n",
      "     |  \n",
      "     |  rdd\n",
      "     |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`RDD`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.range(1)\n",
      "     |      >>> type(df.rdd)\n",
      "     |      <class 'pyspark.rdd.RDD'>\n",
      "     |  \n",
      "     |  schema\n",
      "     |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`StructType`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      \n",
      "     |      Retrieve the schema of the current DataFrame.\n",
      "     |      \n",
      "     |      >>> df.schema\n",
      "     |      StructType([StructField('age', LongType(), True),\n",
      "     |                  StructField('name', StringType(), True)])\n",
      "     |  \n",
      "     |  sparkSession\n",
      "     |      Returns Spark session that created this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`SparkSession`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.range(1)\n",
      "     |      >>> type(df.sparkSession)\n",
      "     |      <class '...session.SparkSession'>\n",
      "     |  \n",
      "     |  sql_ctx\n",
      "     |  \n",
      "     |  stat\n",
      "     |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrameStatFunctions`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import pyspark.sql.functions as f\n",
      "     |      >>> df = spark.range(3).withColumn(\"c\", f.expr(\"id + 1\"))\n",
      "     |      >>> type(df.stat)\n",
      "     |      <class '...dataframe.DataFrameStatFunctions'>\n",
      "     |      >>> df.stat.corr(\"id\", \"c\")\n",
      "     |      1.0\n",
      "     |  \n",
      "     |  storageLevel\n",
      "     |      Get the :class:`DataFrame`'s current storage level.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`StorageLevel`\n",
      "     |          Currently defined storage level.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df1 = spark.range(10)\n",
      "     |      >>> df1.storageLevel\n",
      "     |      StorageLevel(False, False, False, False, 1)\n",
      "     |      >>> df1.cache().storageLevel\n",
      "     |      StorageLevel(True, True, False, True, 1)\n",
      "     |      \n",
      "     |      >>> df2 = spark.range(5)\n",
      "     |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n",
      "     |      StorageLevel(True, False, False, False, 2)\n",
      "     |  \n",
      "     |  write\n",
      "     |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
      "     |      storage.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrameWriter`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "     |      >>> type(df.write)\n",
      "     |      <class '...readwriter.DataFrameWriter'>\n",
      "     |      \n",
      "     |      Write the DataFrame as a table.\n",
      "     |      \n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tab2\")\n",
      "     |      >>> df.write.saveAsTable(\"tab2\")\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tab2\")\n",
      "     |  \n",
      "     |  writeStream\n",
      "     |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n",
      "     |      storage.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataStreamWriter`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import tempfile\n",
      "     |      >>> df = spark.readStream.format(\"rate\").load()\n",
      "     |      >>> type(df.writeStream)\n",
      "     |      <class '...streaming.readwriter.DataStreamWriter'>\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Create a table with Rate source.\n",
      "     |      ...     df.writeStream.toTable(\n",
      "     |      ...         \"my_table\", checkpointLocation=d)\n",
      "     |      <...streaming.query.StreamingQuery object at 0x...>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      "     |  \n",
      "     |  mapInArrow(self, func: 'ArrowMapIterFunction', schema: Union[pyspark.sql.types.StructType, str], barrier: bool = False) -> 'DataFrame'\n",
      "     |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      "     |      function that takes and outputs a PyArrow's `RecordBatch`, and returns the result as a\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The function should take an iterator of `pyarrow.RecordBatch`\\s and return\n",
      "     |      another iterator of `pyarrow.RecordBatch`\\s. All columns are passed\n",
      "     |      together as an iterator of `pyarrow.RecordBatch`\\s to the function and the\n",
      "     |      returned iterator of `pyarrow.RecordBatch`\\s are combined as a :class:`DataFrame`.\n",
      "     |      Each `pyarrow.RecordBatch` size can be controlled by\n",
      "     |      `spark.sql.execution.arrow.maxRecordsPerBatch`. The size of the function's input and\n",
      "     |      output can be different.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : function\n",
      "     |          a Python native function that takes an iterator of `pyarrow.RecordBatch`\\s, and\n",
      "     |          outputs an iterator of `pyarrow.RecordBatch`\\s.\n",
      "     |      schema : :class:`pyspark.sql.types.DataType` or str\n",
      "     |          the return type of the `func` in PySpark. The value can be either a\n",
      "     |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "     |      barrier : bool, optional, default True\n",
      "     |          Use barrier mode execution.\n",
      "     |      \n",
      "     |          .. versionchanged: 3.5.0\n",
      "     |              Added ``barrier`` argument.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import pyarrow  # doctest: +SKIP\n",
      "     |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      "     |      >>> def filter_func(iterator):\n",
      "     |      ...     for batch in iterator:\n",
      "     |      ...         pdf = batch.to_pandas()\n",
      "     |      ...         yield pyarrow.RecordBatch.from_pandas(pdf[pdf.id == 1])\n",
      "     |      >>> df.mapInArrow(filter_func, df.schema).show()  # doctest: +SKIP\n",
      "     |      +---+---+\n",
      "     |      | id|age|\n",
      "     |      +---+---+\n",
      "     |      |  1| 21|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      Set ``barrier`` to ``True`` to force the ``mapInArrow`` stage running in the\n",
      "     |      barrier mode, it ensures all Python workers in the stage will be\n",
      "     |      launched concurrently.\n",
      "     |      \n",
      "     |      >>> df.mapInArrow(filter_func, df.schema, barrier=True).show()  # doctest: +SKIP\n",
      "     |      +---+---+\n",
      "     |      | id|age|\n",
      "     |      +---+---+\n",
      "     |      |  1| 21|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is unstable, and for developers.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.functions.pandas_udf\n",
      "     |      pyspark.sql.DataFrame.mapInPandas\n",
      "     |  \n",
      "     |  mapInPandas(self, func: 'PandasMapIterFunction', schema: Union[pyspark.sql.types.StructType, str], barrier: bool = False) -> 'DataFrame'\n",
      "     |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      "     |      function that takes and outputs a pandas DataFrame, and returns the result as a\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The function should take an iterator of `pandas.DataFrame`\\s and return\n",
      "     |      another iterator of `pandas.DataFrame`\\s. All columns are passed\n",
      "     |      together as an iterator of `pandas.DataFrame`\\s to the function and the\n",
      "     |      returned iterator of `pandas.DataFrame`\\s are combined as a :class:`DataFrame`.\n",
      "     |      Each `pandas.DataFrame` size can be controlled by\n",
      "     |      `spark.sql.execution.arrow.maxRecordsPerBatch`. The size of the function's input and\n",
      "     |      output can be different.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : function\n",
      "     |          a Python native function that takes an iterator of `pandas.DataFrame`\\s, and\n",
      "     |          outputs an iterator of `pandas.DataFrame`\\s.\n",
      "     |      schema : :class:`pyspark.sql.types.DataType` or str\n",
      "     |          the return type of the `func` in PySpark. The value can be either a\n",
      "     |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "     |      barrier : bool, optional, default True\n",
      "     |          Use barrier mode execution.\n",
      "     |      \n",
      "     |          .. versionchanged: 3.5.0\n",
      "     |              Added ``barrier`` argument.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import pandas_udf\n",
      "     |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      "     |      >>> def filter_func(iterator):\n",
      "     |      ...     for pdf in iterator:\n",
      "     |      ...         yield pdf[pdf.id == 1]\n",
      "     |      ...\n",
      "     |      >>> df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n",
      "     |      +---+---+\n",
      "     |      | id|age|\n",
      "     |      +---+---+\n",
      "     |      |  1| 21|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      Set ``barrier`` to ``True`` to force the ``mapInPandas`` stage running in the\n",
      "     |      barrier mode, it ensures all Python workers in the stage will be\n",
      "     |      launched concurrently.\n",
      "     |      \n",
      "     |      >>> df.mapInPandas(filter_func, df.schema, barrier=True).show()  # doctest: +SKIP\n",
      "     |      +---+---+\n",
      "     |      | id|age|\n",
      "     |      +---+---+\n",
      "     |      |  1| 21|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is experimental\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.functions.pandas_udf\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pyspark.sql.pandas.conversion.PandasConversionMixin:\n",
      "     |  \n",
      "     |  toPandas(self) -> 'PandasDataFrameLike'\n",
      "     |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n",
      "     |      \n",
      "     |      This is only available if Pandas is installed and available.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method should only be used if the resulting Pandas ``pandas.DataFrame`` is\n",
      "     |      expected to be small, as all the data is loaded into the driver's memory.\n",
      "     |      \n",
      "     |      Usage with ``spark.sql.execution.arrow.pyspark.enabled=True`` is experimental.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.toPandas()  # doctest: +SKIP\n",
      "     |         age   name\n",
      "     |      0    2  Alice\n",
      "     |      1    5    Bob\n",
      "    \n",
      "    class DataFrameNaFunctions(builtins.object)\n",
      "     |  DataFrameNaFunctions(df: pyspark.sql.dataframe.DataFrame)\n",
      "     |  \n",
      "     |  Functionality for working with missing data in :class:`DataFrame`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df: pyspark.sql.dataframe.DataFrame)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  drop(self, how: str = 'any', thresh: Optional[int] = None, subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      "     |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      how : str, optional\n",
      "     |          'any' or 'all'.\n",
      "     |          If 'any', drop a row if it contains any nulls.\n",
      "     |          If 'all', drop a row only if all its values are null.\n",
      "     |      thresh: int, optional\n",
      "     |          default None\n",
      "     |          If specified, drop rows that have less than `thresh` non-null values.\n",
      "     |          This overwrites the `how` parameter.\n",
      "     |      subset : str, tuple or list, optional\n",
      "     |          optional list of column names to consider.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with null only rows excluded.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
      "     |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
      "     |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
      "     |      ...     Row(age=None, height=None, name=None),\n",
      "     |      ... ])\n",
      "     |      >>> df.na.drop().show()\n",
      "     |      +---+------+-----+\n",
      "     |      |age|height| name|\n",
      "     |      +---+------+-----+\n",
      "     |      | 10|    80|Alice|\n",
      "     |      +---+------+-----+\n",
      "     |  \n",
      "     |  fill(self, value: Union[ForwardRef('LiteralType'), Dict[str, ForwardRef('LiteralType')]], subset: Optional[List[str]] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Replace null values, alias for ``na.fill()``.\n",
      "     |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      value : int, float, string, bool or dict\n",
      "     |          Value to replace null values with.\n",
      "     |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      "     |          from column name (string) to replacement value. The replacement value must be\n",
      "     |          an int, float, boolean, or string.\n",
      "     |      subset : str, tuple or list, optional\n",
      "     |          optional list of column names to consider.\n",
      "     |          Columns specified in subset that do not have matching data types are ignored.\n",
      "     |          For example, if `value` is a string, and subset contains a non-string column,\n",
      "     |          then the non-string column is simply ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with replaced null values.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (10, 80.5, \"Alice\", None),\n",
      "     |      ...     (5, None, \"Bob\", None),\n",
      "     |      ...     (None, None, \"Tom\", None),\n",
      "     |      ...     (None, None, None, True)],\n",
      "     |      ...     schema=[\"age\", \"height\", \"name\", \"bool\"])\n",
      "     |      \n",
      "     |      Fill all null values with 50 for numeric columns.\n",
      "     |      \n",
      "     |      >>> df.na.fill(50).show()\n",
      "     |      +---+------+-----+----+\n",
      "     |      |age|height| name|bool|\n",
      "     |      +---+------+-----+----+\n",
      "     |      | 10|  80.5|Alice|NULL|\n",
      "     |      |  5|  50.0|  Bob|NULL|\n",
      "     |      | 50|  50.0|  Tom|NULL|\n",
      "     |      | 50|  50.0| NULL|true|\n",
      "     |      +---+------+-----+----+\n",
      "     |      \n",
      "     |      Fill all null values with ``False`` for boolean columns.\n",
      "     |      \n",
      "     |      >>> df.na.fill(False).show()\n",
      "     |      +----+------+-----+-----+\n",
      "     |      | age|height| name| bool|\n",
      "     |      +----+------+-----+-----+\n",
      "     |      |  10|  80.5|Alice|false|\n",
      "     |      |   5|  NULL|  Bob|false|\n",
      "     |      |NULL|  NULL|  Tom|false|\n",
      "     |      |NULL|  NULL| NULL| true|\n",
      "     |      +----+------+-----+-----+\n",
      "     |      \n",
      "     |      Fill all null values with to 50 and \"unknown\" for 'age' and 'name' column respectively.\n",
      "     |      \n",
      "     |      >>> df.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      "     |      +---+------+-------+----+\n",
      "     |      |age|height|   name|bool|\n",
      "     |      +---+------+-------+----+\n",
      "     |      | 10|  80.5|  Alice|NULL|\n",
      "     |      |  5|  NULL|    Bob|NULL|\n",
      "     |      | 50|  NULL|    Tom|NULL|\n",
      "     |      | 50|  NULL|unknown|true|\n",
      "     |      +---+------+-------+----+\n",
      "     |  \n",
      "     |  replace(self, to_replace: Union[List[ForwardRef('LiteralType')], Dict[ForwardRef('LiteralType'), ForwardRef('OptionalPrimitiveType')]], value: Union[ForwardRef('OptionalPrimitiveType'), List[ForwardRef('OptionalPrimitiveType')], pyspark._globals._NoValueType, NoneType] = <no value>, subset: Optional[List[str]] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      "     |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      "     |      aliases of each other.\n",
      "     |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      "     |      or strings. Value can have None. When replacing, the new value will be cast\n",
      "     |      to the type of the existing column.\n",
      "     |      For numeric replacements all values to be replaced should have unique\n",
      "     |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      "     |      and arbitrary replacement will be used.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      to_replace : bool, int, float, string, list or dict\n",
      "     |          Value to be replaced.\n",
      "     |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      "     |          must be a mapping between a value and a replacement.\n",
      "     |      value : bool, int, float, string or None, optional\n",
      "     |          The replacement value must be a bool, int, float, string or None. If `value` is a\n",
      "     |          list, `value` should be of the same length and type as `to_replace`.\n",
      "     |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      "     |          used as a replacement for each item in `to_replace`.\n",
      "     |      subset : list, optional\n",
      "     |          optional list of column names to consider.\n",
      "     |          Columns specified in subset that do not have matching data types are ignored.\n",
      "     |          For example, if `value` is a string, and subset contains a non-string column,\n",
      "     |          then the non-string column is simply ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with replaced values.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (10, 80, \"Alice\"),\n",
      "     |      ...     (5, None, \"Bob\"),\n",
      "     |      ...     (None, 10, \"Tom\"),\n",
      "     |      ...     (None, None, None)],\n",
      "     |      ...     schema=[\"age\", \"height\", \"name\"])\n",
      "     |      \n",
      "     |      Replace 10 to 20 in all columns.\n",
      "     |      \n",
      "     |      >>> df.na.replace(10, 20).show()\n",
      "     |      +----+------+-----+\n",
      "     |      | age|height| name|\n",
      "     |      +----+------+-----+\n",
      "     |      |  20|    80|Alice|\n",
      "     |      |   5|  NULL|  Bob|\n",
      "     |      |NULL|    20|  Tom|\n",
      "     |      |NULL|  NULL| NULL|\n",
      "     |      +----+------+-----+\n",
      "     |      \n",
      "     |      Replace 'Alice' to null in all columns.\n",
      "     |      \n",
      "     |      >>> df.na.replace('Alice', None).show()\n",
      "     |      +----+------+----+\n",
      "     |      | age|height|name|\n",
      "     |      +----+------+----+\n",
      "     |      |  10|    80|NULL|\n",
      "     |      |   5|  NULL| Bob|\n",
      "     |      |NULL|    10| Tom|\n",
      "     |      |NULL|  NULL|NULL|\n",
      "     |      +----+------+----+\n",
      "     |      \n",
      "     |      Replace 'Alice' to 'A', and 'Bob' to 'B' in the 'name' column.\n",
      "     |      \n",
      "     |      >>> df.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      "     |      +----+------+----+\n",
      "     |      | age|height|name|\n",
      "     |      +----+------+----+\n",
      "     |      |  10|    80|   A|\n",
      "     |      |   5|  NULL|   B|\n",
      "     |      |NULL|    10| Tom|\n",
      "     |      |NULL|  NULL|NULL|\n",
      "     |      +----+------+----+\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DataFrameReader(OptionUtils)\n",
      "     |  DataFrameReader(spark: 'SparkSession')\n",
      "     |  \n",
      "     |  Interface used to load a :class:`DataFrame` from external storage systems\n",
      "     |  (e.g. file systems, key-value stores, etc). Use :attr:`SparkSession.read`\n",
      "     |  to access this.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DataFrameReader\n",
      "     |      OptionUtils\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, spark: 'SparkSession')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  csv(self, path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[int, str, NoneType] = None, maxCharsPerColumn: Union[int, str, NoneType] = None, maxMalformedLogPerPartition: Union[int, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame'\n",
      "     |      Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      "     |      \n",
      "     |      This function will go through the input once to determine the input schema if\n",
      "     |      ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      "     |      ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str or list\n",
      "     |          string, or list of strings, for input path(s),\n",
      "     |          or RDD of Strings storing CSV rows.\n",
      "     |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      "     |          an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      "     |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a CSV file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a CSV file\n",
      "     |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      "     |      ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
      "     |      ...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |100|NULL|\n",
      "     |      +---+----+\n",
      "     |  \n",
      "     |  format(self, source: str) -> 'DataFrameReader'\n",
      "     |      Specifies the input data source format.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      source : str\n",
      "     |          string, name of the data source, e.g. 'json', 'parquet'.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.read.format('json')\n",
      "     |      <...readwriter.DataFrameReader object ...>\n",
      "     |      \n",
      "     |      Write a DataFrame into a JSON file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a JSON file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the JSON file as a DataFrame.\n",
      "     |      ...     spark.read.format('json').load(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  jdbc(self, url: str, table: str, column: Optional[str] = None, lowerBound: Union[int, str, NoneType] = None, upperBound: Union[int, str, NoneType] = None, numPartitions: Optional[int] = None, predicates: Optional[List[str]] = None, properties: Optional[Dict[str, str]] = None) -> 'DataFrame'\n",
      "     |      Construct a :class:`DataFrame` representing the database table named ``table``\n",
      "     |      accessible via JDBC URL ``url`` and connection ``properties``.\n",
      "     |      \n",
      "     |      Partitions of the table will be retrieved in parallel if either ``column`` or\n",
      "     |      ``predicates`` is specified. ``lowerBound``, ``upperBound`` and ``numPartitions``\n",
      "     |      is needed when ``column`` is specified.\n",
      "     |      \n",
      "     |      If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      table : str\n",
      "     |          the name of the table\n",
      "     |      column : str, optional\n",
      "     |          alias of ``partitionColumn`` option. Refer to ``partitionColumn`` in\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      predicates : list, optional\n",
      "     |          a list of expressions suitable for inclusion in WHERE clauses;\n",
      "     |          each one defines one partition of the :class:`DataFrame`\n",
      "     |      properties : dict, optional\n",
      "     |          a dictionary of JDBC database connection arguments. Normally at\n",
      "     |          least properties \"user\" and \"password\" with their corresponding values.\n",
      "     |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Don't create too many partitions in parallel on a large cluster;\n",
      "     |      otherwise Spark might crash your external database systems.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |  \n",
      "     |  json(self, path: Union[str, List[str], pyspark.rdd.RDD[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, primitivesAsString: Union[bool, str, NoneType] = None, prefersDecimal: Union[bool, str, NoneType] = None, allowComments: Union[bool, str, NoneType] = None, allowUnquotedFieldNames: Union[bool, str, NoneType] = None, allowSingleQuotes: Union[bool, str, NoneType] = None, allowNumericLeadingZero: Union[bool, str, NoneType] = None, allowBackslashEscapingAnyCharacter: Union[bool, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, allowUnquotedControlChars: Union[bool, str, NoneType] = None, lineSep: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, dropFieldIfAllNull: Union[bool, str, NoneType] = None, encoding: Optional[str] = None, locale: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, allowNonNumericNumbers: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
      "     |      Loads JSON files and returns the results as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
      "     |      For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
      "     |      \n",
      "     |      If the ``schema`` parameter is not specified, this function goes\n",
      "     |      through the input once to determine the input schema.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str, list or :class:`RDD`\n",
      "     |          string represents path to the JSON dataset, or a list of paths,\n",
      "     |          or RDD of Strings storing JSON objects.\n",
      "     |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      "     |          an optional :class:`pyspark.sql.types.StructType` for the input schema or\n",
      "     |          a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a JSON file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a JSON file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the JSON file as a DataFrame.\n",
      "     |      ...     spark.read.json(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  load(self, path: Union[str, List[str], NoneType] = None, format: Optional[str] = None, schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n",
      "     |      Loads data from a data source and returns it as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str or list, optional\n",
      "     |          optional string or a list of string for file-system backed data sources.\n",
      "     |      format : str, optional\n",
      "     |          optional string for format of the data source. Default to 'parquet'.\n",
      "     |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      "     |          optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      "     |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "     |      **options : dict\n",
      "     |          all other string options\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Load a CSV file with format, schema and options specified.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a CSV file with a header\n",
      "     |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      "     |      ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n",
      "     |      ...     # and 'header' option set to `True`.\n",
      "     |      ...     df = spark.read.load(\n",
      "     |      ...         d, schema=df.schema, format=\"csv\", nullValue=\"Hyukjin Kwon\", header=True)\n",
      "     |      ...     df.printSchema()\n",
      "     |      ...     df.show()\n",
      "     |      root\n",
      "     |       |-- age: long (nullable = true)\n",
      "     |       |-- name: string (nullable = true)\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |100|NULL|\n",
      "     |      +---+----+\n",
      "     |  \n",
      "     |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameReader'\n",
      "     |      Adds an input option for the underlying data source.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key : str\n",
      "     |          The key for the option to set.\n",
      "     |      value\n",
      "     |          The value for the option to set.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.read.option(\"key\", \"value\")\n",
      "     |      <...readwriter.DataFrameReader object ...>\n",
      "     |      \n",
      "     |      Specify the option 'nullValue' with reading a CSV file.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a CSV file\n",
      "     |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      "     |      ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
      "     |      ...     spark.read.schema(df.schema).option(\n",
      "     |      ...         \"nullValue\", \"Hyukjin Kwon\").format('csv').load(d).show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |100|NULL|\n",
      "     |      +---+----+\n",
      "     |  \n",
      "     |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameReader'\n",
      "     |      Adds input options for the underlying data source.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **options : dict\n",
      "     |          The dictionary of string keys and prmitive-type values.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.read.option(\"key\", \"value\")\n",
      "     |      <...readwriter.DataFrameReader object ...>\n",
      "     |      \n",
      "     |      Specify the option 'nullValue' and 'header' with reading a CSV file.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a CSV file with a header.\n",
      "     |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      "     |      ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n",
      "     |      ...     # and 'header' option set to `True`.\n",
      "     |      ...     spark.read.options(\n",
      "     |      ...         nullValue=\"Hyukjin Kwon\",\n",
      "     |      ...         header=True\n",
      "     |      ...     ).format('csv').load(d).show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |100|NULL|\n",
      "     |      +---+----+\n",
      "     |  \n",
      "     |  orc(self, path: Union[str, List[str]], mergeSchema: Optional[bool] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
      "     |      Loads ORC files, returning the result as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str or list\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a ORC file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a ORC file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.mode(\"overwrite\").format(\"orc\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the Parquet file as a DataFrame.\n",
      "     |      ...     spark.read.orc(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  parquet(self, *paths: str, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n",
      "     |      Loads Parquet files, returning the result as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      paths : str\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      **options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a Parquet file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a Parquet file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the Parquet file as a DataFrame.\n",
      "     |      ...     spark.read.parquet(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  schema(self, schema: Union[pyspark.sql.types.StructType, str]) -> 'DataFrameReader'\n",
      "     |      Specifies the input schema.\n",
      "     |      \n",
      "     |      Some data sources (e.g. JSON) can infer the input schema automatically from data.\n",
      "     |      By specifying the schema here, the underlying data source can skip the schema\n",
      "     |      inference step, and thus speed up data loading.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      schema : :class:`pyspark.sql.types.StructType` or str\n",
      "     |          a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\n",
      "     |          (For example ``col0 INT, col1 DOUBLE``).\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.read.schema(\"col0 INT, col1 DOUBLE\")\n",
      "     |      <...readwriter.DataFrameReader object ...>\n",
      "     |      \n",
      "     |      Specify the schema with reading a CSV file.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     spark.read.schema(\"col0 INT, col1 DOUBLE\").format(\"csv\").load(d).printSchema()\n",
      "     |      root\n",
      "     |       |-- col0: integer (nullable = true)\n",
      "     |       |-- col1: double (nullable = true)\n",
      "     |  \n",
      "     |  table(self, tableName: str) -> 'DataFrame'\n",
      "     |      Returns the specified table as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          string, name of the table.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.range(10)\n",
      "     |      >>> df.createOrReplaceTempView('tblA')\n",
      "     |      >>> spark.read.table('tblA').show()\n",
      "     |      +---+\n",
      "     |      | id|\n",
      "     |      +---+\n",
      "     |      |  0|\n",
      "     |      |  1|\n",
      "     |      |  2|\n",
      "     |      |  3|\n",
      "     |      |  4|\n",
      "     |      |  5|\n",
      "     |      |  6|\n",
      "     |      |  7|\n",
      "     |      |  8|\n",
      "     |      |  9|\n",
      "     |      +---+\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n",
      "     |  \n",
      "     |  text(self, paths: Union[str, List[str]], wholetext: bool = False, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
      "     |      Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
      "     |      string column named \"value\", and followed by partitioned columns if there\n",
      "     |      are any.\n",
      "     |      The text files must be encoded as UTF-8.\n",
      "     |      \n",
      "     |      By default, each line in the text file is a new row in the resulting DataFrame.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      paths : str or list\n",
      "     |          string, or list of strings, for input path(s).\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a text file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a text file\n",
      "     |      ...     df = spark.createDataFrame([(\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n",
      "     |      ...     df.write.mode(\"overwrite\").format(\"text\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the text file as a DataFrame.\n",
      "     |      ...     spark.read.schema(df.schema).text(d).sort(\"alphabets\").show()\n",
      "     |      +---------+\n",
      "     |      |alphabets|\n",
      "     |      +---------+\n",
      "     |      |        a|\n",
      "     |      |        b|\n",
      "     |      |        c|\n",
      "     |      +---------+\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from OptionUtils:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DataFrameStatFunctions(builtins.object)\n",
      "     |  DataFrameStatFunctions(df: pyspark.sql.dataframe.DataFrame)\n",
      "     |  \n",
      "     |  Functionality for statistic functions with :class:`DataFrame`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df: pyspark.sql.dataframe.DataFrame)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  approxQuantile(self, col: Union[str, List[str], Tuple[str]], probabilities: Union[List[float], Tuple[float]], relativeError: float) -> Union[List[float], List[List[float]]]\n",
      "     |      Calculates the approximate quantiles of numerical columns of a\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The result of this algorithm has the following deterministic bound:\n",
      "     |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
      "     |      probability `p` up to error `err`, then the algorithm will return\n",
      "     |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
      "     |      close to (p * N). More precisely,\n",
      "     |      \n",
      "     |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
      "     |      \n",
      "     |      This method implements a variation of the Greenwald-Khanna\n",
      "     |      algorithm (with some speed optimizations). The algorithm was first\n",
      "     |      present in [[https://doi.org/10.1145/375663.375670\n",
      "     |      Space-efficient Online Computation of Quantile Summaries]]\n",
      "     |      by Greenwald and Khanna.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col: str, tuple or list\n",
      "     |          Can be a single column name, or a list of names for multiple columns.\n",
      "     |      \n",
      "     |          .. versionchanged:: 2.2.0\n",
      "     |             Added support for multiple columns.\n",
      "     |      probabilities : list or tuple\n",
      "     |          a list of quantile probabilities\n",
      "     |          Each number must belong to [0, 1].\n",
      "     |          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
      "     |      relativeError : float\n",
      "     |          The relative target precision to achieve\n",
      "     |          (>= 0). If set to zero, the exact quantiles are computed, which\n",
      "     |          could be very expensive. Note that values greater than 1 are\n",
      "     |          accepted but gives the same result as 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          the approximate quantiles at the given probabilities.\n",
      "     |      \n",
      "     |          * If the input `col` is a string, the output is a list of floats.\n",
      "     |      \n",
      "     |          * If the input `col` is a list or tuple of strings, the output is also a\n",
      "     |              list, but each element in it is a list of floats, i.e., the output\n",
      "     |              is a list of list of floats.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Null values will be ignored in numerical columns before calculation.\n",
      "     |      For columns only containing null values, an empty list is returned.\n",
      "     |  \n",
      "     |  corr(self, col1: str, col2: str, method: Optional[str] = None) -> float\n",
      "     |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
      "     |      Currently only supports the Pearson Correlation Coefficient.\n",
      "     |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col1 : str\n",
      "     |          The name of the first column\n",
      "     |      col2 : str\n",
      "     |          The name of the second column\n",
      "     |      method : str, optional\n",
      "     |          The correlation method. Currently only supports \"pearson\"\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float\n",
      "     |          Pearson Correlation Coefficient of two columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
      "     |      >>> df.corr(\"c1\", \"c2\")\n",
      "     |      -0.3592106040535498\n",
      "     |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
      "     |      >>> df.corr(\"small\", \"bigger\")\n",
      "     |      1.0\n",
      "     |  \n",
      "     |  cov(self, col1: str, col2: str) -> float\n",
      "     |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
      "     |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col1 : str\n",
      "     |          The name of the first column\n",
      "     |      col2 : str\n",
      "     |          The name of the second column\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float\n",
      "     |          Covariance of two columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
      "     |      >>> df.cov(\"c1\", \"c2\")\n",
      "     |      -18.0\n",
      "     |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
      "     |      >>> df.cov(\"small\", \"bigger\")\n",
      "     |      1.0\n",
      "     |  \n",
      "     |  crosstab(self, col1: str, col2: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
      "     |      table.\n",
      "     |      The first column of each row will be the distinct values of `col1` and the column names\n",
      "     |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
      "     |      Pairs that have no occurrences will have zero as their counts.\n",
      "     |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col1 : str\n",
      "     |          The name of the first column. Distinct items will make the first item of\n",
      "     |          each row.\n",
      "     |      col2 : str\n",
      "     |          The name of the second column. Distinct items will make the column names\n",
      "     |          of the :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          Frequency matrix of two columns.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
      "     |      >>> df.crosstab(\"c1\", \"c2\").sort(\"c1_c2\").show()\n",
      "     |      +-----+---+---+---+\n",
      "     |      |c1_c2| 10| 11|  8|\n",
      "     |      +-----+---+---+---+\n",
      "     |      |    1|  0|  2|  0|\n",
      "     |      |    3|  1|  0|  0|\n",
      "     |      |    4|  0|  0|  2|\n",
      "     |      +-----+---+---+---+\n",
      "     |  \n",
      "     |  freqItems(self, cols: List[str], support: Optional[float] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Finding frequent items for columns, possibly with false positives. Using the\n",
      "     |      frequent element count algorithm described in\n",
      "     |      \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
      "     |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : list or tuple\n",
      "     |          Names of the columns to calculate frequent items for as a list or tuple of\n",
      "     |          strings.\n",
      "     |      support : float, optional\n",
      "     |          The frequency with which to consider an item 'frequent'. Default is 1%.\n",
      "     |          The support must be greater than 1e-4.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |          DataFrame with frequent items.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This function is meant for exploratory data analysis, as we make no\n",
      "     |      guarantee about the backward compatibility of the schema of the resulting\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
      "     |      >>> df.freqItems([\"c1\", \"c2\"]).show()  # doctest: +SKIP\n",
      "     |      +------------+------------+\n",
      "     |      |c1_freqItems|c2_freqItems|\n",
      "     |      +------------+------------+\n",
      "     |      |   [4, 1, 3]| [8, 11, 10]|\n",
      "     |      +------------+------------+\n",
      "     |  \n",
      "     |  sampleBy(self, col: str, fractions: Dict[Any, float], seed: Optional[int] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns a stratified sample without replacement based on the\n",
      "     |      fraction given on each stratum.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col : :class:`Column` or str\n",
      "     |          column that defines strata\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.0.0\n",
      "     |             Added sampling by a column of :class:`Column`\n",
      "     |      fractions : dict\n",
      "     |          sampling fraction for each stratum. If a stratum is not\n",
      "     |          specified, we treat its fraction as zero.\n",
      "     |      seed : int, optional\n",
      "     |          random seed\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a new :class:`DataFrame` that represents the stratified sample\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import col\n",
      "     |      >>> dataset = spark.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
      "     |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
      "     |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
      "     |      +---+-----+\n",
      "     |      |key|count|\n",
      "     |      +---+-----+\n",
      "     |      |  0|    3|\n",
      "     |      |  1|    6|\n",
      "     |      +---+-----+\n",
      "     |      >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
      "     |      33\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DataFrameWriter(OptionUtils)\n",
      "     |  DataFrameWriter(df: 'DataFrame')\n",
      "     |  \n",
      "     |  Interface used to write a :class:`DataFrame` to external storage systems\n",
      "     |  (e.g. file systems, key-value stores, etc). Use :attr:`DataFrame.write`\n",
      "     |  to access this.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DataFrameWriter\n",
      "     |      OptionUtils\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df: 'DataFrame')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  bucketBy(self, numBuckets: int, col: Union[str, List[str], Tuple[str, ...]], *cols: Optional[str]) -> 'DataFrameWriter'\n",
      "     |      Buckets the output by the given columns. If specified,\n",
      "     |      the output is laid out on the file system similar to Hive's bucketing scheme,\n",
      "     |      but with a different bucket hash function and is not compatible with Hive's bucketing.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      numBuckets : int\n",
      "     |          the number of buckets to save\n",
      "     |      col : str, list or tuple\n",
      "     |          a name of a column, or a list of names.\n",
      "     |      cols : str\n",
      "     |          additional names (optional). If `col` is a list it should be empty.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Applicable for file-based data sources in combination with\n",
      "     |      :py:meth:`DataFrameWriter.saveAsTable`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a Parquet file in a buckted manner, and read it back.\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.functions import input_file_name\n",
      "     |      >>> # Write a DataFrame into a Parquet file in a bucketed manner.\n",
      "     |      ... _ = spark.sql(\"DROP TABLE IF EXISTS bucketed_table\")\n",
      "     |      >>> spark.createDataFrame([\n",
      "     |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n",
      "     |      ...     schema=[\"age\", \"name\"]\n",
      "     |      ... ).write.bucketBy(2, \"name\").mode(\"overwrite\").saveAsTable(\"bucketed_table\")\n",
      "     |      >>> # Read the Parquet file as a DataFrame.\n",
      "     |      ... spark.read.table(\"bucketed_table\").sort(\"age\").show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      |120|Hyukjin Kwon|\n",
      "     |      |140| Haejoon Lee|\n",
      "     |      +---+------------+\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE bucketed_table\")\n",
      "     |  \n",
      "     |  csv(self, path: str, mode: Optional[str] = None, compression: Optional[str] = None, sep: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, header: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, escapeQuotes: Union[bool, str, NoneType] = None, quoteAll: Union[bool, str, NoneType] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, encoding: Optional[str] = None, emptyValue: Optional[str] = None, lineSep: Optional[str] = None) -> None\n",
      "     |      Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          the path in any Hadoop supported file system\n",
      "     |      mode : str, optional\n",
      "     |          specifies the behavior of the save operation when data already exists.\n",
      "     |      \n",
      "     |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "     |          * ``overwrite``: Overwrite existing data.\n",
      "     |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      "     |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n",
      "     |              exists.\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a CSV file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a CSV file\n",
      "     |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      "     |      ...     df.write.csv(d, mode=\"overwrite\")\n",
      "     |      ...\n",
      "     |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
      "     |      ...     spark.read.schema(df.schema).format(\"csv\").option(\n",
      "     |      ...         \"nullValue\", \"Hyukjin Kwon\").load(d).show()\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |100|NULL|\n",
      "     |      +---+----+\n",
      "     |  \n",
      "     |  format(self, source: str) -> 'DataFrameWriter'\n",
      "     |      Specifies the underlying output data source.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      source : str\n",
      "     |          string, name of the data source, e.g. 'json', 'parquet'.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.range(1).write.format('parquet')\n",
      "     |      <...readwriter.DataFrameWriter object ...>\n",
      "     |      \n",
      "     |      Write a DataFrame into a Parquet file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a Parquet file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the Parquet file as a DataFrame.\n",
      "     |      ...     spark.read.format('parquet').load(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  insertInto(self, tableName: str, overwrite: Optional[bool] = None) -> None\n",
      "     |      Inserts the content of the :class:`DataFrame` to the specified table.\n",
      "     |      \n",
      "     |      It requires that the schema of the :class:`DataFrame` is the same as the\n",
      "     |      schema of the table.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      overwrite : bool, optional\n",
      "     |          If true, overwrites existing data. Disabled by default\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Unlike :meth:`DataFrameWriter.saveAsTable`, :meth:`DataFrameWriter.insertInto` ignores\n",
      "     |      the column names and just uses position-based resolution.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tblA\")\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n",
      "     |      ...     schema=[\"age\", \"name\"]\n",
      "     |      ... )\n",
      "     |      >>> df.write.saveAsTable(\"tblA\")\n",
      "     |      \n",
      "     |      Insert the data into 'tblA' table but with different column names.\n",
      "     |      \n",
      "     |      >>> df.selectExpr(\"age AS col1\", \"name AS col2\").write.insertInto(\"tblA\")\n",
      "     |      >>> spark.read.table(\"tblA\").sort(\"age\").show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      |120|Hyukjin Kwon|\n",
      "     |      |120|Hyukjin Kwon|\n",
      "     |      |140| Haejoon Lee|\n",
      "     |      |140| Haejoon Lee|\n",
      "     |      +---+------------+\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n",
      "     |  \n",
      "     |  jdbc(self, url: str, table: str, mode: Optional[str] = None, properties: Optional[Dict[str, str]] = None) -> None\n",
      "     |      Saves the content of the :class:`DataFrame` to an external database table via JDBC.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      table : str\n",
      "     |          Name of the table in the external database.\n",
      "     |      mode : str, optional\n",
      "     |          specifies the behavior of the save operation when data already exists.\n",
      "     |      \n",
      "     |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "     |          * ``overwrite``: Overwrite existing data.\n",
      "     |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      "     |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "     |      properties : dict\n",
      "     |          a dictionary of JDBC database connection arguments. Normally at\n",
      "     |          least properties \"user\" and \"password\" with their corresponding values.\n",
      "     |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Don't create too many partitions in parallel on a large cluster;\n",
      "     |      otherwise Spark might crash your external database systems.\n",
      "     |  \n",
      "     |  json(self, path: str, mode: Optional[str] = None, compression: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, lineSep: Optional[str] = None, encoding: Optional[str] = None, ignoreNullFields: Union[bool, str, NoneType] = None) -> None\n",
      "     |      Saves the content of the :class:`DataFrame` in JSON format\n",
      "     |      (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\n",
      "     |      specified path.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          the path in any Hadoop supported file system\n",
      "     |      mode : str, optional\n",
      "     |          specifies the behavior of the save operation when data already exists.\n",
      "     |      \n",
      "     |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "     |          * ``overwrite``: Overwrite existing data.\n",
      "     |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      "     |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a JSON file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a JSON file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.json(d, mode=\"overwrite\")\n",
      "     |      ...\n",
      "     |      ...     # Read the JSON file as a DataFrame.\n",
      "     |      ...     spark.read.format(\"json\").load(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  mode(self, saveMode: Optional[str]) -> 'DataFrameWriter'\n",
      "     |      Specifies the behavior when data or table already exists.\n",
      "     |      \n",
      "     |      Options include:\n",
      "     |      \n",
      "     |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n",
      "     |      * `overwrite`: Overwrite existing data.\n",
      "     |      * `error` or `errorifexists`: Throw an exception if data already exists.\n",
      "     |      * `ignore`: Silently ignore this operation if data already exists.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Raise an error when writing to an existing path.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 80, \"name\": \"Xinrong Meng\"}]\n",
      "     |      ...     ).write.mode(\"error\").format(\"parquet\").save(d) # doctest: +SKIP\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      ...AnalysisException: ...\n",
      "     |      \n",
      "     |      Write a Parquet file back with various options, and read it back.\n",
      "     |      \n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Overwrite the path with a new Parquet file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Append another DataFrame into the Parquet file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 120, \"name\": \"Takuya Ueshin\"}]\n",
      "     |      ...     ).write.mode(\"append\").format(\"parquet\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Append another DataFrame into the Parquet file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 140, \"name\": \"Haejoon Lee\"}]\n",
      "     |      ...     ).write.mode(\"ignore\").format(\"parquet\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the Parquet file as a DataFrame.\n",
      "     |      ...     spark.read.parquet(d).show()\n",
      "     |      +---+-------------+\n",
      "     |      |age|         name|\n",
      "     |      +---+-------------+\n",
      "     |      |120|Takuya Ueshin|\n",
      "     |      |100| Hyukjin Kwon|\n",
      "     |      +---+-------------+\n",
      "     |  \n",
      "     |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameWriter'\n",
      "     |      Adds an output option for the underlying data source.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key : str\n",
      "     |          The key for the option to set.\n",
      "     |      value\n",
      "     |          The value for the option to set.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.range(1).write.option(\"key\", \"value\")\n",
      "     |      <...readwriter.DataFrameWriter object ...>\n",
      "     |      \n",
      "     |      Specify the option 'nullValue' with writing a CSV file.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a CSV file with 'nullValue' option set to 'Hyukjin Kwon'.\n",
      "     |      ...     df = spark.createDataFrame([(100, None)], \"age INT, name STRING\")\n",
      "     |      ...     df.write.option(\"nullValue\", \"Hyukjin Kwon\").mode(\"overwrite\").format(\"csv\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the CSV file as a DataFrame.\n",
      "     |      ...     spark.read.schema(df.schema).format('csv').load(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameWriter'\n",
      "     |      Adds output options for the underlying data source.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **options : dict\n",
      "     |          The dictionary of string keys and primitive-type values.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.range(1).write.option(\"key\", \"value\")\n",
      "     |      <...readwriter.DataFrameWriter object ...>\n",
      "     |      \n",
      "     |      Specify the option 'nullValue' and 'header' with writing a CSV file.\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
      "     |      >>> schema = StructType([\n",
      "     |      ...     StructField(\"age\",IntegerType(),True),\n",
      "     |      ...     StructField(\"name\",StringType(),True),\n",
      "     |      ... ])\n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a CSV file with 'nullValue' option set to 'Hyukjin Kwon',\n",
      "     |      ...     # and 'header' option set to `True`.\n",
      "     |      ...     df = spark.createDataFrame([(100, None)], schema=schema)\n",
      "     |      ...     df.write.options(nullValue=\"Hyukjin Kwon\", header=True).mode(\n",
      "     |      ...         \"overwrite\").format(\"csv\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the CSV file as a DataFrame.\n",
      "     |      ...     spark.read.option(\"header\", True).format('csv').load(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  orc(self, path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None\n",
      "     |      Saves the content of the :class:`DataFrame` in ORC format at the specified path.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          the path in any Hadoop supported file system\n",
      "     |      mode : str, optional\n",
      "     |          specifies the behavior of the save operation when data already exists.\n",
      "     |      \n",
      "     |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "     |          * ``overwrite``: Overwrite existing data.\n",
      "     |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      "     |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "     |      partitionBy : str or list, optional\n",
      "     |          names of partitioning columns\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a ORC file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a ORC file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.orc(d, mode=\"overwrite\")\n",
      "     |      ...\n",
      "     |      ...     # Read the Parquet file as a DataFrame.\n",
      "     |      ...     spark.read.format(\"orc\").load(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  parquet(self, path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None\n",
      "     |      Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          the path in any Hadoop supported file system\n",
      "     |      mode : str, optional\n",
      "     |          specifies the behavior of the save operation when data already exists.\n",
      "     |      \n",
      "     |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "     |          * ``overwrite``: Overwrite existing data.\n",
      "     |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      "     |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "     |      partitionBy : str or list, optional\n",
      "     |          names of partitioning columns\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a Parquet file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a Parquet file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.parquet(d, mode=\"overwrite\")\n",
      "     |      ...\n",
      "     |      ...     # Read the Parquet file as a DataFrame.\n",
      "     |      ...     spark.read.format(\"parquet\").load(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  partitionBy(self, *cols: Union[str, List[str]]) -> 'DataFrameWriter'\n",
      "     |      Partitions the output by the given columns on the file system.\n",
      "     |      \n",
      "     |      If specified, the output is laid out on the file system similar\n",
      "     |      to Hive's partitioning scheme.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str or list\n",
      "     |          name of columns\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a Parquet file in a partitioned manner, and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> import os\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a Parquet file in a partitioned manner.\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}, {\"age\": 120, \"name\": \"Ruifeng Zheng\"}]\n",
      "     |      ...     ).write.partitionBy(\"name\").mode(\"overwrite\").format(\"parquet\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the Parquet file as a DataFrame.\n",
      "     |      ...     spark.read.parquet(d).sort(\"age\").show()\n",
      "     |      ...\n",
      "     |      ...     # Read one partition as a DataFrame.\n",
      "     |      ...     spark.read.parquet(f\"{d}{os.path.sep}name=Hyukjin Kwon\").show()\n",
      "     |      +---+-------------+\n",
      "     |      |age|         name|\n",
      "     |      +---+-------------+\n",
      "     |      |100| Hyukjin Kwon|\n",
      "     |      |120|Ruifeng Zheng|\n",
      "     |      +---+-------------+\n",
      "     |      +---+\n",
      "     |      |age|\n",
      "     |      +---+\n",
      "     |      |100|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  save(self, path: Optional[str] = None, format: Optional[str] = None, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, **options: 'OptionalPrimitiveType') -> None\n",
      "     |      Saves the contents of the :class:`DataFrame` to a data source.\n",
      "     |      \n",
      "     |      The data source is specified by the ``format`` and a set of ``options``.\n",
      "     |      If ``format`` is not specified, the default data source configured by\n",
      "     |      ``spark.sql.sources.default`` will be used.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str, optional\n",
      "     |          the path in a Hadoop supported file system\n",
      "     |      format : str, optional\n",
      "     |          the format used to save\n",
      "     |      mode : str, optional\n",
      "     |          specifies the behavior of the save operation when data already exists.\n",
      "     |      \n",
      "     |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "     |          * ``overwrite``: Overwrite existing data.\n",
      "     |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      "     |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "     |      partitionBy : list, optional\n",
      "     |          names of partitioning columns\n",
      "     |      **options : dict\n",
      "     |          all other string options\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a JSON file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a JSON file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the JSON file as a DataFrame.\n",
      "     |      ...     spark.read.format('json').load(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  saveAsTable(self, name: str, format: Optional[str] = None, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, **options: 'OptionalPrimitiveType') -> None\n",
      "     |      Saves the content of the :class:`DataFrame` as the specified table.\n",
      "     |      \n",
      "     |      In the case the table already exists, behavior of this function depends on the\n",
      "     |      save mode, specified by the `mode` function (default to throwing an exception).\n",
      "     |      When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be\n",
      "     |      the same as that of the existing table.\n",
      "     |      \n",
      "     |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n",
      "     |      * `overwrite`: Overwrite existing data.\n",
      "     |      * `error` or `errorifexists`: Throw an exception if data already exists.\n",
      "     |      * `ignore`: Silently ignore this operation if data already exists.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      When `mode` is `Append`, if there is an existing table, we will use the format and\n",
      "     |      options of the existing table. The column order in the schema of the :class:`DataFrame`\n",
      "     |      doesn't need to be the same as that of the existing table. Unlike\n",
      "     |      :meth:`DataFrameWriter.insertInto`, :meth:`DataFrameWriter.saveAsTable` will use the\n",
      "     |      column names to find the correct column positions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          the table name\n",
      "     |      format : str, optional\n",
      "     |          the format used to save\n",
      "     |      mode : str, optional\n",
      "     |          one of `append`, `overwrite`, `error`, `errorifexists`, `ignore`             (default: error)\n",
      "     |      partitionBy : str or list\n",
      "     |          names of partitioning columns\n",
      "     |      **options : dict\n",
      "     |          all other string options\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Creates a table from a DataFrame, and read it back.\n",
      "     |      \n",
      "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tblA\")\n",
      "     |      >>> spark.createDataFrame([\n",
      "     |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n",
      "     |      ...     schema=[\"age\", \"name\"]\n",
      "     |      ... ).write.saveAsTable(\"tblA\")\n",
      "     |      >>> spark.read.table(\"tblA\").sort(\"age\").show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      |120|Hyukjin Kwon|\n",
      "     |      |140| Haejoon Lee|\n",
      "     |      +---+------------+\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n",
      "     |  \n",
      "     |  sortBy(self, col: Union[str, List[str], Tuple[str, ...]], *cols: Optional[str]) -> 'DataFrameWriter'\n",
      "     |      Sorts the output in each bucket by the given columns on the file system.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col : str, tuple or list\n",
      "     |          a name of a column, or a list of names.\n",
      "     |      cols : str\n",
      "     |          additional names (optional). If `col` is a list it should be empty.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a Parquet file in a sorted-buckted manner, and read it back.\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.functions import input_file_name\n",
      "     |      >>> # Write a DataFrame into a Parquet file in a sorted-bucketed manner.\n",
      "     |      ... _ = spark.sql(\"DROP TABLE IF EXISTS sorted_bucketed_table\")\n",
      "     |      >>> spark.createDataFrame([\n",
      "     |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n",
      "     |      ...     schema=[\"age\", \"name\"]\n",
      "     |      ... ).write.bucketBy(1, \"name\").sortBy(\"age\").mode(\n",
      "     |      ...     \"overwrite\").saveAsTable(\"sorted_bucketed_table\")\n",
      "     |      >>> # Read the Parquet file as a DataFrame.\n",
      "     |      ... spark.read.table(\"sorted_bucketed_table\").sort(\"age\").show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      |120|Hyukjin Kwon|\n",
      "     |      |140| Haejoon Lee|\n",
      "     |      +---+------------+\n",
      "     |      >>> _ = spark.sql(\"DROP TABLE sorted_bucketed_table\")\n",
      "     |  \n",
      "     |  text(self, path: str, compression: Optional[str] = None, lineSep: Optional[str] = None) -> None\n",
      "     |      Saves the content of the DataFrame in a text file at the specified path.\n",
      "     |      The text files will be encoded as UTF-8.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          the path in any Hadoop supported file system\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      Extra options\n",
      "     |          For the extra options, refer to\n",
      "     |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n",
      "     |          for the version you use.\n",
      "     |      \n",
      "     |          .. # noqa\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The DataFrame must have only one column that is of string type.\n",
      "     |      Each row becomes a new line in the output file.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Write a DataFrame into a text file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a text file\n",
      "     |      ...     df = spark.createDataFrame([(\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n",
      "     |      ...     df.write.mode(\"overwrite\").text(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the text file as a DataFrame.\n",
      "     |      ...     spark.read.schema(df.schema).format(\"text\").load(d).sort(\"alphabets\").show()\n",
      "     |      +---------+\n",
      "     |      |alphabets|\n",
      "     |      +---------+\n",
      "     |      |        a|\n",
      "     |      |        b|\n",
      "     |      |        c|\n",
      "     |      +---------+\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from OptionUtils:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DataFrameWriterV2(builtins.object)\n",
      "     |  DataFrameWriterV2(df: 'DataFrame', table: str)\n",
      "     |  \n",
      "     |  Interface used to write a class:`pyspark.sql.dataframe.DataFrame`\n",
      "     |  to external storage using the v2 API.\n",
      "     |  \n",
      "     |  .. versionadded:: 3.1.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df: 'DataFrame', table: str)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  append(self) -> None\n",
      "     |      Append the contents of the data frame to the output table.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  create(self) -> None\n",
      "     |      Create a new table from the contents of the data frame.\n",
      "     |      \n",
      "     |      The new table's schema, partition layout, properties, and other configuration will be\n",
      "     |      based on the configuration set on this writer.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  createOrReplace(self) -> None\n",
      "     |      Create a new table or replace an existing table with the contents of the data frame.\n",
      "     |      \n",
      "     |      The output table's schema, partition layout, properties,\n",
      "     |      and other configuration will be based on the contents of the data frame\n",
      "     |      and the configuration set on this writer.\n",
      "     |      If the table exists, its configuration and data will be replaced.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameWriterV2'\n",
      "     |      Add a write option.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameWriterV2'\n",
      "     |      Add write options.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  overwrite(self, condition: pyspark.sql.column.Column) -> None\n",
      "     |      Overwrite rows matching the given filter condition with the contents of the data frame in\n",
      "     |      the output table.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  overwritePartitions(self) -> None\n",
      "     |      Overwrite all partition for which the data frame contains at least one row with the contents\n",
      "     |      of the data frame in the output table.\n",
      "     |      \n",
      "     |      This operation is equivalent to Hive's `INSERT OVERWRITE ... PARTITION`, which replaces\n",
      "     |      partitions dynamically depending on the contents of the data frame.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  partitionedBy(self, col: pyspark.sql.column.Column, *cols: pyspark.sql.column.Column) -> 'DataFrameWriterV2'\n",
      "     |      Partition the output table created by `create`, `createOrReplace`, or `replace` using\n",
      "     |      the given columns or transforms.\n",
      "     |      \n",
      "     |      When specified, the table data will be stored by these values for efficient reads.\n",
      "     |      \n",
      "     |      For example, when a table is partitioned by day, it may be stored\n",
      "     |      in a directory layout like:\n",
      "     |      \n",
      "     |      * `table/day=2019-06-01/`\n",
      "     |      * `table/day=2019-06-02/`\n",
      "     |      \n",
      "     |      Partitioning is one of the most widely used techniques to optimize physical data layout.\n",
      "     |      It provides a coarse-grained index for skipping unnecessary data reads when queries have\n",
      "     |      predicates on the partitioned columns. In order for partitioning to work well, the number\n",
      "     |      of distinct values in each column should typically be less than tens of thousands.\n",
      "     |      \n",
      "     |      `col` and `cols` support only the following functions:\n",
      "     |      \n",
      "     |      * :py:func:`pyspark.sql.functions.years`\n",
      "     |      * :py:func:`pyspark.sql.functions.months`\n",
      "     |      * :py:func:`pyspark.sql.functions.days`\n",
      "     |      * :py:func:`pyspark.sql.functions.hours`\n",
      "     |      * :py:func:`pyspark.sql.functions.bucket`\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  replace(self) -> None\n",
      "     |      Replace an existing table with the contents of the data frame.\n",
      "     |      \n",
      "     |      The existing table's schema, partition layout, properties, and other configuration will be\n",
      "     |      replaced with the contents of the data frame and the configuration set on this writer.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  tableProperty(self, property: str, value: str) -> 'DataFrameWriterV2'\n",
      "     |      Add table property.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  using(self, provider: str) -> 'DataFrameWriterV2'\n",
      "     |      Specifies a provider for the underlying output data source.\n",
      "     |      Spark's default catalog supports \"parquet\", \"json\", etc.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class GroupedData(pyspark.sql.pandas.group_ops.PandasGroupedOpsMixin)\n",
      "     |  GroupedData(jgd: py4j.java_gateway.JavaObject, df: pyspark.sql.dataframe.DataFrame)\n",
      "     |  \n",
      "     |  A set of methods for aggregations on a :class:`DataFrame`,\n",
      "     |  created by :func:`DataFrame.groupBy`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.3.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GroupedData\n",
      "     |      pyspark.sql.pandas.group_ops.PandasGroupedOpsMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, jgd: py4j.java_gateway.JavaObject, df: pyspark.sql.dataframe.DataFrame)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  agg(self, *exprs: Union[pyspark.sql.column.Column, Dict[str, str]]) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Compute aggregates and returns the result as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The available aggregate functions can be:\n",
      "     |      \n",
      "     |      1. built-in aggregation functions, such as `avg`, `max`, `min`, `sum`, `count`\n",
      "     |      \n",
      "     |      2. group aggregate pandas UDFs, created with :func:`pyspark.sql.functions.pandas_udf`\n",
      "     |      \n",
      "     |         .. note:: There is no partial aggregation with group aggregate UDFs, i.e.,\n",
      "     |             a full shuffle is required. Also, all the data of a group will be loaded into\n",
      "     |             memory, so the user should be aware of the potential OOM risk if data is skewed\n",
      "     |             and certain groups are too large to fit in memory.\n",
      "     |      \n",
      "     |         .. seealso:: :func:`pyspark.sql.functions.pandas_udf`\n",
      "     |      \n",
      "     |      If ``exprs`` is a single :class:`dict` mapping from string to string, then the key\n",
      "     |      is the column to perform aggregation on, and the value is the aggregate function.\n",
      "     |      \n",
      "     |      Alternatively, ``exprs`` can also be a list of aggregate :class:`Column` expressions.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      exprs : dict\n",
      "     |          a dict mapping from column name (string) to aggregate functions (string),\n",
      "     |          or a list of :class:`Column`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Built-in aggregation functions and group aggregate pandas UDFs cannot be mixed\n",
      "     |      in a single call to this function.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import functions as sf\n",
      "     |      >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (3, \"Alice\"), (5, \"Bob\"), (10, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  2|Alice|\n",
      "     |      |  3|Alice|\n",
      "     |      |  5|  Bob|\n",
      "     |      | 10|  Bob|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Group-by name, and count each group.\n",
      "     |      \n",
      "     |      >>> df.groupBy(df.name)\n",
      "     |      GroupedData[grouping...: [name...], value: [age: bigint, name: string], type: GroupBy]\n",
      "     |      \n",
      "     |      >>> df.groupBy(df.name).agg({\"*\": \"count\"}).sort(\"name\").show()\n",
      "     |      +-----+--------+\n",
      "     |      | name|count(1)|\n",
      "     |      +-----+--------+\n",
      "     |      |Alice|       2|\n",
      "     |      |  Bob|       2|\n",
      "     |      +-----+--------+\n",
      "     |      \n",
      "     |      Group-by name, and calculate the minimum age.\n",
      "     |      \n",
      "     |      >>> df.groupBy(df.name).agg(sf.min(df.age)).sort(\"name\").show()\n",
      "     |      +-----+--------+\n",
      "     |      | name|min(age)|\n",
      "     |      +-----+--------+\n",
      "     |      |Alice|       2|\n",
      "     |      |  Bob|       5|\n",
      "     |      +-----+--------+\n",
      "     |      \n",
      "     |      Same as above but uses pandas UDF.\n",
      "     |      \n",
      "     |      >>> @pandas_udf('int', PandasUDFType.GROUPED_AGG)  # doctest: +SKIP\n",
      "     |      ... def min_udf(v):\n",
      "     |      ...     return v.min()\n",
      "     |      ...\n",
      "     |      >>> df.groupBy(df.name).agg(min_udf(df.age)).sort(\"name\").show()  # doctest: +SKIP\n",
      "     |      +-----+------------+\n",
      "     |      | name|min_udf(age)|\n",
      "     |      +-----+------------+\n",
      "     |      |Alice|           2|\n",
      "     |      |  Bob|           5|\n",
      "     |      +-----+------------+\n",
      "     |  \n",
      "     |  avg(self: 'GroupedData', *cols: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Computes average values for each numeric columns for each group.\n",
      "     |      \n",
      "     |      :func:`mean` is an alias for :func:`avg`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str\n",
      "     |          column names. Non-numeric columns are ignored.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\", 80), (3, \"Alice\", 100),\n",
      "     |      ...     (5, \"Bob\", 120), (10, \"Bob\", 140)], [\"age\", \"name\", \"height\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+-----+------+\n",
      "     |      |age| name|height|\n",
      "     |      +---+-----+------+\n",
      "     |      |  2|Alice|    80|\n",
      "     |      |  3|Alice|   100|\n",
      "     |      |  5|  Bob|   120|\n",
      "     |      | 10|  Bob|   140|\n",
      "     |      +---+-----+------+\n",
      "     |      \n",
      "     |      Group-by name, and calculate the mean of the age in each group.\n",
      "     |      \n",
      "     |      >>> df.groupBy(\"name\").avg('age').sort(\"name\").show()\n",
      "     |      +-----+--------+\n",
      "     |      | name|avg(age)|\n",
      "     |      +-----+--------+\n",
      "     |      |Alice|     2.5|\n",
      "     |      |  Bob|     7.5|\n",
      "     |      +-----+--------+\n",
      "     |      \n",
      "     |      Calculate the mean of the age and height in all data.\n",
      "     |      \n",
      "     |      >>> df.groupBy().avg('age', 'height').show()\n",
      "     |      +--------+-----------+\n",
      "     |      |avg(age)|avg(height)|\n",
      "     |      +--------+-----------+\n",
      "     |      |     5.0|      110.0|\n",
      "     |      +--------+-----------+\n",
      "     |  \n",
      "     |  count(self: 'GroupedData') -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Counts the number of records for each group.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(2, \"Alice\"), (3, \"Alice\"), (5, \"Bob\"), (10, \"Bob\")], [\"age\", \"name\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  2|Alice|\n",
      "     |      |  3|Alice|\n",
      "     |      |  5|  Bob|\n",
      "     |      | 10|  Bob|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Group-by name, and count each group.\n",
      "     |      \n",
      "     |      >>> df.groupBy(df.name).count().sort(\"name\").show()\n",
      "     |      +-----+-----+\n",
      "     |      | name|count|\n",
      "     |      +-----+-----+\n",
      "     |      |Alice|    2|\n",
      "     |      |  Bob|    2|\n",
      "     |      +-----+-----+\n",
      "     |  \n",
      "     |  max(self: 'GroupedData', *cols: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Computes the max value for each numeric columns for each group.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\", 80), (3, \"Alice\", 100),\n",
      "     |      ...     (5, \"Bob\", 120), (10, \"Bob\", 140)], [\"age\", \"name\", \"height\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+-----+------+\n",
      "     |      |age| name|height|\n",
      "     |      +---+-----+------+\n",
      "     |      |  2|Alice|    80|\n",
      "     |      |  3|Alice|   100|\n",
      "     |      |  5|  Bob|   120|\n",
      "     |      | 10|  Bob|   140|\n",
      "     |      +---+-----+------+\n",
      "     |      \n",
      "     |      Group-by name, and calculate the max of the age in each group.\n",
      "     |      \n",
      "     |      >>> df.groupBy(\"name\").max(\"age\").sort(\"name\").show()\n",
      "     |      +-----+--------+\n",
      "     |      | name|max(age)|\n",
      "     |      +-----+--------+\n",
      "     |      |Alice|       3|\n",
      "     |      |  Bob|      10|\n",
      "     |      +-----+--------+\n",
      "     |      \n",
      "     |      Calculate the max of the age and height in all data.\n",
      "     |      \n",
      "     |      >>> df.groupBy().max(\"age\", \"height\").show()\n",
      "     |      +--------+-----------+\n",
      "     |      |max(age)|max(height)|\n",
      "     |      +--------+-----------+\n",
      "     |      |      10|        140|\n",
      "     |      +--------+-----------+\n",
      "     |  \n",
      "     |  mean(self: 'GroupedData', *cols: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Computes average values for each numeric columns for each group.\n",
      "     |      \n",
      "     |      :func:`mean` is an alias for :func:`avg`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str\n",
      "     |          column names. Non-numeric columns are ignored.\n",
      "     |  \n",
      "     |  min(self: 'GroupedData', *cols: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Computes the min value for each numeric column for each group.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str\n",
      "     |          column names. Non-numeric columns are ignored.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\", 80), (3, \"Alice\", 100),\n",
      "     |      ...     (5, \"Bob\", 120), (10, \"Bob\", 140)], [\"age\", \"name\", \"height\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+-----+------+\n",
      "     |      |age| name|height|\n",
      "     |      +---+-----+------+\n",
      "     |      |  2|Alice|    80|\n",
      "     |      |  3|Alice|   100|\n",
      "     |      |  5|  Bob|   120|\n",
      "     |      | 10|  Bob|   140|\n",
      "     |      +---+-----+------+\n",
      "     |      \n",
      "     |      Group-by name, and calculate the min of the age in each group.\n",
      "     |      \n",
      "     |      >>> df.groupBy(\"name\").min(\"age\").sort(\"name\").show()\n",
      "     |      +-----+--------+\n",
      "     |      | name|min(age)|\n",
      "     |      +-----+--------+\n",
      "     |      |Alice|       2|\n",
      "     |      |  Bob|       5|\n",
      "     |      +-----+--------+\n",
      "     |      \n",
      "     |      Calculate the min of the age and height in all data.\n",
      "     |      \n",
      "     |      >>> df.groupBy().min(\"age\", \"height\").show()\n",
      "     |      +--------+-----------+\n",
      "     |      |min(age)|min(height)|\n",
      "     |      +--------+-----------+\n",
      "     |      |       2|         80|\n",
      "     |      +--------+-----------+\n",
      "     |  \n",
      "     |  pivot(self, pivot_col: str, values: Optional[List[ForwardRef('LiteralType')]] = None) -> 'GroupedData'\n",
      "     |      Pivots a column of the current :class:`DataFrame` and perform the specified aggregation.\n",
      "     |      There are two versions of the pivot function: one that requires the caller\n",
      "     |      to specify the list of distinct values to pivot on, and one that does not.\n",
      "     |      The latter is more concise but less efficient,\n",
      "     |      because Spark needs to first compute the list of distinct values internally.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      pivot_col : str\n",
      "     |          Name of the column to pivot.\n",
      "     |      values : list, optional\n",
      "     |          List of values that will be translated to columns in the output DataFrame.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df1 = spark.createDataFrame([\n",
      "     |      ...     Row(course=\"dotNET\", year=2012, earnings=10000),\n",
      "     |      ...     Row(course=\"Java\", year=2012, earnings=20000),\n",
      "     |      ...     Row(course=\"dotNET\", year=2012, earnings=5000),\n",
      "     |      ...     Row(course=\"dotNET\", year=2013, earnings=48000),\n",
      "     |      ...     Row(course=\"Java\", year=2013, earnings=30000),\n",
      "     |      ... ])\n",
      "     |      >>> df1.show()\n",
      "     |      +------+----+--------+\n",
      "     |      |course|year|earnings|\n",
      "     |      +------+----+--------+\n",
      "     |      |dotNET|2012|   10000|\n",
      "     |      |  Java|2012|   20000|\n",
      "     |      |dotNET|2012|    5000|\n",
      "     |      |dotNET|2013|   48000|\n",
      "     |      |  Java|2013|   30000|\n",
      "     |      +------+----+--------+\n",
      "     |      >>> df2 = spark.createDataFrame([\n",
      "     |      ...     Row(training=\"expert\", sales=Row(course=\"dotNET\", year=2012, earnings=10000)),\n",
      "     |      ...     Row(training=\"junior\", sales=Row(course=\"Java\", year=2012, earnings=20000)),\n",
      "     |      ...     Row(training=\"expert\", sales=Row(course=\"dotNET\", year=2012, earnings=5000)),\n",
      "     |      ...     Row(training=\"junior\", sales=Row(course=\"dotNET\", year=2013, earnings=48000)),\n",
      "     |      ...     Row(training=\"expert\", sales=Row(course=\"Java\", year=2013, earnings=30000)),\n",
      "     |      ... ])  # doctest: +SKIP\n",
      "     |      >>> df2.show()  # doctest: +SKIP\n",
      "     |      +--------+--------------------+\n",
      "     |      |training|               sales|\n",
      "     |      +--------+--------------------+\n",
      "     |      |  expert|{dotNET, 2012, 10...|\n",
      "     |      |  junior| {Java, 2012, 20000}|\n",
      "     |      |  expert|{dotNET, 2012, 5000}|\n",
      "     |      |  junior|{dotNET, 2013, 48...|\n",
      "     |      |  expert| {Java, 2013, 30000}|\n",
      "     |      +--------+--------------------+\n",
      "     |      \n",
      "     |      Compute the sum of earnings for each year by course with each course as a separate column\n",
      "     |      \n",
      "     |      >>> df1.groupBy(\"year\").pivot(\"course\", [\"dotNET\", \"Java\"]).sum(\"earnings\").show()\n",
      "     |      +----+------+-----+\n",
      "     |      |year|dotNET| Java|\n",
      "     |      +----+------+-----+\n",
      "     |      |2012| 15000|20000|\n",
      "     |      |2013| 48000|30000|\n",
      "     |      +----+------+-----+\n",
      "     |      \n",
      "     |      Or without specifying column values (less efficient)\n",
      "     |      \n",
      "     |      >>> df1.groupBy(\"year\").pivot(\"course\").sum(\"earnings\").show()\n",
      "     |      +----+-----+------+\n",
      "     |      |year| Java|dotNET|\n",
      "     |      +----+-----+------+\n",
      "     |      |2012|20000| 15000|\n",
      "     |      |2013|30000| 48000|\n",
      "     |      +----+-----+------+\n",
      "     |      >>> df2.groupBy(\"sales.year\").pivot(\"sales.course\").sum(\"sales.earnings\").show()\n",
      "     |      ... # doctest: +SKIP\n",
      "     |      +----+-----+------+\n",
      "     |      |year| Java|dotNET|\n",
      "     |      +----+-----+------+\n",
      "     |      |2012|20000| 15000|\n",
      "     |      |2013|30000| 48000|\n",
      "     |      +----+-----+------+\n",
      "     |  \n",
      "     |  sum(self: 'GroupedData', *cols: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Computes the sum for each numeric columns for each group.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str\n",
      "     |          column names. Non-numeric columns are ignored.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([\n",
      "     |      ...     (2, \"Alice\", 80), (3, \"Alice\", 100),\n",
      "     |      ...     (5, \"Bob\", 120), (10, \"Bob\", 140)], [\"age\", \"name\", \"height\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+-----+------+\n",
      "     |      |age| name|height|\n",
      "     |      +---+-----+------+\n",
      "     |      |  2|Alice|    80|\n",
      "     |      |  3|Alice|   100|\n",
      "     |      |  5|  Bob|   120|\n",
      "     |      | 10|  Bob|   140|\n",
      "     |      +---+-----+------+\n",
      "     |      \n",
      "     |      Group-by name, and calculate the sum of the age in each group.\n",
      "     |      \n",
      "     |      >>> df.groupBy(\"name\").sum(\"age\").sort(\"name\").show()\n",
      "     |      +-----+--------+\n",
      "     |      | name|sum(age)|\n",
      "     |      +-----+--------+\n",
      "     |      |Alice|       5|\n",
      "     |      |  Bob|      15|\n",
      "     |      +-----+--------+\n",
      "     |      \n",
      "     |      Calculate the sum of the age and height in all data.\n",
      "     |      \n",
      "     |      >>> df.groupBy().sum(\"age\", \"height\").show()\n",
      "     |      +--------+-----------+\n",
      "     |      |sum(age)|sum(height)|\n",
      "     |      +--------+-----------+\n",
      "     |      |      20|        440|\n",
      "     |      +--------+-----------+\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pyspark.sql.pandas.group_ops.PandasGroupedOpsMixin:\n",
      "     |  \n",
      "     |  apply(self, udf: 'GroupedMapPandasUserDefinedFunction') -> pyspark.sql.dataframe.DataFrame\n",
      "     |      It is an alias of :meth:`pyspark.sql.GroupedData.applyInPandas`; however, it takes a\n",
      "     |      :meth:`pyspark.sql.functions.pandas_udf` whereas\n",
      "     |      :meth:`pyspark.sql.GroupedData.applyInPandas` takes a Python native function.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Support Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      udf : :func:`pyspark.sql.functions.pandas_udf`\n",
      "     |          a grouped map user-defined function returned by\n",
      "     |          :func:`pyspark.sql.functions.pandas_udf`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      It is preferred to use :meth:`pyspark.sql.GroupedData.applyInPandas` over this\n",
      "     |      API. This API will be deprecated in the future releases.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
      "     |      ...     (\"id\", \"v\"))\n",
      "     |      >>> @pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\n",
      "     |      ... def normalize(pdf):\n",
      "     |      ...     v = pdf.v\n",
      "     |      ...     return pdf.assign(v=(v - v.mean()) / v.std())\n",
      "     |      ...\n",
      "     |      >>> df.groupby(\"id\").apply(normalize).show()  # doctest: +SKIP\n",
      "     |      +---+-------------------+\n",
      "     |      | id|                  v|\n",
      "     |      +---+-------------------+\n",
      "     |      |  1|-0.7071067811865475|\n",
      "     |      |  1| 0.7071067811865475|\n",
      "     |      |  2|-0.8320502943378437|\n",
      "     |      |  2|-0.2773500981126146|\n",
      "     |      |  2| 1.1094003924504583|\n",
      "     |      +---+-------------------+\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.functions.pandas_udf\n",
      "     |  \n",
      "     |  applyInPandas(self, func: 'PandasGroupedMapFunction', schema: Union[pyspark.sql.types.StructType, str]) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Maps each group of the current :class:`DataFrame` using a pandas udf and returns the result\n",
      "     |      as a `DataFrame`.\n",
      "     |      \n",
      "     |      The function should take a `pandas.DataFrame` and return another\n",
      "     |      `pandas.DataFrame`. Alternatively, the user can pass a function that takes\n",
      "     |      a tuple of the grouping key(s) and a `pandas.DataFrame`.\n",
      "     |      For each group, all columns are passed together as a `pandas.DataFrame`\n",
      "     |      to the user-function and the returned `pandas.DataFrame` are combined as a\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The `schema` should be a :class:`StructType` describing the schema of the returned\n",
      "     |      `pandas.DataFrame`. The column labels of the returned `pandas.DataFrame` must either match\n",
      "     |      the field names in the defined schema if specified as strings, or match the\n",
      "     |      field data types by position if not strings, e.g. integer indices.\n",
      "     |      The length of the returned `pandas.DataFrame` can be arbitrary.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Support Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : function\n",
      "     |          a Python native function that takes a `pandas.DataFrame` and outputs a\n",
      "     |          `pandas.DataFrame`, or that takes one tuple (grouping keys) and a\n",
      "     |          `pandas.DataFrame` and outputs a `pandas.DataFrame`.\n",
      "     |      schema : :class:`pyspark.sql.types.DataType` or str\n",
      "     |          the return type of the `func` in PySpark. The value can be either a\n",
      "     |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import pandas as pd  # doctest: +SKIP\n",
      "     |      >>> from pyspark.sql.functions import pandas_udf, ceil\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
      "     |      ...     (\"id\", \"v\"))  # doctest: +SKIP\n",
      "     |      >>> def normalize(pdf):\n",
      "     |      ...     v = pdf.v\n",
      "     |      ...     return pdf.assign(v=(v - v.mean()) / v.std())\n",
      "     |      ...\n",
      "     |      >>> df.groupby(\"id\").applyInPandas(\n",
      "     |      ...     normalize, schema=\"id long, v double\").show()  # doctest: +SKIP\n",
      "     |      +---+-------------------+\n",
      "     |      | id|                  v|\n",
      "     |      +---+-------------------+\n",
      "     |      |  1|-0.7071067811865475|\n",
      "     |      |  1| 0.7071067811865475|\n",
      "     |      |  2|-0.8320502943378437|\n",
      "     |      |  2|-0.2773500981126146|\n",
      "     |      |  2| 1.1094003924504583|\n",
      "     |      +---+-------------------+\n",
      "     |      \n",
      "     |      Alternatively, the user can pass a function that takes two arguments.\n",
      "     |      In this case, the grouping key(s) will be passed as the first argument and the data will\n",
      "     |      be passed as the second argument. The grouping key(s) will be passed as a tuple of numpy\n",
      "     |      data types, e.g., `numpy.int32` and `numpy.float64`. The data will still be passed in\n",
      "     |      as a `pandas.DataFrame` containing all columns from the original Spark DataFrame.\n",
      "     |      This is useful when the user does not want to hardcode grouping key(s) in the function.\n",
      "     |      \n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
      "     |      ...     (\"id\", \"v\"))  # doctest: +SKIP\n",
      "     |      >>> def mean_func(key, pdf):\n",
      "     |      ...     # key is a tuple of one numpy.int64, which is the value\n",
      "     |      ...     # of 'id' for the current group\n",
      "     |      ...     return pd.DataFrame([key + (pdf.v.mean(),)])\n",
      "     |      ...\n",
      "     |      >>> df.groupby('id').applyInPandas(\n",
      "     |      ...     mean_func, schema=\"id long, v double\").show()  # doctest: +SKIP\n",
      "     |      +---+---+\n",
      "     |      | id|  v|\n",
      "     |      +---+---+\n",
      "     |      |  1|1.5|\n",
      "     |      |  2|6.0|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      >>> def sum_func(key, pdf):\n",
      "     |      ...     # key is a tuple of two numpy.int64s, which is the values\n",
      "     |      ...     # of 'id' and 'ceil(df.v / 2)' for the current group\n",
      "     |      ...     return pd.DataFrame([key + (pdf.v.sum(),)])\n",
      "     |      ...\n",
      "     |      >>> df.groupby(df.id, ceil(df.v / 2)).applyInPandas(\n",
      "     |      ...     sum_func, schema=\"id long, `ceil(v / 2)` long, v double\").show()  # doctest: +SKIP\n",
      "     |      +---+-----------+----+\n",
      "     |      | id|ceil(v / 2)|   v|\n",
      "     |      +---+-----------+----+\n",
      "     |      |  2|          5|10.0|\n",
      "     |      |  1|          1| 3.0|\n",
      "     |      |  2|          3| 5.0|\n",
      "     |      |  2|          2| 3.0|\n",
      "     |      +---+-----------+----+\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This function requires a full shuffle. All the data of a group will be loaded\n",
      "     |      into memory, so the user should be aware of the potential OOM risk if data is skewed\n",
      "     |      and certain groups are too large to fit in memory.\n",
      "     |      \n",
      "     |      This API is experimental.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.functions.pandas_udf\n",
      "     |  \n",
      "     |  applyInPandasWithState(self, func: 'PandasGroupedMapFunctionWithState', outputStructType: Union[pyspark.sql.types.StructType, str], stateStructType: Union[pyspark.sql.types.StructType, str], outputMode: str, timeoutConf: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Applies the given function to each group of data, while maintaining a user-defined\n",
      "     |      per-group state. The result Dataset will represent the flattened record returned by the\n",
      "     |      function.\n",
      "     |      \n",
      "     |      For a streaming :class:`DataFrame`, the function will be invoked first for all input groups\n",
      "     |      and then for all timed out states where the input data is set to be empty. Updates to each\n",
      "     |      group's state will be saved across invocations.\n",
      "     |      \n",
      "     |      The function should take parameters (key, Iterator[`pandas.DataFrame`], state) and\n",
      "     |      return another Iterator[`pandas.DataFrame`]. The grouping key(s) will be passed as a tuple\n",
      "     |      of numpy data types, e.g., `numpy.int32` and `numpy.float64`. The state will be passed as\n",
      "     |      :class:`pyspark.sql.streaming.state.GroupState`.\n",
      "     |      \n",
      "     |      For each group, all columns are passed together as `pandas.DataFrame` to the user-function,\n",
      "     |      and the returned `pandas.DataFrame` across all invocations are combined as a\n",
      "     |      :class:`DataFrame`. Note that the user function should not make a guess of the number of\n",
      "     |      elements in the iterator. To process all data, the user function needs to iterate all\n",
      "     |      elements and process them. On the other hand, the user function is not strictly required to\n",
      "     |      iterate through all elements in the iterator if it intends to read a part of data.\n",
      "     |      \n",
      "     |      The `outputStructType` should be a :class:`StructType` describing the schema of all\n",
      "     |      elements in the returned value, `pandas.DataFrame`. The column labels of all elements in\n",
      "     |      returned `pandas.DataFrame` must either match the field names in the defined schema if\n",
      "     |      specified as strings, or match the field data types by position if not strings,\n",
      "     |      e.g. integer indices.\n",
      "     |      \n",
      "     |      The `stateStructType` should be :class:`StructType` describing the schema of the\n",
      "     |      user-defined state. The value of the state will be presented as a tuple, as well as the\n",
      "     |      update should be performed with the tuple. The corresponding Python types for\n",
      "     |      :class:DataType are supported. Please refer to the page\n",
      "     |      https://spark.apache.org/docs/latest/sql-ref-datatypes.html (Python tab).\n",
      "     |      \n",
      "     |      The size of each `pandas.DataFrame` in both the input and output can be arbitrary. The\n",
      "     |      number of `pandas.DataFrame` in both the input and output can also be arbitrary.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : function\n",
      "     |          a Python native function to be called on every group. It should take parameters\n",
      "     |          (key, Iterator[`pandas.DataFrame`], state) and return Iterator[`pandas.DataFrame`].\n",
      "     |          Note that the type of the key is tuple and the type of the state is\n",
      "     |          :class:`pyspark.sql.streaming.state.GroupState`.\n",
      "     |      outputStructType : :class:`pyspark.sql.types.DataType` or str\n",
      "     |          the type of the output records. The value can be either a\n",
      "     |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "     |      stateStructType : :class:`pyspark.sql.types.DataType` or str\n",
      "     |          the type of the user-defined state. The value can be either a\n",
      "     |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "     |      outputMode : str\n",
      "     |          the output mode of the function.\n",
      "     |      timeoutConf : str\n",
      "     |          timeout configuration for groups that do not receive data for a while. valid values\n",
      "     |          are defined in :class:`pyspark.sql.streaming.state.GroupStateTimeout`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import pandas as pd  # doctest: +SKIP\n",
      "     |      >>> from pyspark.sql.streaming.state import GroupStateTimeout\n",
      "     |      >>> def count_fn(key, pdf_iter, state):\n",
      "     |      ...     assert isinstance(state, GroupStateImpl)\n",
      "     |      ...     total_len = 0\n",
      "     |      ...     for pdf in pdf_iter:\n",
      "     |      ...         total_len += len(pdf)\n",
      "     |      ...     state.update((total_len,))\n",
      "     |      ...     yield pd.DataFrame({\"id\": [key[0]], \"countAsString\": [str(total_len)]})\n",
      "     |      ...\n",
      "     |      >>> df.groupby(\"id\").applyInPandasWithState(\n",
      "     |      ...     count_fn, outputStructType=\"id long, countAsString string\",\n",
      "     |      ...     stateStructType=\"len long\", outputMode=\"Update\",\n",
      "     |      ...     timeoutConf=GroupStateTimeout.NoTimeout) # doctest: +SKIP\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This function requires a full shuffle.\n",
      "     |      \n",
      "     |      This API is experimental.\n",
      "     |  \n",
      "     |  cogroup(self, other: 'GroupedData') -> 'PandasCogroupedOps'\n",
      "     |      Cogroups this group with another group so that we can run cogrouped operations.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Support Spark Connect.\n",
      "     |      \n",
      "     |      See :class:`PandasCogroupedOps` for the operations that can be run.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pyspark.sql.pandas.group_ops.PandasGroupedOpsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class HiveContext(SQLContext)\n",
      "     |  HiveContext(sparkContext: pyspark.context.SparkContext, sparkSession: Optional[pyspark.sql.session.SparkSession] = None, jhiveContext: Optional[py4j.java_gateway.JavaObject] = None)\n",
      "     |  \n",
      "     |  A variant of Spark SQL that integrates with data stored in Hive.\n",
      "     |  \n",
      "     |  Configuration for Hive is read from ``hive-site.xml`` on the classpath.\n",
      "     |  It supports running both SQL and HiveQL commands.\n",
      "     |  \n",
      "     |  .. deprecated:: 2.0.0\n",
      "     |      Use SparkSession.builder.enableHiveSupport().getOrCreate().\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  sparkContext : :class:`SparkContext`\n",
      "     |      The SparkContext to wrap.\n",
      "     |  jhiveContext : optional\n",
      "     |      An optional JVM Scala HiveContext. If set, we do not instantiate a new\n",
      "     |      :class:`HiveContext` in the JVM, instead we make all calls to this object.\n",
      "     |      This is only for internal use.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HiveContext\n",
      "     |      SQLContext\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sparkContext: pyspark.context.SparkContext, sparkSession: Optional[pyspark.sql.session.SparkSession] = None, jhiveContext: Optional[py4j.java_gateway.JavaObject] = None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  refreshTable(self, tableName: str) -> None\n",
      "     |      Invalidate and refresh all the cached metadata of the given\n",
      "     |      table. For performance reasons, Spark SQL or the external data source\n",
      "     |      library it uses might cache certain metadata about a table, such as the\n",
      "     |      location of blocks. When those change outside of Spark SQL, users should\n",
      "     |      call this function to invalidate the cache.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SQLContext:\n",
      "     |  \n",
      "     |  cacheTable(self, tableName: str) -> None\n",
      "     |      Caches the specified table in-memory.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  clearCache(self) -> None\n",
      "     |      Removes all cached tables from the in-memory cache.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  createDataFrame(self, data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      "     |      \n",
      "     |      When ``schema`` is a list of column names, the type of each column\n",
      "     |      will be inferred from ``data``.\n",
      "     |      \n",
      "     |      When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "     |      from ``data``, which should be an RDD of :class:`Row`,\n",
      "     |      or :class:`namedtuple`, or :class:`dict`.\n",
      "     |      \n",
      "     |      When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string it must match\n",
      "     |      the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      "     |      :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "     |      :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\",\n",
      "     |      each record will also be wrapped into a tuple, which can be converted to row later.\n",
      "     |      \n",
      "     |      If schema inference is needed, ``samplingRatio`` is used to determine the ratio of\n",
      "     |      rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 2.0.0\n",
      "     |         The ``schema`` parameter can be a :class:`pyspark.sql.types.DataType` or a\n",
      "     |         datatype string after 2.0.\n",
      "     |         If it's not a :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "     |         :class:`pyspark.sql.types.StructType` and each record will also be wrapped into a tuple.\n",
      "     |      \n",
      "     |      .. versionchanged:: 2.1.0\n",
      "     |         Added verifySchema.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : :class:`RDD` or iterable\n",
      "     |          an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "     |          :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
      "     |          :class:`pandas.DataFrame`.\n",
      "     |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "     |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "     |          column names, default is None.  The data type string format equals to\n",
      "     |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "     |          omit the ``struct<>``.\n",
      "     |      samplingRatio : float, optional\n",
      "     |          the sample ratio of rows used for inferring\n",
      "     |      verifySchema : bool, optional\n",
      "     |          verify data types of every row against schema. Enabled by default.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> l = [('Alice', 1)]\n",
      "     |      >>> sqlContext.createDataFrame(l).collect()\n",
      "     |      [Row(_1='Alice', _2=1)]\n",
      "     |      >>> sqlContext.createDataFrame(l, ['name', 'age']).collect()\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      \n",
      "     |      >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "     |      >>> sqlContext.createDataFrame(d).collect()\n",
      "     |      [Row(age=1, name='Alice')]\n",
      "     |      \n",
      "     |      >>> rdd = sc.parallelize(l)\n",
      "     |      >>> sqlContext.createDataFrame(rdd).collect()\n",
      "     |      [Row(_1='Alice', _2=1)]\n",
      "     |      >>> df = sqlContext.createDataFrame(rdd, ['name', 'age'])\n",
      "     |      >>> df.collect()\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      \n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> Person = Row('name', 'age')\n",
      "     |      >>> person = rdd.map(lambda r: Person(*r))\n",
      "     |      >>> df2 = sqlContext.createDataFrame(person)\n",
      "     |      >>> df2.collect()\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.types import *\n",
      "     |      >>> schema = StructType([\n",
      "     |      ...    StructField(\"name\", StringType(), True),\n",
      "     |      ...    StructField(\"age\", IntegerType(), True)])\n",
      "     |      >>> df3 = sqlContext.createDataFrame(rdd, schema)\n",
      "     |      >>> df3.collect()\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      \n",
      "     |      >>> sqlContext.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      >>> sqlContext.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "     |      [Row(0=1, 1=2)]\n",
      "     |      \n",
      "     |      >>> sqlContext.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      "     |      [Row(a='Alice', b=1)]\n",
      "     |      >>> rdd = rdd.map(lambda row: row[1])\n",
      "     |      >>> sqlContext.createDataFrame(rdd, \"int\").collect()\n",
      "     |      [Row(value=1)]\n",
      "     |      >>> sqlContext.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      Py4JJavaError: ...\n",
      "     |  \n",
      "     |  createExternalTable(self, tableName: str, path: Optional[str] = None, source: Optional[str] = None, schema: Optional[pyspark.sql.types.StructType] = None, **options: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Creates an external table based on the dataset in a data source.\n",
      "     |      \n",
      "     |      It returns the DataFrame associated with the external table.\n",
      "     |      \n",
      "     |      The data source is specified by the ``source`` and a set of ``options``.\n",
      "     |      If ``source`` is not specified, the default data source configured by\n",
      "     |      ``spark.sql.sources.default`` will be used.\n",
      "     |      \n",
      "     |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
      "     |      created external table.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |  \n",
      "     |  dropTempTable(self, tableName: str) -> None\n",
      "     |      Remove the temporary table from catalog.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> sqlContext.dropTempTable(\"table1\")\n",
      "     |  \n",
      "     |  getConf(self, key: str, defaultValue: Union[str, NoneType, pyspark._globals._NoValueType] = <no value>) -> Optional[str]\n",
      "     |      Returns the value of Spark SQL configuration property for the given key.\n",
      "     |      \n",
      "     |      If the key is not set and defaultValue is set, return\n",
      "     |      defaultValue. If the key is not set and defaultValue is not set, return\n",
      "     |      the system default value.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\")\n",
      "     |      '200'\n",
      "     |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", \"10\")\n",
      "     |      '10'\n",
      "     |      >>> sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"50\")\n",
      "     |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", \"10\")\n",
      "     |      '50'\n",
      "     |  \n",
      "     |  newSession(self) -> 'SQLContext'\n",
      "     |      Returns a new SQLContext as new session, that has separate SQLConf,\n",
      "     |      registered temporary views and UDFs, but shared SparkContext and\n",
      "     |      table cache.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |  \n",
      "     |  range(self, start: int, end: Optional[int] = None, step: int = 1, numPartitions: Optional[int] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n",
      "     |      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n",
      "     |      step value ``step``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      start : int\n",
      "     |          the start value\n",
      "     |      end : int, optional\n",
      "     |          the end value (exclusive)\n",
      "     |      step : int, optional\n",
      "     |          the incremental step (default: 1)\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions of the DataFrame\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.range(1, 7, 2).collect()\n",
      "     |      [Row(id=1), Row(id=3), Row(id=5)]\n",
      "     |      \n",
      "     |      If only one argument is specified, it will be used as the end value.\n",
      "     |      \n",
      "     |      >>> sqlContext.range(3).collect()\n",
      "     |      [Row(id=0), Row(id=1), Row(id=2)]\n",
      "     |  \n",
      "     |  registerDataFrameAsTable(self, df: pyspark.sql.dataframe.DataFrame, tableName: str) -> None\n",
      "     |      Registers the given :class:`DataFrame` as a temporary table in the catalog.\n",
      "     |      \n",
      "     |      Temporary tables exist only during the lifetime of this instance of :class:`SQLContext`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |  \n",
      "     |  registerFunction(self, name: str, f: Callable[..., Any], returnType: Optional[pyspark.sql.types.DataType] = None) -> 'UserDefinedFunctionLike'\n",
      "     |      An alias for :func:`spark.udf.register`.\n",
      "     |      See :meth:`pyspark.sql.UDFRegistration.register`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2.0\n",
      "     |      \n",
      "     |      .. deprecated:: 2.3.0\n",
      "     |          Use :func:`spark.udf.register` instead.\n",
      "     |  \n",
      "     |  registerJavaFunction(self, name: str, javaClassName: str, returnType: Optional[pyspark.sql.types.DataType] = None) -> None\n",
      "     |      An alias for :func:`spark.udf.registerJavaFunction`.\n",
      "     |      See :meth:`pyspark.sql.UDFRegistration.registerJavaFunction`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      .. deprecated:: 2.3.0\n",
      "     |          Use :func:`spark.udf.registerJavaFunction` instead.\n",
      "     |  \n",
      "     |  setConf(self, key: str, value: Union[bool, int, str]) -> None\n",
      "     |      Sets the given Spark SQL configuration property.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |  \n",
      "     |  sql(self, sqlQuery: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns a :class:`DataFrame` representing the result of the given query.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> df2 = sqlContext.sql(\"SELECT field1 AS f1, field2 as f2 from table1\")\n",
      "     |      >>> df2.collect()\n",
      "     |      [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\n",
      "     |  \n",
      "     |  table(self, tableName: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns the specified table or view as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> df2 = sqlContext.table(\"table1\")\n",
      "     |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |  \n",
      "     |  tableNames(self, dbName: Optional[str] = None) -> List[str]\n",
      "     |      Returns a list of names of tables in the database ``dbName``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dbName: str\n",
      "     |          name of the database to use. Default to the current database.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          list of table names, in string\n",
      "     |      \n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> \"table1\" in sqlContext.tableNames()\n",
      "     |      True\n",
      "     |      >>> \"table1\" in sqlContext.tableNames(\"default\")\n",
      "     |      True\n",
      "     |  \n",
      "     |  tables(self, dbName: Optional[str] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns a :class:`DataFrame` containing names of tables in the given database.\n",
      "     |      \n",
      "     |      If ``dbName`` is not specified, the current database will be used.\n",
      "     |      \n",
      "     |      The returned DataFrame has two columns: ``tableName`` and ``isTemporary``\n",
      "     |      (a column with :class:`BooleanType` indicating if a table is a temporary one or not).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dbName: str, optional\n",
      "     |          name of the database to use.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> df2 = sqlContext.tables()\n",
      "     |      >>> df2.filter(\"tableName = 'table1'\").first()\n",
      "     |      Row(namespace='', tableName='table1', isTemporary=True)\n",
      "     |  \n",
      "     |  uncacheTable(self, tableName: str) -> None\n",
      "     |      Removes the specified table from the in-memory cache.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from SQLContext:\n",
      "     |  \n",
      "     |  getOrCreate(sc: pyspark.context.SparkContext) -> 'SQLContext' from builtins.type\n",
      "     |      Get the existing SQLContext or create a new one with given SparkContext.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      .. deprecated:: 3.0.0\n",
      "     |          Use :func:`SparkSession.builder.getOrCreate()` instead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sc : :class:`SparkContext`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from SQLContext:\n",
      "     |  \n",
      "     |  read\n",
      "     |      Returns a :class:`DataFrameReader` that can be used to read data\n",
      "     |      in as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrameReader`\n",
      "     |  \n",
      "     |  readStream\n",
      "     |      Returns a :class:`DataStreamReader` that can be used to read data streams\n",
      "     |      as a streaming :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataStreamReader`\n",
      "     |      \n",
      "     |      >>> text_sdf = sqlContext.readStream.text(tempfile.mkdtemp())\n",
      "     |      >>> text_sdf.isStreaming\n",
      "     |      True\n",
      "     |  \n",
      "     |  streams\n",
      "     |      Returns a :class:`StreamingQueryManager` that allows managing all the\n",
      "     |      :class:`StreamingQuery` StreamingQueries active on `this` context.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |  \n",
      "     |  udf\n",
      "     |      Returns a :class:`UDFRegistration` for UDF registration.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`UDFRegistration`\n",
      "     |  \n",
      "     |  udtf\n",
      "     |      Returns a :class:`UDTFRegistration` for UDTF registration.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`UDTFRegistration`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SQLContext:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Observation(builtins.object)\n",
      "     |  Observation(name: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  Class to observe (named) metrics on a :class:`DataFrame`.\n",
      "     |  \n",
      "     |  Metrics are aggregation expressions, which are applied to the DataFrame while it is being\n",
      "     |  processed by an action.\n",
      "     |  \n",
      "     |  The metrics have the following guarantees:\n",
      "     |  \n",
      "     |  - It will compute the defined aggregates (metrics) on all the data that is flowing through\n",
      "     |    the Dataset during the action.\n",
      "     |  - It will report the value of the defined aggregate columns as soon as we reach the end of\n",
      "     |    the action.\n",
      "     |  \n",
      "     |  The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or\n",
      "     |  more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that\n",
      "     |  contain references to the input Dataset's columns must always be wrapped in an aggregate\n",
      "     |  function.\n",
      "     |  \n",
      "     |  An Observation instance collects the metrics while the first action is executed. Subsequent\n",
      "     |  actions do not modify the metrics returned by `Observation.get`. Retrieval of the metric via\n",
      "     |  `Observation.get` blocks until the first action has finished and metrics become available.\n",
      "     |  \n",
      "     |  .. versionadded:: 3.3.0\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  This class does not support streaming datasets.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from pyspark.sql.functions import col, count, lit, max\n",
      "     |  >>> from pyspark.sql import Observation\n",
      "     |  >>> df = spark.createDataFrame([[\"Alice\", 2], [\"Bob\", 5]], [\"name\", \"age\"])\n",
      "     |  >>> observation = Observation(\"my metrics\")\n",
      "     |  >>> observed_df = df.observe(observation, count(lit(1)).alias(\"count\"), max(col(\"age\")))\n",
      "     |  >>> observed_df.count()\n",
      "     |  2\n",
      "     |  >>> observation.get\n",
      "     |  {'count': 2, 'max(age)': 5}\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, name: Optional[str] = None) -> None\n",
      "     |      Constructs a named or unnamed Observation instance.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str, optional\n",
      "     |          default is a random UUID string. This is the name of the Observation and the metric.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  get\n",
      "     |      Get the observed metrics.\n",
      "     |      \n",
      "     |      Waits until the observed dataset finishes its first action. Only the result of the\n",
      "     |      first action is available. Subsequent actions do not modify the result.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dict\n",
      "     |          the observed metrics\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PandasCogroupedOps(builtins.object)\n",
      "     |  PandasCogroupedOps(gd1: 'GroupedData', gd2: 'GroupedData')\n",
      "     |  \n",
      "     |  A logical grouping of two :class:`GroupedData`,\n",
      "     |  created by :func:`GroupedData.cogroup`.\n",
      "     |  \n",
      "     |  .. versionadded:: 3.0.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Support Spark Connect.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  This API is experimental.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, gd1: 'GroupedData', gd2: 'GroupedData')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  applyInPandas(self, func: 'PandasCogroupedMapFunction', schema: Union[pyspark.sql.types.StructType, str]) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Applies a function to each cogroup using pandas and returns the result\n",
      "     |      as a `DataFrame`.\n",
      "     |      \n",
      "     |      The function should take two `pandas.DataFrame`\\s and return another\n",
      "     |      `pandas.DataFrame`. Alternatively, the user can pass a function that takes\n",
      "     |      a tuple of the grouping key(s) and the two `pandas.DataFrame`\\s.\n",
      "     |      For each side of the cogroup, all columns are passed together as a\n",
      "     |      `pandas.DataFrame` to the user-function and the returned `pandas.DataFrame` are combined as\n",
      "     |      a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The `schema` should be a :class:`StructType` describing the schema of the returned\n",
      "     |      `pandas.DataFrame`. The column labels of the returned `pandas.DataFrame` must either match\n",
      "     |      the field names in the defined schema if specified as strings, or match the\n",
      "     |      field data types by position if not strings, e.g. integer indices.\n",
      "     |      The length of the returned `pandas.DataFrame` can be arbitrary.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Support Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : function\n",
      "     |          a Python native function that takes two `pandas.DataFrame`\\s, and\n",
      "     |          outputs a `pandas.DataFrame`, or that takes one tuple (grouping keys) and two\n",
      "     |          ``pandas.DataFrame``\\s, and outputs a ``pandas.DataFrame``.\n",
      "     |      schema : :class:`pyspark.sql.types.DataType` or str\n",
      "     |          the return type of the `func` in PySpark. The value can be either a\n",
      "     |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import pandas_udf\n",
      "     |      >>> df1 = spark.createDataFrame(\n",
      "     |      ...     [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
      "     |      ...     (\"time\", \"id\", \"v1\"))\n",
      "     |      >>> df2 = spark.createDataFrame(\n",
      "     |      ...     [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n",
      "     |      ...     (\"time\", \"id\", \"v2\"))\n",
      "     |      >>> def asof_join(l, r):\n",
      "     |      ...     return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n",
      "     |      ...\n",
      "     |      >>> df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(\n",
      "     |      ...     asof_join, schema=\"time int, id int, v1 double, v2 string\"\n",
      "     |      ... ).show()  # doctest: +SKIP\n",
      "     |      +--------+---+---+---+\n",
      "     |      |    time| id| v1| v2|\n",
      "     |      +--------+---+---+---+\n",
      "     |      |20000101|  1|1.0|  x|\n",
      "     |      |20000102|  1|3.0|  x|\n",
      "     |      |20000101|  2|2.0|  y|\n",
      "     |      |20000102|  2|4.0|  y|\n",
      "     |      +--------+---+---+---+\n",
      "     |      \n",
      "     |      Alternatively, the user can define a function that takes three arguments.  In this case,\n",
      "     |      the grouping key(s) will be passed as the first argument and the data will be passed as the\n",
      "     |      second and third arguments.  The grouping key(s) will be passed as a tuple of numpy data\n",
      "     |      types, e.g., `numpy.int32` and `numpy.float64`. The data will still be passed in as two\n",
      "     |      `pandas.DataFrame` containing all columns from the original Spark DataFrames.\n",
      "     |      \n",
      "     |      >>> def asof_join(k, l, r):\n",
      "     |      ...     if k == (1,):\n",
      "     |      ...         return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n",
      "     |      ...     else:\n",
      "     |      ...         return pd.DataFrame(columns=['time', 'id', 'v1', 'v2'])\n",
      "     |      ...\n",
      "     |      >>> df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(\n",
      "     |      ...     asof_join, \"time int, id int, v1 double, v2 string\").show()  # doctest: +SKIP\n",
      "     |      +--------+---+---+---+\n",
      "     |      |    time| id| v1| v2|\n",
      "     |      +--------+---+---+---+\n",
      "     |      |20000101|  1|1.0|  x|\n",
      "     |      |20000102|  1|3.0|  x|\n",
      "     |      +--------+---+---+---+\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This function requires a full shuffle. All the data of a cogroup will be loaded\n",
      "     |      into memory, so the user should be aware of the potential OOM risk if data is skewed\n",
      "     |      and certain groups are too large to fit in memory.\n",
      "     |      \n",
      "     |      This API is experimental.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.functions.pandas_udf\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Row(builtins.tuple)\n",
      "     |  Row(*args: Optional[str], **kwargs: Optional[Any]) -> 'Row'\n",
      "     |  \n",
      "     |  A row in :class:`DataFrame`.\n",
      "     |  The fields in it can be accessed:\n",
      "     |  \n",
      "     |  * like attributes (``row.key``)\n",
      "     |  * like dictionary values (``row[key]``)\n",
      "     |  \n",
      "     |  ``key in row`` will search through row keys.\n",
      "     |  \n",
      "     |  Row can be used to create a row object by using named arguments.\n",
      "     |  It is not allowed to omit a named argument to represent that the value is\n",
      "     |  None or missing. This should be explicitly set to None in this case.\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.0.0\n",
      "     |      Rows created from named arguments no longer have\n",
      "     |      field names sorted alphabetically and will be ordered in the position as\n",
      "     |      entered.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from pyspark.sql import Row\n",
      "     |  >>> row = Row(name=\"Alice\", age=11)\n",
      "     |  >>> row\n",
      "     |  Row(name='Alice', age=11)\n",
      "     |  >>> row['name'], row['age']\n",
      "     |  ('Alice', 11)\n",
      "     |  >>> row.name, row.age\n",
      "     |  ('Alice', 11)\n",
      "     |  >>> 'name' in row\n",
      "     |  True\n",
      "     |  >>> 'wrong_key' in row\n",
      "     |  False\n",
      "     |  \n",
      "     |  Row also can be used to create another Row like class, then it\n",
      "     |  could be used to create Row objects, such as\n",
      "     |  \n",
      "     |  >>> Person = Row(\"name\", \"age\")\n",
      "     |  >>> Person\n",
      "     |  <Row('name', 'age')>\n",
      "     |  >>> 'name' in Person\n",
      "     |  True\n",
      "     |  >>> 'wrong_key' in Person\n",
      "     |  False\n",
      "     |  >>> Person(\"Alice\", 11)\n",
      "     |  Row(name='Alice', age=11)\n",
      "     |  \n",
      "     |  This form can also be used to create rows as tuple values, i.e. with unnamed\n",
      "     |  fields.\n",
      "     |  \n",
      "     |  >>> row1 = Row(\"Alice\", 11)\n",
      "     |  >>> row2 = Row(name=\"Alice\", age=11)\n",
      "     |  >>> row1 == row2\n",
      "     |  True\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Row\n",
      "     |      builtins.tuple\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, *args: Any) -> 'Row'\n",
      "     |      create new Row object\n",
      "     |  \n",
      "     |  __contains__(self, item: Any) -> bool\n",
      "     |      Return key in self.\n",
      "     |  \n",
      "     |  __getattr__(self, item: str) -> Any\n",
      "     |  \n",
      "     |  __getitem__(self, item: Any) -> Any\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __reduce__(self) -> Union[str, Tuple[Any, ...]]\n",
      "     |      Returns a tuple so Python knows how to pickle Row.\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Printable representation of Row used in Python REPL.\n",
      "     |  \n",
      "     |  __setattr__(self, key: Any, value: Any) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  asDict(self, recursive: bool = False) -> Dict[str, Any]\n",
      "     |      Return as a dict\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      recursive : bool, optional\n",
      "     |          turns the nested Rows to dict (default: False).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If a row contains duplicate field names, e.g., the rows of a join\n",
      "     |      between two :class:`DataFrame` that both have the fields of same names,\n",
      "     |      one of the duplicate fields will be selected by ``asDict``. ``__getitem__``\n",
      "     |      will also return one of the duplicate fields, however returned value might\n",
      "     |      be different to ``asDict``.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n",
      "     |      True\n",
      "     |      >>> row = Row(key=1, value=Row(name='a', age=2))\n",
      "     |      >>> row.asDict() == {'key': 1, 'value': Row(name='a', age=2)}\n",
      "     |      True\n",
      "     |      >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n",
      "     |      True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(cls, *args: Optional[str], **kwargs: Optional[Any]) -> 'Row'\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.tuple:\n",
      "     |  \n",
      "     |  __add__(self, value, /)\n",
      "     |      Return self+value.\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getnewargs__(self, /)\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __hash__(self, /)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __mul__(self, value, /)\n",
      "     |      Return self*value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __rmul__(self, value, /)\n",
      "     |      Return value*self.\n",
      "     |  \n",
      "     |  count(self, value, /)\n",
      "     |      Return number of occurrences of value.\n",
      "     |  \n",
      "     |  index(self, value, start=0, stop=9223372036854775807, /)\n",
      "     |      Return first index of value.\n",
      "     |      \n",
      "     |      Raises ValueError if the value is not present.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.tuple:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "    \n",
      "    class SQLContext(builtins.object)\n",
      "     |  SQLContext(sparkContext: pyspark.context.SparkContext, sparkSession: Optional[pyspark.sql.session.SparkSession] = None, jsqlContext: Optional[py4j.java_gateway.JavaObject] = None)\n",
      "     |  \n",
      "     |  The entry point for working with structured data (rows and columns) in Spark, in Spark 1.x.\n",
      "     |  \n",
      "     |  As of Spark 2.0, this is replaced by :class:`SparkSession`. However, we are keeping the class\n",
      "     |  here for backward compatibility.\n",
      "     |  \n",
      "     |  A SQLContext can be used to create :class:`DataFrame`, register :class:`DataFrame` as\n",
      "     |  tables, execute SQL over tables, cache tables, and read parquet files.\n",
      "     |  \n",
      "     |  .. deprecated:: 3.0.0\n",
      "     |      Use :func:`SparkSession.builder.getOrCreate()` instead.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  sparkContext : :class:`SparkContext`\n",
      "     |      The :class:`SparkContext` backing this SQLContext.\n",
      "     |  sparkSession : :class:`SparkSession`\n",
      "     |      The :class:`SparkSession` around which this SQLContext wraps.\n",
      "     |  jsqlContext : optional\n",
      "     |      An optional JVM Scala SQLContext. If set, we do not instantiate a new\n",
      "     |      SQLContext in the JVM, instead we make all calls to this object.\n",
      "     |      This is only for internal.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from datetime import datetime\n",
      "     |  >>> from pyspark.sql import Row\n",
      "     |  >>> sqlContext = SQLContext(sc)\n",
      "     |  >>> allTypes = sc.parallelize([Row(i=1, s=\"string\", d=1.0, l=1,\n",
      "     |  ...     b=True, list=[1, 2, 3], dict={\"s\": 0}, row=Row(a=1),\n",
      "     |  ...     time=datetime(2014, 8, 1, 14, 1, 5))])\n",
      "     |  >>> df = allTypes.toDF()\n",
      "     |  >>> df.createOrReplaceTempView(\"allTypes\")\n",
      "     |  >>> sqlContext.sql('select i+1, d+1, not b, list[1], dict[\"s\"], time, row.a '\n",
      "     |  ...            'from allTypes where b and i > 0').collect()\n",
      "     |  [Row((i + 1)=2, (d + 1)=2.0, (NOT b)=False, list[1]=2,         dict[s]=0, time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]\n",
      "     |  >>> df.rdd.map(lambda x: (x.i, x.s, x.d, x.l, x.b, x.time, x.row.a, x.list)).collect()\n",
      "     |  [(1, 'string', 1.0, 1, True, datetime.datetime(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sparkContext: pyspark.context.SparkContext, sparkSession: Optional[pyspark.sql.session.SparkSession] = None, jsqlContext: Optional[py4j.java_gateway.JavaObject] = None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cacheTable(self, tableName: str) -> None\n",
      "     |      Caches the specified table in-memory.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  clearCache(self) -> None\n",
      "     |      Removes all cached tables from the in-memory cache.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  createDataFrame(self, data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      "     |      \n",
      "     |      When ``schema`` is a list of column names, the type of each column\n",
      "     |      will be inferred from ``data``.\n",
      "     |      \n",
      "     |      When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "     |      from ``data``, which should be an RDD of :class:`Row`,\n",
      "     |      or :class:`namedtuple`, or :class:`dict`.\n",
      "     |      \n",
      "     |      When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string it must match\n",
      "     |      the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      "     |      :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "     |      :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\",\n",
      "     |      each record will also be wrapped into a tuple, which can be converted to row later.\n",
      "     |      \n",
      "     |      If schema inference is needed, ``samplingRatio`` is used to determine the ratio of\n",
      "     |      rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 2.0.0\n",
      "     |         The ``schema`` parameter can be a :class:`pyspark.sql.types.DataType` or a\n",
      "     |         datatype string after 2.0.\n",
      "     |         If it's not a :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "     |         :class:`pyspark.sql.types.StructType` and each record will also be wrapped into a tuple.\n",
      "     |      \n",
      "     |      .. versionchanged:: 2.1.0\n",
      "     |         Added verifySchema.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : :class:`RDD` or iterable\n",
      "     |          an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "     |          :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
      "     |          :class:`pandas.DataFrame`.\n",
      "     |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "     |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "     |          column names, default is None.  The data type string format equals to\n",
      "     |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "     |          omit the ``struct<>``.\n",
      "     |      samplingRatio : float, optional\n",
      "     |          the sample ratio of rows used for inferring\n",
      "     |      verifySchema : bool, optional\n",
      "     |          verify data types of every row against schema. Enabled by default.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> l = [('Alice', 1)]\n",
      "     |      >>> sqlContext.createDataFrame(l).collect()\n",
      "     |      [Row(_1='Alice', _2=1)]\n",
      "     |      >>> sqlContext.createDataFrame(l, ['name', 'age']).collect()\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      \n",
      "     |      >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "     |      >>> sqlContext.createDataFrame(d).collect()\n",
      "     |      [Row(age=1, name='Alice')]\n",
      "     |      \n",
      "     |      >>> rdd = sc.parallelize(l)\n",
      "     |      >>> sqlContext.createDataFrame(rdd).collect()\n",
      "     |      [Row(_1='Alice', _2=1)]\n",
      "     |      >>> df = sqlContext.createDataFrame(rdd, ['name', 'age'])\n",
      "     |      >>> df.collect()\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      \n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> Person = Row('name', 'age')\n",
      "     |      >>> person = rdd.map(lambda r: Person(*r))\n",
      "     |      >>> df2 = sqlContext.createDataFrame(person)\n",
      "     |      >>> df2.collect()\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.types import *\n",
      "     |      >>> schema = StructType([\n",
      "     |      ...    StructField(\"name\", StringType(), True),\n",
      "     |      ...    StructField(\"age\", IntegerType(), True)])\n",
      "     |      >>> df3 = sqlContext.createDataFrame(rdd, schema)\n",
      "     |      >>> df3.collect()\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      \n",
      "     |      >>> sqlContext.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      "     |      [Row(name='Alice', age=1)]\n",
      "     |      >>> sqlContext.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "     |      [Row(0=1, 1=2)]\n",
      "     |      \n",
      "     |      >>> sqlContext.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      "     |      [Row(a='Alice', b=1)]\n",
      "     |      >>> rdd = rdd.map(lambda row: row[1])\n",
      "     |      >>> sqlContext.createDataFrame(rdd, \"int\").collect()\n",
      "     |      [Row(value=1)]\n",
      "     |      >>> sqlContext.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "     |      Traceback (most recent call last):\n",
      "     |          ...\n",
      "     |      Py4JJavaError: ...\n",
      "     |  \n",
      "     |  createExternalTable(self, tableName: str, path: Optional[str] = None, source: Optional[str] = None, schema: Optional[pyspark.sql.types.StructType] = None, **options: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Creates an external table based on the dataset in a data source.\n",
      "     |      \n",
      "     |      It returns the DataFrame associated with the external table.\n",
      "     |      \n",
      "     |      The data source is specified by the ``source`` and a set of ``options``.\n",
      "     |      If ``source`` is not specified, the default data source configured by\n",
      "     |      ``spark.sql.sources.default`` will be used.\n",
      "     |      \n",
      "     |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
      "     |      created external table.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |  \n",
      "     |  dropTempTable(self, tableName: str) -> None\n",
      "     |      Remove the temporary table from catalog.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> sqlContext.dropTempTable(\"table1\")\n",
      "     |  \n",
      "     |  getConf(self, key: str, defaultValue: Union[str, NoneType, pyspark._globals._NoValueType] = <no value>) -> Optional[str]\n",
      "     |      Returns the value of Spark SQL configuration property for the given key.\n",
      "     |      \n",
      "     |      If the key is not set and defaultValue is set, return\n",
      "     |      defaultValue. If the key is not set and defaultValue is not set, return\n",
      "     |      the system default value.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\")\n",
      "     |      '200'\n",
      "     |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", \"10\")\n",
      "     |      '10'\n",
      "     |      >>> sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"50\")\n",
      "     |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", \"10\")\n",
      "     |      '50'\n",
      "     |  \n",
      "     |  newSession(self) -> 'SQLContext'\n",
      "     |      Returns a new SQLContext as new session, that has separate SQLConf,\n",
      "     |      registered temporary views and UDFs, but shared SparkContext and\n",
      "     |      table cache.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |  \n",
      "     |  range(self, start: int, end: Optional[int] = None, step: int = 1, numPartitions: Optional[int] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n",
      "     |      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n",
      "     |      step value ``step``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      start : int\n",
      "     |          the start value\n",
      "     |      end : int, optional\n",
      "     |          the end value (exclusive)\n",
      "     |      step : int, optional\n",
      "     |          the incremental step (default: 1)\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions of the DataFrame\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.range(1, 7, 2).collect()\n",
      "     |      [Row(id=1), Row(id=3), Row(id=5)]\n",
      "     |      \n",
      "     |      If only one argument is specified, it will be used as the end value.\n",
      "     |      \n",
      "     |      >>> sqlContext.range(3).collect()\n",
      "     |      [Row(id=0), Row(id=1), Row(id=2)]\n",
      "     |  \n",
      "     |  registerDataFrameAsTable(self, df: pyspark.sql.dataframe.DataFrame, tableName: str) -> None\n",
      "     |      Registers the given :class:`DataFrame` as a temporary table in the catalog.\n",
      "     |      \n",
      "     |      Temporary tables exist only during the lifetime of this instance of :class:`SQLContext`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |  \n",
      "     |  registerFunction(self, name: str, f: Callable[..., Any], returnType: Optional[pyspark.sql.types.DataType] = None) -> 'UserDefinedFunctionLike'\n",
      "     |      An alias for :func:`spark.udf.register`.\n",
      "     |      See :meth:`pyspark.sql.UDFRegistration.register`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2.0\n",
      "     |      \n",
      "     |      .. deprecated:: 2.3.0\n",
      "     |          Use :func:`spark.udf.register` instead.\n",
      "     |  \n",
      "     |  registerJavaFunction(self, name: str, javaClassName: str, returnType: Optional[pyspark.sql.types.DataType] = None) -> None\n",
      "     |      An alias for :func:`spark.udf.registerJavaFunction`.\n",
      "     |      See :meth:`pyspark.sql.UDFRegistration.registerJavaFunction`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      .. deprecated:: 2.3.0\n",
      "     |          Use :func:`spark.udf.registerJavaFunction` instead.\n",
      "     |  \n",
      "     |  setConf(self, key: str, value: Union[bool, int, str]) -> None\n",
      "     |      Sets the given Spark SQL configuration property.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |  \n",
      "     |  sql(self, sqlQuery: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns a :class:`DataFrame` representing the result of the given query.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> df2 = sqlContext.sql(\"SELECT field1 AS f1, field2 as f2 from table1\")\n",
      "     |      >>> df2.collect()\n",
      "     |      [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\n",
      "     |  \n",
      "     |  table(self, tableName: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns the specified table or view as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> df2 = sqlContext.table(\"table1\")\n",
      "     |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |  \n",
      "     |  tableNames(self, dbName: Optional[str] = None) -> List[str]\n",
      "     |      Returns a list of names of tables in the database ``dbName``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dbName: str\n",
      "     |          name of the database to use. Default to the current database.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          list of table names, in string\n",
      "     |      \n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> \"table1\" in sqlContext.tableNames()\n",
      "     |      True\n",
      "     |      >>> \"table1\" in sqlContext.tableNames(\"default\")\n",
      "     |      True\n",
      "     |  \n",
      "     |  tables(self, dbName: Optional[str] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns a :class:`DataFrame` containing names of tables in the given database.\n",
      "     |      \n",
      "     |      If ``dbName`` is not specified, the current database will be used.\n",
      "     |      \n",
      "     |      The returned DataFrame has two columns: ``tableName`` and ``isTemporary``\n",
      "     |      (a column with :class:`BooleanType` indicating if a table is a temporary one or not).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dbName: str, optional\n",
      "     |          name of the database to use.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      "     |      >>> df2 = sqlContext.tables()\n",
      "     |      >>> df2.filter(\"tableName = 'table1'\").first()\n",
      "     |      Row(namespace='', tableName='table1', isTemporary=True)\n",
      "     |  \n",
      "     |  uncacheTable(self, tableName: str) -> None\n",
      "     |      Removes the specified table from the in-memory cache.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  getOrCreate(sc: pyspark.context.SparkContext) -> 'SQLContext' from builtins.type\n",
      "     |      Get the existing SQLContext or create a new one with given SparkContext.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      .. deprecated:: 3.0.0\n",
      "     |          Use :func:`SparkSession.builder.getOrCreate()` instead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sc : :class:`SparkContext`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  read\n",
      "     |      Returns a :class:`DataFrameReader` that can be used to read data\n",
      "     |      in as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrameReader`\n",
      "     |  \n",
      "     |  readStream\n",
      "     |      Returns a :class:`DataStreamReader` that can be used to read data streams\n",
      "     |      as a streaming :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataStreamReader`\n",
      "     |      \n",
      "     |      >>> text_sdf = sqlContext.readStream.text(tempfile.mkdtemp())\n",
      "     |      >>> text_sdf.isStreaming\n",
      "     |      True\n",
      "     |  \n",
      "     |  streams\n",
      "     |      Returns a :class:`StreamingQueryManager` that allows managing all the\n",
      "     |      :class:`StreamingQuery` StreamingQueries active on `this` context.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |  \n",
      "     |  udf\n",
      "     |      Returns a :class:`UDFRegistration` for UDF registration.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`UDFRegistration`\n",
      "     |  \n",
      "     |  udtf\n",
      "     |      Returns a :class:`UDTFRegistration` for UDTF registration.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`UDTFRegistration`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'_instantiatedContext': typing.ClassVar[typing.Opti...\n",
      "    \n",
      "    class SparkSession(pyspark.sql.pandas.conversion.SparkConversionMixin)\n",
      "     |  SparkSession(sparkContext: pyspark.context.SparkContext, jsparkSession: Optional[py4j.java_gateway.JavaObject] = None, options: Dict[str, Any] = {})\n",
      "     |  \n",
      "     |  The entry point to programming Spark with the Dataset and DataFrame API.\n",
      "     |  \n",
      "     |  A SparkSession can be used to create :class:`DataFrame`, register :class:`DataFrame` as\n",
      "     |  tables, execute SQL over tables, cache tables, and read parquet files.\n",
      "     |  To create a :class:`SparkSession`, use the following builder pattern:\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  .. autoattribute:: builder\n",
      "     |     :annotation:\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Create a Spark session.\n",
      "     |  \n",
      "     |  >>> spark = (\n",
      "     |  ...     SparkSession.builder\n",
      "     |  ...         .master(\"local\")\n",
      "     |  ...         .appName(\"Word Count\")\n",
      "     |  ...         .config(\"spark.some.config.option\", \"some-value\")\n",
      "     |  ...         .getOrCreate()\n",
      "     |  ... )\n",
      "     |  \n",
      "     |  Create a Spark session with Spark Connect.\n",
      "     |  \n",
      "     |  >>> spark = (\n",
      "     |  ...     SparkSession.builder\n",
      "     |  ...         .remote(\"sc://localhost\")\n",
      "     |  ...         .appName(\"Word Count\")\n",
      "     |  ...         .config(\"spark.some.config.option\", \"some-value\")\n",
      "     |  ...         .getOrCreate()\n",
      "     |  ... )  # doctest: +SKIP\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SparkSession\n",
      "     |      pyspark.sql.pandas.conversion.SparkConversionMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __enter__(self) -> 'SparkSession'\n",
      "     |      Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> with SparkSession.builder.master(\"local\").getOrCreate() as session:\n",
      "     |      ...     session.range(5).show()  # doctest: +SKIP\n",
      "     |      +---+\n",
      "     |      | id|\n",
      "     |      +---+\n",
      "     |      |  0|\n",
      "     |      |  1|\n",
      "     |      |  2|\n",
      "     |      |  3|\n",
      "     |      |  4|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  __exit__(self, exc_type: Optional[Type[BaseException]], exc_val: Optional[BaseException], exc_tb: Optional[traceback]) -> None\n",
      "     |      Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n",
      "     |      \n",
      "     |      Specifically stop the SparkSession on exit of the with block.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> with SparkSession.builder.master(\"local\").getOrCreate() as session:\n",
      "     |      ...     session.range(5).show()  # doctest: +SKIP\n",
      "     |      +---+\n",
      "     |      | id|\n",
      "     |      +---+\n",
      "     |      |  0|\n",
      "     |      |  1|\n",
      "     |      |  2|\n",
      "     |      |  3|\n",
      "     |      |  4|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  __init__(self, sparkContext: pyspark.context.SparkContext, jsparkSession: Optional[py4j.java_gateway.JavaObject] = None, options: Dict[str, Any] = {})\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  addArtifact = addArtifacts(self, *path: str, pyfile: bool = False, archive: bool = False, file: bool = False) -> None\n",
      "     |  \n",
      "     |  addArtifacts(self, *path: str, pyfile: bool = False, archive: bool = False, file: bool = False) -> None\n",
      "     |      Add artifact(s) to the client session. Currently only local files are supported.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      *path : tuple of str\n",
      "     |          Artifact's URIs to add.\n",
      "     |      pyfile : bool\n",
      "     |          Whether to add them as Python dependencies such as .py, .egg, .zip or .jar files.\n",
      "     |          The pyfiles are directly inserted into the path when executing Python functions\n",
      "     |          in executors.\n",
      "     |      archive : bool\n",
      "     |          Whether to add them as archives such as .zip, .jar, .tar.gz, .tgz, or .tar files.\n",
      "     |          The archives are unpacked on the executor side automatically.\n",
      "     |      file : bool\n",
      "     |          Add a file to be downloaded with this Spark job on every node.\n",
      "     |          The ``path`` passed can only be a local file for now.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This is an API dedicated to Spark Connect client only. With regular Spark Session, it throws\n",
      "     |      an exception.\n",
      "     |  \n",
      "     |  addTag(self, tag: str) -> None\n",
      "     |      Add a tag to be assigned to all the operations started by this thread in this session.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tag : list of str\n",
      "     |          The tag to be added. Cannot contain ',' (comma) character or be an empty string.\n",
      "     |  \n",
      "     |  clearTags(self) -> None\n",
      "     |      Clear the current thread's operation tags.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |  \n",
      "     |  copyFromLocalToFs(self, local_path: str, dest_path: str) -> None\n",
      "     |      Copy file from local to cloud storage file system.\n",
      "     |      If the file already exits in destination path, old file is overwritten.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      local_path: str\n",
      "     |          Path to a local file. Directories are not supported.\n",
      "     |          The path can be either an absolute path or a relative path.\n",
      "     |      dest_path: str\n",
      "     |          The cloud storage path to the destination the file will\n",
      "     |          be copied to.\n",
      "     |          The path must be an an absolute path.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is a developer API.\n",
      "     |      Also, this is an API dedicated to Spark Connect client only. With regular\n",
      "     |      Spark Session, it throws an exception.\n",
      "     |  \n",
      "     |  createDataFrame(self, data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n",
      "     |      or a :class:`numpy.ndarray`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : :class:`RDD` or iterable\n",
      "     |          an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "     |          :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n",
      "     |          :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n",
      "     |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "     |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "     |          column names, default is None. The data type string format equals to\n",
      "     |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "     |          omit the ``struct<>``.\n",
      "     |      \n",
      "     |          When ``schema`` is a list of column names, the type of each column\n",
      "     |          will be inferred from ``data``.\n",
      "     |      \n",
      "     |          When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "     |          from ``data``, which should be an RDD of either :class:`Row`,\n",
      "     |          :class:`namedtuple`, or :class:`dict`.\n",
      "     |      \n",
      "     |          When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n",
      "     |          match the real data, or an exception will be thrown at runtime. If the given schema is\n",
      "     |          not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "     |          :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n",
      "     |          \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n",
      "     |          later.\n",
      "     |      samplingRatio : float, optional\n",
      "     |          the sample ratio of rows used for inferring. The first few rows will be used\n",
      "     |          if ``samplingRatio`` is ``None``.\n",
      "     |      verifySchema : bool, optional\n",
      "     |          verify data types of every row against schema. Enabled by default.\n",
      "     |      \n",
      "     |          .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Create a DataFrame from a list of tuples.\n",
      "     |      \n",
      "     |      >>> spark.createDataFrame([('Alice', 1)]).show()\n",
      "     |      +-----+---+\n",
      "     |      |   _1| _2|\n",
      "     |      +-----+---+\n",
      "     |      |Alice|  1|\n",
      "     |      +-----+---+\n",
      "     |      \n",
      "     |      Create a DataFrame from a list of dictionaries.\n",
      "     |      \n",
      "     |      >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "     |      >>> spark.createDataFrame(d).show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  1|Alice|\n",
      "     |      +---+-----+\n",
      "     |      \n",
      "     |      Create a DataFrame with column names specified.\n",
      "     |      \n",
      "     |      >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).show()\n",
      "     |      +-----+---+\n",
      "     |      | name|age|\n",
      "     |      +-----+---+\n",
      "     |      |Alice|  1|\n",
      "     |      +-----+---+\n",
      "     |      \n",
      "     |      Create a DataFrame with the explicit schema specified.\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.types import *\n",
      "     |      >>> schema = StructType([\n",
      "     |      ...    StructField(\"name\", StringType(), True),\n",
      "     |      ...    StructField(\"age\", IntegerType(), True)])\n",
      "     |      >>> spark.createDataFrame([('Alice', 1)], schema).show()\n",
      "     |      +-----+---+\n",
      "     |      | name|age|\n",
      "     |      +-----+---+\n",
      "     |      |Alice|  1|\n",
      "     |      +-----+---+\n",
      "     |      \n",
      "     |      Create a DataFrame with the schema in DDL formatted string.\n",
      "     |      \n",
      "     |      >>> spark.createDataFrame([('Alice', 1)], \"name: string, age: int\").show()\n",
      "     |      +-----+---+\n",
      "     |      | name|age|\n",
      "     |      +-----+---+\n",
      "     |      |Alice|  1|\n",
      "     |      +-----+---+\n",
      "     |      \n",
      "     |      Create an empty DataFrame.\n",
      "     |      When initializing an empty DataFrame in PySpark, it's mandatory to specify its schema,\n",
      "     |      as the DataFrame lacks data from which the schema can be inferred.\n",
      "     |      \n",
      "     |      >>> spark.createDataFrame([], \"name: string, age: int\").show()\n",
      "     |      +----+---+\n",
      "     |      |name|age|\n",
      "     |      +----+---+\n",
      "     |      +----+---+\n",
      "     |      \n",
      "     |      Create a DataFrame from Row objects.\n",
      "     |      \n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> Person = Row('name', 'age')\n",
      "     |      >>> df = spark.createDataFrame([Person(\"Alice\", 1)])\n",
      "     |      >>> df.show()\n",
      "     |      +-----+---+\n",
      "     |      | name|age|\n",
      "     |      +-----+---+\n",
      "     |      |Alice|  1|\n",
      "     |      +-----+---+\n",
      "     |      \n",
      "     |      Create a DataFrame from a pandas DataFrame.\n",
      "     |      \n",
      "     |      >>> spark.createDataFrame(df.toPandas()).show()  # doctest: +SKIP\n",
      "     |      +-----+---+\n",
      "     |      | name|age|\n",
      "     |      +-----+---+\n",
      "     |      |Alice|  1|\n",
      "     |      +-----+---+\n",
      "     |      >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "     |      +---+---+\n",
      "     |      |  0|  1|\n",
      "     |      +---+---+\n",
      "     |      |  1|  2|\n",
      "     |      +---+---+\n",
      "     |  \n",
      "     |  getTags(self) -> Set[str]\n",
      "     |      Get the tags that are currently set to be assigned to all the operations started by this\n",
      "     |      thread.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      set of str\n",
      "     |          Set of tags of interrupted operations.\n",
      "     |  \n",
      "     |  interruptAll(self) -> List[str]\n",
      "     |      Interrupt all operations of this session currently running on the connected server.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list of str\n",
      "     |          List of operationIds of interrupted operations.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      There is still a possibility of operation finishing just as it is interrupted.\n",
      "     |  \n",
      "     |  interruptOperation(self, op_id: str) -> List[str]\n",
      "     |      Interrupt an operation of this session with the given operationId.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list of str\n",
      "     |          List of operationIds of interrupted operations.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      There is still a possibility of operation finishing just as it is interrupted.\n",
      "     |  \n",
      "     |  interruptTag(self, tag: str) -> List[str]\n",
      "     |      Interrupt all operations of this session with the given operation tag.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list of str\n",
      "     |          List of operationIds of interrupted operations.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      There is still a possibility of operation finishing just as it is interrupted.\n",
      "     |  \n",
      "     |  newSession(self) -> 'SparkSession'\n",
      "     |      Returns a new :class:`SparkSession` as new session, that has separate SQLConf,\n",
      "     |      registered temporary views and UDFs, but shared :class:`SparkContext` and\n",
      "     |      table cache.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`SparkSession`\n",
      "     |          Spark session if an active session exists for the current thread\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.newSession()\n",
      "     |      <...SparkSession object ...>\n",
      "     |  \n",
      "     |  range(self, start: int, end: Optional[int] = None, step: int = 1, numPartitions: Optional[int] = None) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n",
      "     |      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n",
      "     |      step value ``step``.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      start : int\n",
      "     |          the start value\n",
      "     |      end : int, optional\n",
      "     |          the end value (exclusive)\n",
      "     |      step : int, optional\n",
      "     |          the incremental step (default: 1)\n",
      "     |      numPartitions : int, optional\n",
      "     |          the number of partitions of the DataFrame\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.range(1, 7, 2).show()\n",
      "     |      +---+\n",
      "     |      | id|\n",
      "     |      +---+\n",
      "     |      |  1|\n",
      "     |      |  3|\n",
      "     |      |  5|\n",
      "     |      +---+\n",
      "     |      \n",
      "     |      If only one argument is specified, it will be used as the end value.\n",
      "     |      \n",
      "     |      >>> spark.range(3).show()\n",
      "     |      +---+\n",
      "     |      | id|\n",
      "     |      +---+\n",
      "     |      |  0|\n",
      "     |      |  1|\n",
      "     |      |  2|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  removeTag(self, tag: str) -> None\n",
      "     |      Remove a tag previously added to be assigned to all the operations started by this thread in\n",
      "     |      this session. Noop if such a tag was not added earlier.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tag : list of str\n",
      "     |          The tag to be removed. Cannot contain ',' (comma) character or be an empty string.\n",
      "     |  \n",
      "     |  sql(self, sqlQuery: str, args: Union[Dict[str, Any], List, NoneType] = None, **kwargs: Any) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns a :class:`DataFrame` representing the result of the given query.\n",
      "     |      When ``kwargs`` is specified, this method formats the given string by using the Python\n",
      "     |      standard formatter. The method binds named parameters to SQL literals or\n",
      "     |      positional parameters from `args`. It doesn't support named and positional parameters\n",
      "     |      in the same SQL query.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect and parameterized SQL.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Added positional parameters.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sqlQuery : str\n",
      "     |          SQL query string.\n",
      "     |      args : dict or list\n",
      "     |          A dictionary of parameter names to Python objects or a list of Python objects\n",
      "     |          that can be converted to SQL literal expressions. See\n",
      "     |          <a href=\"https://spark.apache.org/docs/latest/sql-ref-datatypes.html\">\n",
      "     |          Supported Data Types</a> for supported value types in Python.\n",
      "     |          For example, dictionary keys: \"rank\", \"name\", \"birthdate\";\n",
      "     |          dictionary or list values: 1, \"Steven\", datetime.date(2023, 4, 2).\n",
      "     |          A value can be also a `Column` of literal expression, in that case it is taken as is.\n",
      "     |      \n",
      "     |          .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      kwargs : dict\n",
      "     |          Other variables that the user wants to set that can be referenced in the query\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.3.0\n",
      "     |             Added optional argument ``kwargs`` to specify the mapping of variables in the query.\n",
      "     |             This feature is experimental and unstable.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Executing a SQL query.\n",
      "     |      \n",
      "     |      >>> spark.sql(\"SELECT * FROM range(10) where id > 7\").show()\n",
      "     |      +---+\n",
      "     |      | id|\n",
      "     |      +---+\n",
      "     |      |  8|\n",
      "     |      |  9|\n",
      "     |      +---+\n",
      "     |      \n",
      "     |      Executing a SQL query with variables as Python formatter standard.\n",
      "     |      \n",
      "     |      >>> spark.sql(\n",
      "     |      ...     \"SELECT * FROM range(10) WHERE id > {bound1} AND id < {bound2}\", bound1=7, bound2=9\n",
      "     |      ... ).show()\n",
      "     |      +---+\n",
      "     |      | id|\n",
      "     |      +---+\n",
      "     |      |  8|\n",
      "     |      +---+\n",
      "     |      \n",
      "     |      >>> mydf = spark.range(10)\n",
      "     |      >>> spark.sql(\n",
      "     |      ...     \"SELECT {col} FROM {mydf} WHERE id IN {x}\",\n",
      "     |      ...     col=mydf.id, mydf=mydf, x=tuple(range(4))).show()\n",
      "     |      +---+\n",
      "     |      | id|\n",
      "     |      +---+\n",
      "     |      |  0|\n",
      "     |      |  1|\n",
      "     |      |  2|\n",
      "     |      |  3|\n",
      "     |      +---+\n",
      "     |      \n",
      "     |      >>> spark.sql('''\n",
      "     |      ...   SELECT m1.a, m2.b\n",
      "     |      ...   FROM {table1} m1 INNER JOIN {table2} m2\n",
      "     |      ...   ON m1.key = m2.key\n",
      "     |      ...   ORDER BY m1.a, m2.b''',\n",
      "     |      ...   table1=spark.createDataFrame([(1, \"a\"), (2, \"b\")], [\"a\", \"key\"]),\n",
      "     |      ...   table2=spark.createDataFrame([(3, \"a\"), (4, \"b\"), (5, \"b\")], [\"b\", \"key\"])).show()\n",
      "     |      +---+---+\n",
      "     |      |  a|  b|\n",
      "     |      +---+---+\n",
      "     |      |  1|  3|\n",
      "     |      |  2|  4|\n",
      "     |      |  2|  5|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      Also, it is possible to query using class:`Column` from :class:`DataFrame`.\n",
      "     |      \n",
      "     |      >>> mydf = spark.createDataFrame([(1, 4), (2, 4), (3, 6)], [\"A\", \"B\"])\n",
      "     |      >>> spark.sql(\"SELECT {df.A}, {df[B]} FROM {df}\", df=mydf).show()\n",
      "     |      +---+---+\n",
      "     |      |  A|  B|\n",
      "     |      +---+---+\n",
      "     |      |  1|  4|\n",
      "     |      |  2|  4|\n",
      "     |      |  3|  6|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      And substitude named parameters with the `:` prefix by SQL literals.\n",
      "     |      \n",
      "     |      >>> spark.sql(\"SELECT * FROM {df} WHERE {df[B]} > :minB\", {\"minB\" : 5}, df=mydf).show()\n",
      "     |      +---+---+\n",
      "     |      |  A|  B|\n",
      "     |      +---+---+\n",
      "     |      |  3|  6|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      Or positional parameters marked by `?` in the SQL query by SQL literals.\n",
      "     |      \n",
      "     |      >>> spark.sql(\n",
      "     |      ...   \"SELECT * FROM {df} WHERE {df[B]} > ? and ? < {df[A]}\",\n",
      "     |      ...   args=[5, 2], df=mydf).show()\n",
      "     |      +---+---+\n",
      "     |      |  A|  B|\n",
      "     |      +---+---+\n",
      "     |      |  3|  6|\n",
      "     |      +---+---+\n",
      "     |  \n",
      "     |  stop(self) -> None\n",
      "     |      Stop the underlying :class:`SparkContext`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.stop()  # doctest: +SKIP\n",
      "     |  \n",
      "     |  table(self, tableName: str) -> pyspark.sql.dataframe.DataFrame\n",
      "     |      Returns the specified table as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tableName : str\n",
      "     |          the table name to retrieve.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.range(5).createOrReplaceTempView(\"table1\")\n",
      "     |      >>> spark.table(\"table1\").sort(\"id\").show()\n",
      "     |      +---+\n",
      "     |      | id|\n",
      "     |      +---+\n",
      "     |      |  0|\n",
      "     |      |  1|\n",
      "     |      |  2|\n",
      "     |      |  3|\n",
      "     |      |  4|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  active() -> 'SparkSession' from builtins.type\n",
      "     |      Returns the active or default :class:`SparkSession` for the current thread, returned by\n",
      "     |      the builder.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`SparkSession`\n",
      "     |          Spark session if an active or default session exists for the current thread.\n",
      "     |  \n",
      "     |  getActiveSession() -> Optional[ForwardRef('SparkSession')] from builtins.type\n",
      "     |      Returns the active :class:`SparkSession` for the current thread, returned by the builder\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`SparkSession`\n",
      "     |          Spark session if an active session exists for the current thread\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> s = SparkSession.getActiveSession()\n",
      "     |      >>> df = s.createDataFrame([('Alice', 1)], ['name', 'age'])\n",
      "     |      >>> df.select(\"age\").show()\n",
      "     |      +---+\n",
      "     |      |age|\n",
      "     |      +---+\n",
      "     |      |  1|\n",
      "     |      +---+\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  builder\n",
      "     |      Creates a :class:`Builder` for constructing a :class:`SparkSession`.\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |  \n",
      "     |  catalog\n",
      "     |      Interface through which the user may create, drop, alter or query underlying\n",
      "     |      databases, tables, functions, etc.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`Catalog`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.catalog\n",
      "     |      <...Catalog object ...>\n",
      "     |      \n",
      "     |      Create a temp view, show the list, and drop it.\n",
      "     |      \n",
      "     |      >>> spark.range(1).createTempView(\"test_view\")\n",
      "     |      >>> spark.catalog.listTables()\n",
      "     |      [Table(name='test_view', catalog=None, namespace=[], description=None, ...\n",
      "     |      >>> _ = spark.catalog.dropTempView(\"test_view\")\n",
      "     |  \n",
      "     |  client\n",
      "     |      Gives access to the Spark Connect client. In normal cases this is not necessary to be used\n",
      "     |      and only relevant for testing.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.4.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`SparkConnectClient`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is unstable, and a developer API. It returns non-API instance\n",
      "     |      :class:`SparkConnectClient`.\n",
      "     |      This is an API dedicated to Spark Connect client only. With regular Spark Session, it throws\n",
      "     |      an exception.\n",
      "     |  \n",
      "     |  conf\n",
      "     |      Runtime configuration interface for Spark.\n",
      "     |      \n",
      "     |      This is the interface through which the user can get and set all Spark and Hadoop\n",
      "     |      configurations that are relevant to Spark SQL. When getting the value of a config,\n",
      "     |      this defaults to the value set in the underlying :class:`SparkContext`, if any.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`pyspark.sql.conf.RuntimeConfig`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.conf\n",
      "     |      <pyspark...RuntimeConf...>\n",
      "     |      \n",
      "     |      Set a runtime configuration for the session\n",
      "     |      \n",
      "     |      >>> spark.conf.set(\"key\", \"value\")\n",
      "     |      >>> spark.conf.get(\"key\")\n",
      "     |      'value'\n",
      "     |  \n",
      "     |  read\n",
      "     |      Returns a :class:`DataFrameReader` that can be used to read data\n",
      "     |      in as a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrameReader`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.read\n",
      "     |      <...DataFrameReader object ...>\n",
      "     |      \n",
      "     |      Write a DataFrame into a JSON file and read it back.\n",
      "     |      \n",
      "     |      >>> import tempfile\n",
      "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
      "     |      ...     # Write a DataFrame into a JSON file\n",
      "     |      ...     spark.createDataFrame(\n",
      "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "     |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
      "     |      ...\n",
      "     |      ...     # Read the JSON file as a DataFrame.\n",
      "     |      ...     spark.read.format('json').load(d).show()\n",
      "     |      +---+------------+\n",
      "     |      |age|        name|\n",
      "     |      +---+------------+\n",
      "     |      |100|Hyukjin Kwon|\n",
      "     |      +---+------------+\n",
      "     |  \n",
      "     |  readStream\n",
      "     |      Returns a :class:`DataStreamReader` that can be used to read data streams\n",
      "     |      as a streaming :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataStreamReader`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.readStream\n",
      "     |      <pyspark...DataStreamReader object ...>\n",
      "     |      \n",
      "     |      The example below uses Rate source that generates rows continuously.\n",
      "     |      After that, we operate a modulo by 3, and then write the stream out to the console.\n",
      "     |      The streaming query stops in 3 seconds.\n",
      "     |      \n",
      "     |      >>> import time\n",
      "     |      >>> df = spark.readStream.format(\"rate\").load()\n",
      "     |      >>> df = df.selectExpr(\"value % 3 as v\")\n",
      "     |      >>> q = df.writeStream.format(\"console\").start()\n",
      "     |      >>> time.sleep(3)\n",
      "     |      >>> q.stop()\n",
      "     |  \n",
      "     |  sparkContext\n",
      "     |      Returns the underlying :class:`SparkContext`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`SparkContext`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.sparkContext\n",
      "     |      <SparkContext master=... appName=...>\n",
      "     |      \n",
      "     |      Create an RDD from the Spark context\n",
      "     |      \n",
      "     |      >>> rdd = spark.sparkContext.parallelize([1, 2, 3])\n",
      "     |      >>> rdd.collect()\n",
      "     |      [1, 2, 3]\n",
      "     |  \n",
      "     |  streams\n",
      "     |      Returns a :class:`StreamingQueryManager` that allows managing all the\n",
      "     |      :class:`StreamingQuery` instances active on `this` context.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.5.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`StreamingQueryManager`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.streams\n",
      "     |      <pyspark...StreamingQueryManager object ...>\n",
      "     |      \n",
      "     |      Get the list of active streaming queries\n",
      "     |      \n",
      "     |      >>> sq = spark.readStream.format(\n",
      "     |      ...     \"rate\").load().writeStream.format('memory').queryName('this_query').start()\n",
      "     |      >>> sqm = spark.streams\n",
      "     |      >>> [q.name for q in sqm.active]\n",
      "     |      ['this_query']\n",
      "     |      >>> sq.stop()\n",
      "     |  \n",
      "     |  udf\n",
      "     |      Returns a :class:`UDFRegistration` for UDF registration.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`UDFRegistration`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Register a Python UDF, and use it in SQL.\n",
      "     |      \n",
      "     |      >>> strlen = spark.udf.register(\"strlen\", lambda x: len(x))\n",
      "     |      >>> spark.sql(\"SELECT strlen('test')\").show()\n",
      "     |      +------------+\n",
      "     |      |strlen(test)|\n",
      "     |      +------------+\n",
      "     |      |           4|\n",
      "     |      +------------+\n",
      "     |  \n",
      "     |  udtf\n",
      "     |      Returns a :class:`UDTFRegistration` for UDTF registration.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`UDTFRegistration`\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  version\n",
      "     |      The version of Spark on which this application is running.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      str\n",
      "     |          the version of Spark in string.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> _ = spark.version\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Builder = <class 'pyspark.sql.session.SparkSession.Builder'>\n",
      "     |      Builder for :class:`SparkSession`.\n",
      "     |  \n",
      "     |  \n",
      "     |  __annotations__ = {'_activeSession': typing.ClassVar[typing.Optional[F...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pyspark.sql.pandas.conversion.SparkConversionMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class UDFRegistration(builtins.object)\n",
      "     |  UDFRegistration(sparkSession: 'SparkSession')\n",
      "     |  \n",
      "     |  Wrapper for user-defined function registration. This instance can be accessed by\n",
      "     |  :attr:`spark.udf` or :attr:`sqlContext.udf`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.3.1\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sparkSession: 'SparkSession')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  register(self, name: str, f: Union[Callable[..., Any], ForwardRef('UserDefinedFunctionLike')], returnType: Optional[ForwardRef('DataTypeOrString')] = None) -> 'UserDefinedFunctionLike'\n",
      "     |      Register a Python function (including lambda function) or a user-defined function\n",
      "     |      as a SQL function.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str,\n",
      "     |          name of the user-defined function in SQL statements.\n",
      "     |      f : function, :meth:`pyspark.sql.functions.udf` or :meth:`pyspark.sql.functions.pandas_udf`\n",
      "     |          a Python function, or a user-defined function. The user-defined function can\n",
      "     |          be either row-at-a-time or vectorized. See :meth:`pyspark.sql.functions.udf` and\n",
      "     |          :meth:`pyspark.sql.functions.pandas_udf`.\n",
      "     |      returnType : :class:`pyspark.sql.types.DataType` or str, optional\n",
      "     |          the return type of the registered user-defined function. The value can\n",
      "     |          be either a :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "     |          `returnType` can be optionally specified when `f` is a Python function but not\n",
      "     |          when `f` is a user-defined function. Please see the examples below.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      function\n",
      "     |          a user-defined function\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      To register a nondeterministic Python function, users need to first build\n",
      "     |      a nondeterministic user-defined function for the Python function and then register it\n",
      "     |      as a SQL function.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      1. When `f` is a Python function:\n",
      "     |      \n",
      "     |          `returnType` defaults to string type and can be optionally specified. The produced\n",
      "     |          object must match the specified type. In this case, this API works as if\n",
      "     |          `register(name, f, returnType=StringType())`.\n",
      "     |      \n",
      "     |          >>> strlen = spark.udf.register(\"stringLengthString\", lambda x: len(x))\n",
      "     |          >>> spark.sql(\"SELECT stringLengthString('test')\").collect()\n",
      "     |          [Row(stringLengthString(test)='4')]\n",
      "     |      \n",
      "     |          >>> spark.sql(\"SELECT 'foo' AS text\").select(strlen(\"text\")).collect()\n",
      "     |          [Row(stringLengthString(text)='3')]\n",
      "     |      \n",
      "     |          >>> from pyspark.sql.types import IntegerType\n",
      "     |          >>> _ = spark.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n",
      "     |          >>> spark.sql(\"SELECT stringLengthInt('test')\").collect()\n",
      "     |          [Row(stringLengthInt(test)=4)]\n",
      "     |      \n",
      "     |          >>> from pyspark.sql.types import IntegerType\n",
      "     |          >>> _ = spark.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n",
      "     |          >>> spark.sql(\"SELECT stringLengthInt('test')\").collect()\n",
      "     |          [Row(stringLengthInt(test)=4)]\n",
      "     |      \n",
      "     |      2. When `f` is a user-defined function (from Spark 2.3.0):\n",
      "     |      \n",
      "     |          Spark uses the return type of the given user-defined function as the return type of\n",
      "     |          the registered user-defined function. `returnType` should not be specified.\n",
      "     |          In this case, this API works as if `register(name, f)`.\n",
      "     |      \n",
      "     |          >>> from pyspark.sql.types import IntegerType\n",
      "     |          >>> from pyspark.sql.functions import udf\n",
      "     |          >>> slen = udf(lambda s: len(s), IntegerType())\n",
      "     |          >>> _ = spark.udf.register(\"slen\", slen)\n",
      "     |          >>> spark.sql(\"SELECT slen('test')\").collect()\n",
      "     |          [Row(slen(test)=4)]\n",
      "     |      \n",
      "     |          >>> import random\n",
      "     |          >>> from pyspark.sql.functions import udf\n",
      "     |          >>> from pyspark.sql.types import IntegerType\n",
      "     |          >>> random_udf = udf(lambda: random.randint(0, 100), IntegerType()).asNondeterministic()\n",
      "     |          >>> new_random_udf = spark.udf.register(\"random_udf\", random_udf)\n",
      "     |          >>> spark.sql(\"SELECT random_udf()\").collect()  # doctest: +SKIP\n",
      "     |          [Row(random_udf()=82)]\n",
      "     |      \n",
      "     |          >>> import pandas as pd  # doctest: +SKIP\n",
      "     |          >>> from pyspark.sql.functions import pandas_udf\n",
      "     |          >>> @pandas_udf(\"integer\")  # doctest: +SKIP\n",
      "     |          ... def add_one(s: pd.Series) -> pd.Series:\n",
      "     |          ...     return s + 1\n",
      "     |          ...\n",
      "     |          >>> _ = spark.udf.register(\"add_one\", add_one)  # doctest: +SKIP\n",
      "     |          >>> spark.sql(\"SELECT add_one(id) FROM range(3)\").collect()  # doctest: +SKIP\n",
      "     |          [Row(add_one(id)=1), Row(add_one(id)=2), Row(add_one(id)=3)]\n",
      "     |      \n",
      "     |          >>> @pandas_udf(\"integer\")  # doctest: +SKIP\n",
      "     |          ... def sum_udf(v: pd.Series) -> int:\n",
      "     |          ...     return v.sum()\n",
      "     |          ...\n",
      "     |          >>> _ = spark.udf.register(\"sum_udf\", sum_udf)  # doctest: +SKIP\n",
      "     |          >>> q = \"SELECT sum_udf(v1) FROM VALUES (3, 0), (2, 0), (1, 1) tbl(v1, v2) GROUP BY v2\"\n",
      "     |          >>> spark.sql(q).collect()  # doctest: +SKIP\n",
      "     |          [Row(sum_udf(v1)=1), Row(sum_udf(v1)=5)]\n",
      "     |  \n",
      "     |  registerJavaFunction(self, name: str, javaClassName: str, returnType: Optional[ForwardRef('DataTypeOrString')] = None) -> None\n",
      "     |      Register a Java user-defined function as a SQL function.\n",
      "     |      \n",
      "     |      In addition to a name and the function itself, the return type can be optionally specified.\n",
      "     |      When the return type is not specified we would infer it via reflection.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          name of the user-defined function\n",
      "     |      javaClassName : str\n",
      "     |          fully qualified name of java class\n",
      "     |      returnType : :class:`pyspark.sql.types.DataType` or str, optional\n",
      "     |          the return type of the registered Java function. The value can be either\n",
      "     |          a :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.types import IntegerType\n",
      "     |      >>> spark.udf.registerJavaFunction(\n",
      "     |      ...     \"javaStringLength\", \"test.org.apache.spark.sql.JavaStringLength\", IntegerType())\n",
      "     |      ... # doctest: +SKIP\n",
      "     |      >>> spark.sql(\"SELECT javaStringLength('test')\").collect()  # doctest: +SKIP\n",
      "     |      [Row(javaStringLength(test)=4)]\n",
      "     |      \n",
      "     |      >>> spark.udf.registerJavaFunction(\n",
      "     |      ...     \"javaStringLength2\", \"test.org.apache.spark.sql.JavaStringLength\")\n",
      "     |      ... # doctest: +SKIP\n",
      "     |      >>> spark.sql(\"SELECT javaStringLength2('test')\").collect()  # doctest: +SKIP\n",
      "     |      [Row(javaStringLength2(test)=4)]\n",
      "     |      \n",
      "     |      >>> spark.udf.registerJavaFunction(\n",
      "     |      ...     \"javaStringLength3\", \"test.org.apache.spark.sql.JavaStringLength\", \"integer\")\n",
      "     |      ... # doctest: +SKIP\n",
      "     |      >>> spark.sql(\"SELECT javaStringLength3('test')\").collect()  # doctest: +SKIP\n",
      "     |      [Row(javaStringLength3(test)=4)]\n",
      "     |  \n",
      "     |  registerJavaUDAF(self, name: str, javaClassName: str) -> None\n",
      "     |      Register a Java user-defined aggregate function as a SQL function.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.4.0\n",
      "     |          Supports Spark Connect.\n",
      "     |      \n",
      "     |      name : str\n",
      "     |          name of the user-defined aggregate function\n",
      "     |      javaClassName : str\n",
      "     |          fully qualified name of java class\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.udf.registerJavaUDAF(\"javaUDAF\", \"test.org.apache.spark.sql.MyDoubleAvg\")\n",
      "     |      ... # doctest: +SKIP\n",
      "     |      >>> df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"a\")],[\"id\", \"name\"])\n",
      "     |      >>> df.createOrReplaceTempView(\"df\")\n",
      "     |      >>> q = \"SELECT name, javaUDAF(id) as avg from df group by name order by name desc\"\n",
      "     |      >>> spark.sql(q).collect()  # doctest: +SKIP\n",
      "     |      [Row(name='b', avg=102.0), Row(name='a', avg=102.0)]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class UDTFRegistration(builtins.object)\n",
      "     |  UDTFRegistration(sparkSession: 'SparkSession')\n",
      "     |  \n",
      "     |  Wrapper for user-defined table function registration. This instance can be accessed by\n",
      "     |  :attr:`spark.udtf` or :attr:`sqlContext.udtf`.\n",
      "     |  \n",
      "     |  .. versionadded:: 3.5.0\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sparkSession: 'SparkSession')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  register(self, name: str, f: 'UserDefinedTableFunction') -> 'UserDefinedTableFunction'\n",
      "     |      Register a Python user-defined table function as a SQL table function.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.5.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          The name of the user-defined table function in SQL statements.\n",
      "     |      f : function or :meth:`pyspark.sql.functions.udtf`\n",
      "     |          The user-defined table function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      function\n",
      "     |          The registered user-defined table function.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Spark uses the return type of the given user-defined table function as the return\n",
      "     |      type of the registered user-defined function.\n",
      "     |      \n",
      "     |      To register a nondeterministic Python table function, users need to first build\n",
      "     |      a nondeterministic user-defined table function and then register it as a SQL function.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import udtf\n",
      "     |      >>> @udtf(returnType=\"c1: int, c2: int\")\n",
      "     |      ... class PlusOne:\n",
      "     |      ...     def eval(self, x: int):\n",
      "     |      ...         yield x, x + 1\n",
      "     |      ...\n",
      "     |      >>> _ = spark.udtf.register(name=\"plus_one\", f=PlusOne)\n",
      "     |      >>> spark.sql(\"SELECT * FROM plus_one(1)\").collect()\n",
      "     |      [Row(c1=1, c2=2)]\n",
      "     |      \n",
      "     |      Use it with lateral join\n",
      "     |      \n",
      "     |      >>> spark.sql(\"SELECT * FROM VALUES (0, 1), (1, 2) t(x, y), LATERAL plus_one(x)\").collect()\n",
      "     |      [Row(x=0, y=1, c1=0, c2=1), Row(x=1, y=2, c1=1, c2=2)]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Window(builtins.object)\n",
      "     |  Utility functions for defining window in DataFrames.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  When ordering is not defined, an unbounded window frame (rowFrame,\n",
      "     |  unboundedPreceding, unboundedFollowing) is used by default. When ordering is defined,\n",
      "     |  a growing window frame (rangeFrame, unboundedPreceding, currentRow) is used by default.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> # ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
      "     |  >>> window = Window.orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
      "     |  \n",
      "     |  >>> # PARTITION BY country ORDER BY date RANGE BETWEEN 3 PRECEDING AND 3 FOLLOWING\n",
      "     |  >>> window = Window.orderBy(\"date\").partitionBy(\"country\").rangeBetween(-3, 3)\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  orderBy(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')]]) -> 'WindowSpec'\n",
      "     |      Creates a :class:`WindowSpec` with the ordering defined.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, :class:`Column` or list\n",
      "     |          names of columns or expressions\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class: `WindowSpec`\n",
      "     |          A :class:`WindowSpec` with the ordering defined.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Window\n",
      "     |      >>> from pyspark.sql.functions import row_number\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(1, \"a\"), (1, \"a\"), (2, \"a\"), (1, \"b\"), (2, \"b\"), (3, \"b\")], [\"id\", \"category\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+--------+\n",
      "     |      | id|category|\n",
      "     |      +---+--------+\n",
      "     |      |  1|       a|\n",
      "     |      |  1|       a|\n",
      "     |      |  2|       a|\n",
      "     |      |  1|       b|\n",
      "     |      |  2|       b|\n",
      "     |      |  3|       b|\n",
      "     |      +---+--------+\n",
      "     |      \n",
      "     |      Show row number order by ``category`` in partition ``id``.\n",
      "     |      \n",
      "     |      >>> window = Window.partitionBy(\"id\").orderBy(\"category\")\n",
      "     |      >>> df.withColumn(\"row_number\", row_number().over(window)).show()\n",
      "     |      +---+--------+----------+\n",
      "     |      | id|category|row_number|\n",
      "     |      +---+--------+----------+\n",
      "     |      |  1|       a|         1|\n",
      "     |      |  1|       a|         2|\n",
      "     |      |  1|       b|         3|\n",
      "     |      |  2|       a|         1|\n",
      "     |      |  2|       b|         2|\n",
      "     |      |  3|       b|         1|\n",
      "     |      +---+--------+----------+\n",
      "     |  \n",
      "     |  partitionBy(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')]]) -> 'WindowSpec'\n",
      "     |      Creates a :class:`WindowSpec` with the partitioning defined.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, :class:`Column` or list\n",
      "     |          names of columns or expressions\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class: `WindowSpec`\n",
      "     |          A :class:`WindowSpec` with the partitioning defined.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Window\n",
      "     |      >>> from pyspark.sql.functions import row_number\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(1, \"a\"), (1, \"a\"), (2, \"a\"), (1, \"b\"), (2, \"b\"), (3, \"b\")], [\"id\", \"category\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+--------+\n",
      "     |      | id|category|\n",
      "     |      +---+--------+\n",
      "     |      |  1|       a|\n",
      "     |      |  1|       a|\n",
      "     |      |  2|       a|\n",
      "     |      |  1|       b|\n",
      "     |      |  2|       b|\n",
      "     |      |  3|       b|\n",
      "     |      +---+--------+\n",
      "     |      \n",
      "     |      Show row number order by ``id`` in partition ``category``.\n",
      "     |      \n",
      "     |      >>> window = Window.partitionBy(\"category\").orderBy(\"id\")\n",
      "     |      >>> df.withColumn(\"row_number\", row_number().over(window)).show()\n",
      "     |      +---+--------+----------+\n",
      "     |      | id|category|row_number|\n",
      "     |      +---+--------+----------+\n",
      "     |      |  1|       a|         1|\n",
      "     |      |  1|       a|         2|\n",
      "     |      |  2|       a|         3|\n",
      "     |      |  1|       b|         1|\n",
      "     |      |  2|       b|         2|\n",
      "     |      |  3|       b|         3|\n",
      "     |      +---+--------+----------+\n",
      "     |  \n",
      "     |  rangeBetween(start: int, end: int) -> 'WindowSpec'\n",
      "     |      Creates a :class:`WindowSpec` with the frame boundaries defined,\n",
      "     |      from `start` (inclusive) to `end` (inclusive).\n",
      "     |      \n",
      "     |      Both `start` and `end` are relative from the current row. For example,\n",
      "     |      \"0\" means \"current row\", while \"-1\" means one off before the current row,\n",
      "     |      and \"5\" means the five off after the current row.\n",
      "     |      \n",
      "     |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      "     |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      "     |      values directly.\n",
      "     |      \n",
      "     |      A range-based boundary is based on the actual value of the ORDER BY\n",
      "     |      expression(s). An offset is used to alter the value of the ORDER BY expression, for\n",
      "     |      instance if the current ORDER BY expression has a value of 10 and the lower bound offset\n",
      "     |      is -3, the resulting lower bound for the current row will be 10 - 3 = 7. This however puts a\n",
      "     |      number of constraints on the ORDER BY expressions: there can be only one expression and this\n",
      "     |      expression must have a numerical data type. An exception can be made when the offset is\n",
      "     |      unbounded, because no value modification is needed, in this case multiple and non-numeric\n",
      "     |      ORDER BY expression are allowed.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      start : int\n",
      "     |          boundary start, inclusive.\n",
      "     |          The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      "     |          any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n",
      "     |      end : int\n",
      "     |          boundary end, inclusive.\n",
      "     |          The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      "     |          any value greater than or equal to min(sys.maxsize, 9223372036854775807).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class: `WindowSpec`\n",
      "     |          A :class:`WindowSpec` with the frame boundaries defined,\n",
      "     |          from `start` (inclusive) to `end` (inclusive).\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Window\n",
      "     |      >>> from pyspark.sql import functions as func\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(1, \"a\"), (1, \"a\"), (2, \"a\"), (1, \"b\"), (2, \"b\"), (3, \"b\")], [\"id\", \"category\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+--------+\n",
      "     |      | id|category|\n",
      "     |      +---+--------+\n",
      "     |      |  1|       a|\n",
      "     |      |  1|       a|\n",
      "     |      |  2|       a|\n",
      "     |      |  1|       b|\n",
      "     |      |  2|       b|\n",
      "     |      |  3|       b|\n",
      "     |      +---+--------+\n",
      "     |      \n",
      "     |      Calculate sum of ``id`` in the range from ``id`` of currentRow to ``id`` of currentRow + 1\n",
      "     |      in partition ``category``\n",
      "     |      \n",
      "     |      >>> window = Window.partitionBy(\"category\").orderBy(\"id\").rangeBetween(Window.currentRow, 1)\n",
      "     |      >>> df.withColumn(\"sum\", func.sum(\"id\").over(window)).sort(\"id\", \"category\").show()\n",
      "     |      +---+--------+---+\n",
      "     |      | id|category|sum|\n",
      "     |      +---+--------+---+\n",
      "     |      |  1|       a|  4|\n",
      "     |      |  1|       a|  4|\n",
      "     |      |  1|       b|  3|\n",
      "     |      |  2|       a|  2|\n",
      "     |      |  2|       b|  5|\n",
      "     |      |  3|       b|  3|\n",
      "     |      +---+--------+---+\n",
      "     |  \n",
      "     |  rowsBetween(start: int, end: int) -> 'WindowSpec'\n",
      "     |      Creates a :class:`WindowSpec` with the frame boundaries defined,\n",
      "     |      from `start` (inclusive) to `end` (inclusive).\n",
      "     |      \n",
      "     |      Both `start` and `end` are relative positions from the current row.\n",
      "     |      For example, \"0\" means \"current row\", while \"-1\" means the row before\n",
      "     |      the current row, and \"5\" means the fifth row after the current row.\n",
      "     |      \n",
      "     |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      "     |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      "     |      values directly.\n",
      "     |      \n",
      "     |      A row based boundary is based on the position of the row within the partition.\n",
      "     |      An offset indicates the number of rows above or below the current row, the frame for the\n",
      "     |      current row starts or ends. For instance, given a row based sliding frame with a lower bound\n",
      "     |      offset of -1 and a upper bound offset of +2. The frame for row with index 5 would range from\n",
      "     |      index 4 to index 7.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      start : int\n",
      "     |          boundary start, inclusive.\n",
      "     |          The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      "     |          any value less than or equal to -9223372036854775808.\n",
      "     |      end : int\n",
      "     |          boundary end, inclusive.\n",
      "     |          The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      "     |          any value greater than or equal to 9223372036854775807.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class: `WindowSpec`\n",
      "     |          A :class:`WindowSpec` with the frame boundaries defined,\n",
      "     |          from `start` (inclusive) to `end` (inclusive).\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Window\n",
      "     |      >>> from pyspark.sql import functions as func\n",
      "     |      >>> df = spark.createDataFrame(\n",
      "     |      ...      [(1, \"a\"), (1, \"a\"), (2, \"a\"), (1, \"b\"), (2, \"b\"), (3, \"b\")], [\"id\", \"category\"])\n",
      "     |      >>> df.show()\n",
      "     |      +---+--------+\n",
      "     |      | id|category|\n",
      "     |      +---+--------+\n",
      "     |      |  1|       a|\n",
      "     |      |  1|       a|\n",
      "     |      |  2|       a|\n",
      "     |      |  1|       b|\n",
      "     |      |  2|       b|\n",
      "     |      |  3|       b|\n",
      "     |      +---+--------+\n",
      "     |      \n",
      "     |      Calculate sum of ``id`` in the range from currentRow to currentRow + 1\n",
      "     |      in partition ``category``\n",
      "     |      \n",
      "     |      >>> window = Window.partitionBy(\"category\").orderBy(\"id\").rowsBetween(Window.currentRow, 1)\n",
      "     |      >>> df.withColumn(\"sum\", func.sum(\"id\").over(window)).sort(\"id\", \"category\", \"sum\").show()\n",
      "     |      +---+--------+---+\n",
      "     |      | id|category|sum|\n",
      "     |      +---+--------+---+\n",
      "     |      |  1|       a|  2|\n",
      "     |      |  1|       a|  3|\n",
      "     |      |  1|       b|  3|\n",
      "     |      |  2|       a|  2|\n",
      "     |      |  2|       b|  5|\n",
      "     |      |  3|       b|  3|\n",
      "     |      +---+--------+---+\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'currentRow': <class 'int'>, 'unboundedFollowing': ...\n",
      "     |  \n",
      "     |  currentRow = 0\n",
      "     |  \n",
      "     |  unboundedFollowing = 9223372036854775807\n",
      "     |  \n",
      "     |  unboundedPreceding = -9223372036854775808\n",
      "    \n",
      "    class WindowSpec(builtins.object)\n",
      "     |  WindowSpec(jspec: py4j.java_gateway.JavaObject) -> None\n",
      "     |  \n",
      "     |  A window specification that defines the partitioning, ordering,\n",
      "     |  and frame boundaries.\n",
      "     |  \n",
      "     |  Use the static methods in :class:`Window` to create a :class:`WindowSpec`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  .. versionchanged:: 3.4.0\n",
      "     |      Supports Spark Connect.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, jspec: py4j.java_gateway.JavaObject) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  orderBy(self, *cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')]]) -> 'WindowSpec'\n",
      "     |      Defines the ordering columns in a :class:`WindowSpec`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, :class:`Column` or list\n",
      "     |          names of columns or expressions\n",
      "     |  \n",
      "     |  partitionBy(self, *cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')]]) -> 'WindowSpec'\n",
      "     |      Defines the partitioning columns in a :class:`WindowSpec`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, :class:`Column` or list\n",
      "     |          names of columns or expressions\n",
      "     |  \n",
      "     |  rangeBetween(self, start: int, end: int) -> 'WindowSpec'\n",
      "     |      Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).\n",
      "     |      \n",
      "     |      Both `start` and `end` are relative from the current row. For example,\n",
      "     |      \"0\" means \"current row\", while \"-1\" means one off before the current row,\n",
      "     |      and \"5\" means the five off after the current row.\n",
      "     |      \n",
      "     |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      "     |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      "     |      values directly.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      start : int\n",
      "     |          boundary start, inclusive.\n",
      "     |          The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      "     |          any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n",
      "     |      end : int\n",
      "     |          boundary end, inclusive.\n",
      "     |          The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      "     |          any value greater than or equal to min(sys.maxsize, 9223372036854775807).\n",
      "     |  \n",
      "     |  rowsBetween(self, start: int, end: int) -> 'WindowSpec'\n",
      "     |      Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).\n",
      "     |      \n",
      "     |      Both `start` and `end` are relative positions from the current row.\n",
      "     |      For example, \"0\" means \"current row\", while \"-1\" means the row before\n",
      "     |      the current row, and \"5\" means the fifth row after the current row.\n",
      "     |      \n",
      "     |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      "     |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      "     |      values directly.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      start : int\n",
      "     |          boundary start, inclusive.\n",
      "     |          The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      "     |          any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n",
      "     |      end : int\n",
      "     |          boundary end, inclusive.\n",
      "     |          The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      "     |          any value greater than or equal to min(sys.maxsize, 9223372036854775807).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['SparkSession', 'SQLContext', 'HiveContext', 'UDFRegistrati...\n",
      "\n",
      "FILE\n",
      "    /usr/local/spark/python/pyspark/sql/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql\n",
    "help(pyspark.sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e330ac5b-1e5f-4d27-9dc0-f14efa42b668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module pyspark.sql.functions in pyspark.sql:\n",
      "\n",
      "NAME\n",
      "    pyspark.sql.functions - A collections of builtin functions\n",
      "\n",
      "FUNCTIONS\n",
      "    abs(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the absolute value.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(abs(lit(-1))).show()\n",
      "        +-------+\n",
      "        |abs(-1)|\n",
      "        +-------+\n",
      "        |      1|\n",
      "        +-------+\n",
      "    \n",
      "    acos(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes inverse cosine of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            inverse cosine of `col`, as if computed by `java.lang.Math.acos()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1, 3)\n",
      "        >>> df.select(acos(df.id)).show()\n",
      "        +--------+\n",
      "        |ACOS(id)|\n",
      "        +--------+\n",
      "        |     0.0|\n",
      "        |     NaN|\n",
      "        +--------+\n",
      "    \n",
      "    acosh(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes inverse hyperbolic cosine of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(2)\n",
      "        >>> df.select(acosh(col(\"id\"))).show()\n",
      "        +---------+\n",
      "        |ACOSH(id)|\n",
      "        +---------+\n",
      "        |      NaN|\n",
      "        |      0.0|\n",
      "        +---------+\n",
      "    \n",
      "    add_months(start: 'ColumnOrName', months: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Returns the date that is `months` months after `start`. If `months` is a negative value\n",
      "        then these amount of months will be deducted from the `start`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            date column to work on.\n",
      "        months : :class:`~pyspark.sql.Column` or str or int\n",
      "            how many months after the given date to calculate.\n",
      "            Accepts negative value as well to calculate backwards.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a date after/before given number of months.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08', 2)], ['dt', 'add'])\n",
      "        >>> df.select(add_months(df.dt, 1).alias('next_month')).collect()\n",
      "        [Row(next_month=datetime.date(2015, 5, 8))]\n",
      "        >>> df.select(add_months(df.dt, df.add.cast('integer')).alias('next_month')).collect()\n",
      "        [Row(next_month=datetime.date(2015, 6, 8))]\n",
      "        >>> df.select(add_months('dt', -2).alias('prev_month')).collect()\n",
      "        [Row(prev_month=datetime.date(2015, 2, 8))]\n",
      "    \n",
      "    aes_decrypt(input: 'ColumnOrName', key: 'ColumnOrName', mode: Optional[ForwardRef('ColumnOrName')] = None, padding: Optional[ForwardRef('ColumnOrName')] = None, aad: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Returns a decrypted value of `input` using AES in `mode` with `padding`. Key lengths of 16,\n",
      "        24 and 32 bits are supported. Supported combinations of (`mode`, `padding`) are ('ECB',\n",
      "        'PKCS'), ('GCM', 'NONE') and ('CBC', 'PKCS'). Optional additional authenticated data (AAD) is\n",
      "        only supported for GCM. If provided for encryption, the identical AAD value must be provided\n",
      "        for decryption. The default mode is GCM.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        input : :class:`~pyspark.sql.Column` or str\n",
      "            The binary value to decrypt.\n",
      "        key : :class:`~pyspark.sql.Column` or str\n",
      "            The passphrase to use to decrypt the data.\n",
      "        mode : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Specifies which block cipher mode should be used to decrypt messages. Valid modes: ECB,\n",
      "            GCM, CBC.\n",
      "        padding : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Specifies how to pad messages whose length is not a multiple of the block size. Valid\n",
      "            values: PKCS, NONE, DEFAULT. The DEFAULT padding means PKCS for ECB, NONE for GCM and PKCS\n",
      "            for CBC.\n",
      "        aad : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Optional additional authenticated data. Only supported for GCM mode. This can be any\n",
      "            free-form input and must be provided for both encryption and decryption.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4\",\n",
      "        ...     \"abcdefghijklmnop12345678ABCDEFGH\", \"GCM\", \"DEFAULT\",\n",
      "        ...     \"This is an AAD mixed into the input\",)],\n",
      "        ...     [\"input\", \"key\", \"mode\", \"padding\", \"aad\"]\n",
      "        ... )\n",
      "        >>> df.select(aes_decrypt(\n",
      "        ...     unbase64(df.input), df.key, df.mode, df.padding, df.aad).alias('r')\n",
      "        ... ).collect()\n",
      "        [Row(r=bytearray(b'Spark'))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"AAAAAAAAAAAAAAAAAAAAAPSd4mWyMZ5mhvjiAPQJnfg=\",\n",
      "        ...     \"abcdefghijklmnop12345678ABCDEFGH\", \"CBC\", \"DEFAULT\",)],\n",
      "        ...     [\"input\", \"key\", \"mode\", \"padding\"]\n",
      "        ... )\n",
      "        >>> df.select(aes_decrypt(\n",
      "        ...     unbase64(df.input), df.key, df.mode, df.padding).alias('r')\n",
      "        ... ).collect()\n",
      "        [Row(r=bytearray(b'Spark'))]\n",
      "        \n",
      "        >>> df.select(aes_decrypt(unbase64(df.input), df.key, df.mode).alias('r')).collect()\n",
      "        [Row(r=bytearray(b'Spark'))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"83F16B2AA704794132802D248E6BFD4E380078182D1544813898AC97E709B28A94\",\n",
      "        ...     \"0000111122223333\",)],\n",
      "        ...     [\"input\", \"key\"]\n",
      "        ... )\n",
      "        >>> df.select(aes_decrypt(unhex(df.input), df.key).alias('r')).collect()\n",
      "        [Row(r=bytearray(b'Spark'))]\n",
      "    \n",
      "    aes_encrypt(input: 'ColumnOrName', key: 'ColumnOrName', mode: Optional[ForwardRef('ColumnOrName')] = None, padding: Optional[ForwardRef('ColumnOrName')] = None, iv: Optional[ForwardRef('ColumnOrName')] = None, aad: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Returns an encrypted value of `input` using AES in given `mode` with the specified `padding`.\n",
      "        Key lengths of 16, 24 and 32 bits are supported. Supported combinations of (`mode`,\n",
      "        `padding`) are ('ECB', 'PKCS'), ('GCM', 'NONE') and ('CBC', 'PKCS'). Optional initialization\n",
      "        vectors (IVs) are only supported for CBC and GCM modes. These must be 16 bytes for CBC and 12\n",
      "        bytes for GCM. If not provided, a random vector will be generated and prepended to the\n",
      "        output. Optional additional authenticated data (AAD) is only supported for GCM. If provided\n",
      "        for encryption, the identical AAD value must be provided for decryption. The default mode is\n",
      "        GCM.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        input : :class:`~pyspark.sql.Column` or str\n",
      "            The binary value to encrypt.\n",
      "        key : :class:`~pyspark.sql.Column` or str\n",
      "            The passphrase to use to encrypt the data.\n",
      "        mode : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Specifies which block cipher mode should be used to encrypt messages. Valid modes: ECB,\n",
      "            GCM, CBC.\n",
      "        padding : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Specifies how to pad messages whose length is not a multiple of the block size. Valid\n",
      "            values: PKCS, NONE, DEFAULT. The DEFAULT padding means PKCS for ECB, NONE for GCM and PKCS\n",
      "            for CBC.\n",
      "        iv : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Optional initialization vector. Only supported for CBC and GCM modes. Valid values: None or\n",
      "            \"\". 16-byte array for CBC mode. 12-byte array for GCM mode.\n",
      "        aad : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Optional additional authenticated data. Only supported for GCM mode. This can be any\n",
      "            free-form input and must be provided for both encryption and decryption.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"Spark\", \"abcdefghijklmnop12345678ABCDEFGH\", \"GCM\", \"DEFAULT\",\n",
      "        ...     \"000000000000000000000000\", \"This is an AAD mixed into the input\",)],\n",
      "        ...     [\"input\", \"key\", \"mode\", \"padding\", \"iv\", \"aad\"]\n",
      "        ... )\n",
      "        >>> df.select(base64(aes_encrypt(\n",
      "        ...     df.input, df.key, df.mode, df.padding, to_binary(df.iv, lit(\"hex\")), df.aad)\n",
      "        ... ).alias('r')).collect()\n",
      "        [Row(r='AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4')]\n",
      "        \n",
      "        >>> df.select(base64(aes_encrypt(\n",
      "        ...     df.input, df.key, df.mode, df.padding, to_binary(df.iv, lit(\"hex\")))\n",
      "        ... ).alias('r')).collect()\n",
      "        [Row(r='AAAAAAAAAAAAAAAAQiYi+sRNYDAOTjdSEcYBFsAWPL1f')]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"Spark SQL\", \"1234567890abcdef\", \"ECB\", \"PKCS\",)],\n",
      "        ...     [\"input\", \"key\", \"mode\", \"padding\"]\n",
      "        ... )\n",
      "        >>> df.select(aes_decrypt(aes_encrypt(df.input, df.key, df.mode, df.padding),\n",
      "        ...     df.key, df.mode, df.padding).alias('r')\n",
      "        ... ).collect()\n",
      "        [Row(r=bytearray(b'Spark SQL'))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"Spark SQL\", \"0000111122223333\", \"ECB\",)],\n",
      "        ...     [\"input\", \"key\", \"mode\"]\n",
      "        ... )\n",
      "        >>> df.select(aes_decrypt(aes_encrypt(df.input, df.key, df.mode),\n",
      "        ...     df.key, df.mode).alias('r')\n",
      "        ... ).collect()\n",
      "        [Row(r=bytearray(b'Spark SQL'))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"Spark SQL\", \"abcdefghijklmnop\",)],\n",
      "        ...     [\"input\", \"key\"]\n",
      "        ... )\n",
      "        >>> df.select(aes_decrypt(\n",
      "        ...     unbase64(base64(aes_encrypt(df.input, df.key))), df.key\n",
      "        ... ).cast(\"STRING\").alias('r')).collect()\n",
      "        [Row(r='Spark SQL')]\n",
      "    \n",
      "    aggregate(col: 'ColumnOrName', initialValue: 'ColumnOrName', merge: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column], finish: Optional[Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column]] = None) -> pyspark.sql.column.Column\n",
      "        Applies a binary operator to an initial state and all elements in the array,\n",
      "        and reduces this to a single state. The final state is converted into the final result\n",
      "        by applying a finish function.\n",
      "        \n",
      "        Both functions can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "        Python ``UserDefinedFunctions`` are not supported\n",
      "        (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        initialValue : :class:`~pyspark.sql.Column` or str\n",
      "            initial value. Name of column or expression\n",
      "        merge : function\n",
      "            a binary function ``(acc: Column, x: Column) -> Column...`` returning expression\n",
      "            of the same type as ``zero``\n",
      "        finish : function\n",
      "            an optional unary function ``(x: Column) -> Column: ...``\n",
      "            used to convert accumulated value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            final value after aggregate function is applied.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\"))\n",
      "        >>> df.select(aggregate(\"values\", lit(0.0), lambda acc, x: acc + x).alias(\"sum\")).show()\n",
      "        +----+\n",
      "        | sum|\n",
      "        +----+\n",
      "        |42.0|\n",
      "        +----+\n",
      "        \n",
      "        >>> def merge(acc, x):\n",
      "        ...     count = acc.count + 1\n",
      "        ...     sum = acc.sum + x\n",
      "        ...     return struct(count.alias(\"count\"), sum.alias(\"sum\"))\n",
      "        ...\n",
      "        >>> df.select(\n",
      "        ...     aggregate(\n",
      "        ...         \"values\",\n",
      "        ...         struct(lit(0).alias(\"count\"), lit(0.0).alias(\"sum\")),\n",
      "        ...         merge,\n",
      "        ...         lambda acc: acc.sum / acc.count,\n",
      "        ...     ).alias(\"mean\")\n",
      "        ... ).show()\n",
      "        +----+\n",
      "        |mean|\n",
      "        +----+\n",
      "        | 8.4|\n",
      "        +----+\n",
      "    \n",
      "    any_value(col: 'ColumnOrName', ignoreNulls: Union[bool, pyspark.sql.column.Column, NoneType] = None) -> pyspark.sql.column.Column\n",
      "        Returns some value of `col` for a group of rows.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        ignorenulls : :class:`~pyspark.sql.Column` or bool\n",
      "            if first value is null then look for first non-null value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            some value of `col` for a group of rows.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None, 1),\n",
      "        ...                             (\"a\", 2),\n",
      "        ...                             (\"a\", 3),\n",
      "        ...                             (\"b\", 8),\n",
      "        ...                             (\"b\", 2)], [\"c1\", \"c2\"])\n",
      "        >>> df.select(any_value('c1'), any_value('c2')).collect()\n",
      "        [Row(any_value(c1)=None, any_value(c2)=1)]\n",
      "        >>> df.select(any_value('c1', True), any_value('c2', True)).collect()\n",
      "        [Row(any_value(c1)='a', any_value(c2)=1)]\n",
      "    \n",
      "    approxCountDistinct(col: 'ColumnOrName', rsd: Optional[float] = None) -> pyspark.sql.column.Column\n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 2.1.0\n",
      "            Use :func:`approx_count_distinct` instead.\n",
      "    \n",
      "    approx_count_distinct(col: 'ColumnOrName', rsd: Optional[float] = None) -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns a new :class:`~pyspark.sql.Column` for approximate distinct count\n",
      "        of column `col`.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        rsd : float, optional\n",
      "            maximum relative standard deviation allowed (default = 0.05).\n",
      "            For rsd < 0.01, it is more efficient to use :func:`count_distinct`\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column of computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([1,2,2,3], \"INT\")\n",
      "        >>> df.agg(approx_count_distinct(\"value\").alias('distinct_values')).show()\n",
      "        +---------------+\n",
      "        |distinct_values|\n",
      "        +---------------+\n",
      "        |              3|\n",
      "        +---------------+\n",
      "    \n",
      "    approx_percentile(col: 'ColumnOrName', percentage: Union[pyspark.sql.column.Column, float, List[float], Tuple[float]], accuracy: Union[pyspark.sql.column.Column, float] = 10000) -> pyspark.sql.column.Column\n",
      "        Returns the approximate `percentile` of the numeric column `col` which is the smallest value\n",
      "        in the ordered `col` values (sorted from least to greatest) such that no more than `percentage`\n",
      "        of `col` values is less than the value or equal to that value.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column.\n",
      "        percentage : :class:`~pyspark.sql.Column`, float, list of floats or tuple of floats\n",
      "            percentage in decimal (must be between 0.0 and 1.0).\n",
      "            When percentage is an array, each value of the percentage array must be between 0.0 and 1.0.\n",
      "            In this case, returns the approximate percentile array of column col\n",
      "            at the given percentage array.\n",
      "        accuracy : :class:`~pyspark.sql.Column` or float\n",
      "            is a positive numeric literal which controls approximation accuracy\n",
      "            at the cost of memory. Higher value of accuracy yields better accuracy,\n",
      "            1.0/accuracy is the relative error of the approximation. (default: 10000).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            approximate `percentile` of the numeric column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> key = (sf.col(\"id\") % 3).alias(\"key\")\n",
      "        >>> value = (sf.randn(42) + key * 10).alias(\"value\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(key, value)\n",
      "        >>> df.select(\n",
      "        ...     sf.approx_percentile(\"value\", [0.25, 0.5, 0.75], 1000000)\n",
      "        ... ).printSchema()\n",
      "        root\n",
      "         |-- approx_percentile(value, array(0.25, 0.5, 0.75), 1000000): array (nullable = true)\n",
      "         |    |-- element: double (containsNull = false)\n",
      "        \n",
      "        >>> df.groupBy(\"key\").agg(\n",
      "        ...     sf.approx_percentile(\"value\", 0.5, sf.lit(1000000))\n",
      "        ... ).printSchema()\n",
      "        root\n",
      "         |-- key: long (nullable = true)\n",
      "         |-- approx_percentile(value, 0.5, 1000000): double (nullable = true)\n",
      "    \n",
      "    array(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')], Tuple[ForwardRef('ColumnOrName_'), ...]]) -> pyspark.sql.column.Column\n",
      "        Creates a new array column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s that have\n",
      "            the same data type.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of array type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n",
      "        >>> df.select(array('age', 'age').alias(\"arr\")).collect()\n",
      "        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "        >>> df.select(array([df.age, df.age]).alias(\"arr\")).collect()\n",
      "        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "        >>> df.select(array('age', 'age').alias(\"col\")).printSchema()\n",
      "        root\n",
      "         |-- col: array (nullable = false)\n",
      "         |    |-- element: long (containsNull = true)\n",
      "    \n",
      "    array_agg(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns a list of objects with duplicates.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            list of objects with duplicates.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
      "        >>> df.agg(array_agg('c').alias('r')).collect()\n",
      "        [Row(r=[1, 1, 2])]\n",
      "    \n",
      "    array_append(col: 'ColumnOrName', value: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: returns an array of the elements in col1 along\n",
      "        with the added element in col2 at the last of the array.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        value :\n",
      "            a literal value, or a :class:`~pyspark.sql.Column` expression.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of values from first array along with the element.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=\"c\")])\n",
      "        >>> df.select(array_append(df.c1, df.c2)).collect()\n",
      "        [Row(array_append(c1, c2)=['b', 'a', 'c', 'c'])]\n",
      "        >>> df.select(array_append(df.c1, 'x')).collect()\n",
      "        [Row(array_append(c1, x)=['b', 'a', 'c', 'x'])]\n",
      "    \n",
      "    array_compact(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: removes null values from the array.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array by excluding the null values.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, None, 2, 3],), ([4, 5, None, 4],)], ['data'])\n",
      "        >>> df.select(array_compact(df.data)).collect()\n",
      "        [Row(array_compact(data)=[1, 2, 3]), Row(array_compact(data)=[4, 5, 4])]\n",
      "    \n",
      "    array_contains(col: 'ColumnOrName', value: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: returns null if the array is null, true if the array contains the\n",
      "        given value, and false otherwise.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        value :\n",
      "            value or column to check for in array\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of Boolean type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
      "        >>> df.select(array_contains(df.data, \"a\")).collect()\n",
      "        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n",
      "        >>> df.select(array_contains(df.data, lit(\"a\"))).collect()\n",
      "        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n",
      "    \n",
      "    array_distinct(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: removes duplicate values from the array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of unique values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3, 2],), ([4, 5, 5, 4],)], ['data'])\n",
      "        >>> df.select(array_distinct(df.data)).collect()\n",
      "        [Row(array_distinct(data)=[1, 2, 3]), Row(array_distinct(data)=[4, 5])]\n",
      "    \n",
      "    array_except(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns an array of the elements in col1 but not in col2,\n",
      "        without duplicates.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of values from first array that are not in the second.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
      "        >>> df.select(array_except(df.c1, df.c2)).collect()\n",
      "        [Row(array_except(c1, c2)=['b'])]\n",
      "    \n",
      "    array_insert(arr: 'ColumnOrName', pos: Union[ForwardRef('ColumnOrName'), int], value: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: adds an item into a given array at a specified array index.\n",
      "        Array indices start at 1, or start from the end if index is negative.\n",
      "        Index above array size appends the array, or prepends the array if index is negative,\n",
      "        with 'null' elements.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        arr : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing an array\n",
      "        pos : :class:`~pyspark.sql.Column` or str or int\n",
      "            name of Numeric type column indicating position of insertion\n",
      "            (starting at index 1, negative position is a start from the back of the array)\n",
      "        value :\n",
      "            a literal value, or a :class:`~pyspark.sql.Column` expression.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of values, including the new specified value\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(['a', 'b', 'c'], 2, 'd'), (['c', 'b', 'a'], -2, 'd')],\n",
      "        ...     ['data', 'pos', 'val']\n",
      "        ... )\n",
      "        >>> df.select(array_insert(df.data, df.pos.cast('integer'), df.val).alias('data')).collect()\n",
      "        [Row(data=['a', 'd', 'b', 'c']), Row(data=['c', 'b', 'd', 'a'])]\n",
      "        >>> df.select(array_insert(df.data, 5, 'hello').alias('data')).collect()\n",
      "        [Row(data=['a', 'b', 'c', None, 'hello']), Row(data=['c', 'b', 'a', None, 'hello'])]\n",
      "    \n",
      "    array_intersect(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns an array of the elements in the intersection of col1 and col2,\n",
      "        without duplicates.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of values in the intersection of two arrays.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
      "        >>> df.select(array_intersect(df.c1, df.c2)).collect()\n",
      "        [Row(array_intersect(c1, c2)=['a', 'c'])]\n",
      "    \n",
      "    array_join(col: 'ColumnOrName', delimiter: str, null_replacement: Optional[str] = None) -> pyspark.sql.column.Column\n",
      "        Concatenates the elements of `column` using the `delimiter`. Null values are replaced with\n",
      "        `null_replacement` if set, otherwise they are ignored.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        delimiter : str\n",
      "            delimiter used to concatenate elements\n",
      "        null_replacement : str, optional\n",
      "            if set then null values will be replaced by this value\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of string type. Concatenated values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\", None],)], ['data'])\n",
      "        >>> df.select(array_join(df.data, \",\").alias(\"joined\")).collect()\n",
      "        [Row(joined='a,b,c'), Row(joined='a')]\n",
      "        >>> df.select(array_join(df.data, \",\", \"NULL\").alias(\"joined\")).collect()\n",
      "        [Row(joined='a,b,c'), Row(joined='a,NULL')]\n",
      "    \n",
      "    array_max(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns the maximum value of the array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            maximum value of an array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n",
      "        >>> df.select(array_max(df.data).alias('max')).collect()\n",
      "        [Row(max=3), Row(max=10)]\n",
      "    \n",
      "    array_min(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns the minimum value of the array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            minimum value of array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n",
      "        >>> df.select(array_min(df.data).alias('min')).collect()\n",
      "        [Row(min=1), Row(min=-1)]\n",
      "    \n",
      "    array_position(col: 'ColumnOrName', value: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: Locates the position of the first occurrence of the given value\n",
      "        in the given array. Returns null if either of the arguments are null.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index. Returns 0 if the given\n",
      "        value could not be found in the array.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        value : Any\n",
      "            value to look for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            position of the value in the given array if found and 0 otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"c\", \"b\", \"a\"],), ([],)], ['data'])\n",
      "        >>> df.select(array_position(df.data, \"a\")).collect()\n",
      "        [Row(array_position(data, a)=3), Row(array_position(data, a)=0)]\n",
      "    \n",
      "    array_prepend(col: 'ColumnOrName', value: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: Returns an array containing element as\n",
      "        well as all elements from array. The new element is positioned\n",
      "        at the beginning of the array.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        value :\n",
      "            a literal value, or a :class:`~pyspark.sql.Column` expression.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array excluding given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 3, 4],), ([],)], ['data'])\n",
      "        >>> df.select(array_prepend(df.data, 1)).collect()\n",
      "        [Row(array_prepend(data, 1)=[1, 2, 3, 4]), Row(array_prepend(data, 1)=[1])]\n",
      "    \n",
      "    array_remove(col: 'ColumnOrName', element: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: Remove all elements that equal to element from the given array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        element :\n",
      "            element to be removed from the array\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array excluding given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], ['data'])\n",
      "        >>> df.select(array_remove(df.data, 1)).collect()\n",
      "        [Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]\n",
      "    \n",
      "    array_repeat(col: 'ColumnOrName', count: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Collection function: creates an array containing a column repeated count times.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column name or column that contains the element to be repeated\n",
      "        count : :class:`~pyspark.sql.Column` or str or int\n",
      "            column name, column, or int containing the number of times to repeat the first argument\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of repeated elements.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('ab',)], ['data'])\n",
      "        >>> df.select(array_repeat(df.data, 3).alias('r')).collect()\n",
      "        [Row(r=['ab', 'ab', 'ab'])]\n",
      "    \n",
      "    array_size(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the total number of elements in the array. The function returns null for null input.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            total number of elements in the array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],), (None,)], ['data'])\n",
      "        >>> df.select(array_size(df.data).alias('r')).collect()\n",
      "        [Row(r=3), Row(r=None)]\n",
      "    \n",
      "    array_sort(col: 'ColumnOrName', comparator: Optional[Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]] = None) -> pyspark.sql.column.Column\n",
      "        Collection function: sorts the input array in ascending order. The elements of the input array\n",
      "        must be orderable. Null elements will be placed at the end of the returned array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Can take a `comparator` function.\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        comparator : callable, optional\n",
      "            A binary ``(Column, Column) -> Column: ...``.\n",
      "            The comparator will take two\n",
      "            arguments representing two elements of the array. It returns a negative integer, 0, or a\n",
      "            positive integer as the first element is less than, equal to, or greater than the second\n",
      "            element. If the comparator function returns null, the function will fail and raise an error.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            sorted array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(array_sort(df.data).alias('r')).collect()\n",
      "        [Row(r=[1, 2, 3, None]), Row(r=[1]), Row(r=[])]\n",
      "        >>> df = spark.createDataFrame([([\"foo\", \"foobar\", None, \"bar\"],),([\"foo\"],),([],)], ['data'])\n",
      "        >>> df.select(array_sort(\n",
      "        ...     \"data\",\n",
      "        ...     lambda x, y: when(x.isNull() | y.isNull(), lit(0)).otherwise(length(y) - length(x))\n",
      "        ... ).alias(\"r\")).collect()\n",
      "        [Row(r=['foobar', 'foo', None, 'bar']), Row(r=['foo']), Row(r=[])]\n",
      "    \n",
      "    array_union(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns an array of the elements in the union of col1 and col2,\n",
      "        without duplicates.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of values in union of two arrays.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
      "        >>> df.select(array_union(df.c1, df.c2)).collect()\n",
      "        [Row(array_union(c1, c2)=['b', 'a', 'c', 'd', 'f'])]\n",
      "    \n",
      "    arrays_overlap(a1: 'ColumnOrName', a2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns true if the arrays contain any common non-null element; if not,\n",
      "        returns null if both the arrays are non-empty and any of them contains a null element; returns\n",
      "        false otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of Boolean type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\"], [\"b\", \"c\"]), ([\"a\"], [\"b\", \"c\"])], ['x', 'y'])\n",
      "        >>> df.select(arrays_overlap(df.x, df.y).alias(\"overlap\")).collect()\n",
      "        [Row(overlap=True), Row(overlap=False)]\n",
      "    \n",
      "    arrays_zip(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: Returns a merged array of structs in which the N-th struct contains all\n",
      "        N-th values of input arrays. If one of the arrays is shorter than others then\n",
      "        resulting struct type value will be a `null` for missing elements.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            columns of arrays to be merged.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            merged array of entries.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import arrays_zip\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3], [2, 4, 6], [3, 6])], ['vals1', 'vals2', 'vals3'])\n",
      "        >>> df = df.select(arrays_zip(df.vals1, df.vals2, df.vals3).alias('zipped'))\n",
      "        >>> df.show(truncate=False)\n",
      "        +------------------------------------+\n",
      "        |zipped                              |\n",
      "        +------------------------------------+\n",
      "        |[{1, 2, 3}, {2, 4, 6}, {3, 6, NULL}]|\n",
      "        +------------------------------------+\n",
      "        >>> df.printSchema()\n",
      "        root\n",
      "         |-- zipped: array (nullable = true)\n",
      "         |    |-- element: struct (containsNull = false)\n",
      "         |    |    |-- vals1: long (nullable = true)\n",
      "         |    |    |-- vals2: long (nullable = true)\n",
      "         |    |    |-- vals3: long (nullable = true)\n",
      "    \n",
      "    asc(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sort expression based on the ascending order of the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to sort by in the ascending order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column specifying the order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Sort by the column 'id' in the descending order.\n",
      "        \n",
      "        >>> df = spark.range(5)\n",
      "        >>> df = df.sort(desc(\"id\"))\n",
      "        >>> df.show()\n",
      "        +---+\n",
      "        | id|\n",
      "        +---+\n",
      "        |  4|\n",
      "        |  3|\n",
      "        |  2|\n",
      "        |  1|\n",
      "        |  0|\n",
      "        +---+\n",
      "        \n",
      "        Sort by the column 'id' in the ascending order.\n",
      "        \n",
      "        >>> df.orderBy(asc(\"id\")).show()\n",
      "        +---+\n",
      "        | id|\n",
      "        +---+\n",
      "        |  0|\n",
      "        |  1|\n",
      "        |  2|\n",
      "        |  3|\n",
      "        |  4|\n",
      "        +---+\n",
      "    \n",
      "    asc_nulls_first(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sort expression based on the ascending order of the given\n",
      "        column name, and null values return before non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to sort by in the ascending order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column specifying the order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(1, \"Bob\"),\n",
      "        ...                              (0, None),\n",
      "        ...                              (2, \"Alice\")], [\"age\", \"name\"])\n",
      "        >>> df1.sort(asc_nulls_first(df1.name)).show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  0| NULL|\n",
      "        |  2|Alice|\n",
      "        |  1|  Bob|\n",
      "        +---+-----+\n",
      "    \n",
      "    asc_nulls_last(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sort expression based on the ascending order of the given\n",
      "        column name, and null values appear after non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to sort by in the ascending order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column specifying the order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(0, None),\n",
      "        ...                              (1, \"Bob\"),\n",
      "        ...                              (2, \"Alice\")], [\"age\", \"name\"])\n",
      "        >>> df1.sort(asc_nulls_last(df1.name)).show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  2|Alice|\n",
      "        |  1|  Bob|\n",
      "        |  0| NULL|\n",
      "        +---+-----+\n",
      "    \n",
      "    ascii(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the numeric value of the first character of the string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            numeric value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
      "        >>> df.select(ascii(\"value\")).show()\n",
      "        +------------+\n",
      "        |ascii(value)|\n",
      "        +------------+\n",
      "        |          83|\n",
      "        |          80|\n",
      "        |          80|\n",
      "        +------------+\n",
      "    \n",
      "    asin(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes inverse sine of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            inverse sine of `col`, as if computed by `java.lang.Math.asin()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(0,), (2,)])\n",
      "        >>> df.select(asin(df.schema.fieldNames()[0])).show()\n",
      "        +--------+\n",
      "        |ASIN(_1)|\n",
      "        +--------+\n",
      "        |     0.0|\n",
      "        |     NaN|\n",
      "        +--------+\n",
      "    \n",
      "    asinh(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes inverse hyperbolic sine of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(asinh(col(\"id\"))).show()\n",
      "        +---------+\n",
      "        |ASINH(id)|\n",
      "        +---------+\n",
      "        |      0.0|\n",
      "        +---------+\n",
      "    \n",
      "    assert_true(col: 'ColumnOrName', errMsg: Union[pyspark.sql.column.Column, str, NoneType] = None) -> pyspark.sql.column.Column\n",
      "        Returns `null` if the input column is `true`; throws an exception\n",
      "        with the provided error message otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column name or column that represents the input column to test\n",
      "        errMsg : :class:`~pyspark.sql.Column` or str, optional\n",
      "            A Python string literal or column containing the error message\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            `null` if the input column is `true` otherwise throws an error with specified message.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(0,1)], ['a', 'b'])\n",
      "        >>> df.select(assert_true(df.a < df.b).alias('r')).collect()\n",
      "        [Row(r=None)]\n",
      "        >>> df.select(assert_true(df.a < df.b, df.a).alias('r')).collect()\n",
      "        [Row(r=None)]\n",
      "        >>> df.select(assert_true(df.a < df.b, 'error').alias('r')).collect()\n",
      "        [Row(r=None)]\n",
      "        >>> df.select(assert_true(df.a > df.b, 'My error msg').alias('r')).collect() # doctest: +SKIP\n",
      "        ...\n",
      "        java.lang.RuntimeException: My error msg\n",
      "        ...\n",
      "    \n",
      "    atan(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Compute inverse tangent of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            inverse tangent of `col`, as if computed by `java.lang.Math.atan()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(atan(df.id)).show()\n",
      "        +--------+\n",
      "        |ATAN(id)|\n",
      "        +--------+\n",
      "        |     0.0|\n",
      "        +--------+\n",
      "    \n",
      "    atan2(col1: Union[ForwardRef('ColumnOrName'), float], col2: Union[ForwardRef('ColumnOrName'), float]) -> pyspark.sql.column.Column\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str, :class:`~pyspark.sql.Column` or float\n",
      "            coordinate on y-axis\n",
      "        col2 : str, :class:`~pyspark.sql.Column` or float\n",
      "            coordinate on x-axis\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the `theta` component of the point\n",
      "            (`r`, `theta`)\n",
      "            in polar coordinates that corresponds to the point\n",
      "            (`x`, `y`) in Cartesian coordinates,\n",
      "            as if computed by `java.lang.Math.atan2()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(atan2(lit(1), lit(2))).first()\n",
      "        Row(ATAN2(1, 2)=0.46364...)\n",
      "    \n",
      "    atanh(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes inverse hyperbolic tangent of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(0,), (2,)], schema=[\"numbers\"])\n",
      "        >>> df.select(atanh(df[\"numbers\"])).show()\n",
      "        +--------------+\n",
      "        |ATANH(numbers)|\n",
      "        +--------------+\n",
      "        |           0.0|\n",
      "        |           NaN|\n",
      "        +--------------+\n",
      "    \n",
      "    avg(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the average of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(10)\n",
      "        >>> df.select(avg(col(\"id\"))).show()\n",
      "        +-------+\n",
      "        |avg(id)|\n",
      "        +-------+\n",
      "        |    4.5|\n",
      "        +-------+\n",
      "    \n",
      "    base64(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the BASE64 encoding of a binary column and returns it as a string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            BASE64 encoding of string value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
      "        >>> df.select(base64(\"value\")).show()\n",
      "        +----------------+\n",
      "        |   base64(value)|\n",
      "        +----------------+\n",
      "        |        U3Bhcms=|\n",
      "        |    UHlTcGFyaw==|\n",
      "        |UGFuZGFzIEFQSQ==|\n",
      "        +----------------+\n",
      "    \n",
      "    bin(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the string representation of the binary value of the given column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            binary representation of given value as string.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([2,5], \"INT\")\n",
      "        >>> df.select(bin(df.value).alias('c')).collect()\n",
      "        [Row(c='10'), Row(c='101')]\n",
      "    \n",
      "    bit_and(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the bitwise AND of all non-null input values, or null if none.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the bitwise AND of all non-null input values, or null if none.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
      "        >>> df.select(bit_and(\"c\")).first()\n",
      "        Row(bit_and(c)=0)\n",
      "    \n",
      "    bit_count(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of bits that are set in the argument expr as an unsigned 64-bit integer,\n",
      "        or NULL if the argument is NULL.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the number of bits that are set in the argument expr as an unsigned 64-bit integer,\n",
      "            or NULL if the argument is NULL.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
      "        >>> df.select(bit_count(\"c\")).show()\n",
      "        +------------+\n",
      "        |bit_count(c)|\n",
      "        +------------+\n",
      "        |           1|\n",
      "        |           1|\n",
      "        |           1|\n",
      "        +------------+\n",
      "    \n",
      "    bit_get(col: 'ColumnOrName', pos: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the value of the bit (0 or 1) at the specified position.\n",
      "        The positions are numbered from right to left, starting at zero.\n",
      "        The position argument cannot be negative.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        pos : :class:`~pyspark.sql.Column` or str\n",
      "            The positions are numbered from right to left, starting at zero.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the value of the bit (0 or 1) at the specified position.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
      "        >>> df.select(bit_get(\"c\", lit(1))).show()\n",
      "        +-------------+\n",
      "        |bit_get(c, 1)|\n",
      "        +-------------+\n",
      "        |            0|\n",
      "        |            0|\n",
      "        |            1|\n",
      "        +-------------+\n",
      "    \n",
      "    bit_length(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calculates the bit length for the specified string column.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Source column or strings\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            Bit length of the col\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import bit_length\n",
      "        >>> spark.createDataFrame([('cat',), ( '🐈',)], ['cat']) \\\n",
      "        ...      .select(bit_length('cat')).collect()\n",
      "            [Row(bit_length(cat)=24), Row(bit_length(cat)=32)]\n",
      "    \n",
      "    bit_or(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the bitwise OR of all non-null input values, or null if none.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the bitwise OR of all non-null input values, or null if none.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
      "        >>> df.select(bit_or(\"c\")).first()\n",
      "        Row(bit_or(c)=3)\n",
      "    \n",
      "    bit_xor(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the bitwise XOR of all non-null input values, or null if none.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the bitwise XOR of all non-null input values, or null if none.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
      "        >>> df.select(bit_xor(\"c\")).first()\n",
      "        Row(bit_xor(c)=2)\n",
      "    \n",
      "    bitmap_bit_position(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the bit position for the given input column.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            The input column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(123,)], [\"a\"])\n",
      "        >>> df.select(bitmap_bit_position(df.a).alias(\"r\")).collect()\n",
      "        [Row(r=122)]\n",
      "    \n",
      "    bitmap_bucket_number(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the bucket number for the given input column.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            The input column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(123,)], [\"a\"])\n",
      "        >>> df.select(bitmap_bucket_number(df.a).alias(\"r\")).collect()\n",
      "        [Row(r=1)]\n",
      "    \n",
      "    bitmap_construct_agg(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a bitmap with the positions of the bits set from all the values from the input column.\n",
      "        The input column will most likely be bitmap_bit_position().\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            The input column will most likely be bitmap_bit_position().\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1,),(2,),(3,)], [\"a\"])\n",
      "        >>> df.select(substring(hex(\n",
      "        ...     bitmap_construct_agg(bitmap_bit_position(df.a))\n",
      "        ... ), 0, 6).alias(\"r\")).collect()\n",
      "        [Row(r='070000')]\n",
      "    \n",
      "    bitmap_count(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of set bits in the input bitmap.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            The input bitmap.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"FFFF\",)], [\"a\"])\n",
      "        >>> df.select(bitmap_count(to_binary(df.a, lit(\"hex\"))).alias('r')).collect()\n",
      "        [Row(r=16)]\n",
      "    \n",
      "    bitmap_or_agg(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a bitmap that is the bitwise OR of all of the bitmaps from the input column.\n",
      "        The input column should be bitmaps created from bitmap_construct_agg().\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            The input column should be bitmaps created from bitmap_construct_agg().\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"10\",),(\"20\",),(\"40\",)], [\"a\"])\n",
      "        >>> df.select(substring(hex(\n",
      "        ...     bitmap_or_agg(to_binary(df.a, lit(\"hex\")))\n",
      "        ... ), 0, 6).alias(\"r\")).collect()\n",
      "        [Row(r='700000')]\n",
      "    \n",
      "    bitwiseNOT(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes bitwise not.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 3.2.0\n",
      "            Use :func:`bitwise_not` instead.\n",
      "    \n",
      "    bitwise_not(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes bitwise not.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(bitwise_not(lit(0))).show()\n",
      "        +---+\n",
      "        | ~0|\n",
      "        +---+\n",
      "        | -1|\n",
      "        +---+\n",
      "        >>> df.select(bitwise_not(lit(1))).show()\n",
      "        +---+\n",
      "        | ~1|\n",
      "        +---+\n",
      "        | -2|\n",
      "        +---+\n",
      "    \n",
      "    bool_and(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns true if all values of `col` are true.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to check if all values are true.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            true if all values of `col` are true, false otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[True], [True], [True]], [\"flag\"])\n",
      "        >>> df.select(bool_and(\"flag\")).show()\n",
      "        +--------------+\n",
      "        |bool_and(flag)|\n",
      "        +--------------+\n",
      "        |          true|\n",
      "        +--------------+\n",
      "        >>> df = spark.createDataFrame([[True], [False], [True]], [\"flag\"])\n",
      "        >>> df.select(bool_and(\"flag\")).show()\n",
      "        +--------------+\n",
      "        |bool_and(flag)|\n",
      "        +--------------+\n",
      "        |         false|\n",
      "        +--------------+\n",
      "        >>> df = spark.createDataFrame([[False], [False], [False]], [\"flag\"])\n",
      "        >>> df.select(bool_and(\"flag\")).show()\n",
      "        +--------------+\n",
      "        |bool_and(flag)|\n",
      "        +--------------+\n",
      "        |         false|\n",
      "        +--------------+\n",
      "    \n",
      "    bool_or(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns true if at least one value of `col` is true.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to check if at least one value is true.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            true if at least one value of `col` is true, false otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[True], [True], [True]], [\"flag\"])\n",
      "        >>> df.select(bool_or(\"flag\")).show()\n",
      "        +-------------+\n",
      "        |bool_or(flag)|\n",
      "        +-------------+\n",
      "        |         true|\n",
      "        +-------------+\n",
      "        >>> df = spark.createDataFrame([[True], [False], [True]], [\"flag\"])\n",
      "        >>> df.select(bool_or(\"flag\")).show()\n",
      "        +-------------+\n",
      "        |bool_or(flag)|\n",
      "        +-------------+\n",
      "        |         true|\n",
      "        +-------------+\n",
      "        >>> df = spark.createDataFrame([[False], [False], [False]], [\"flag\"])\n",
      "        >>> df.select(bool_or(\"flag\")).show()\n",
      "        +-------------+\n",
      "        |bool_or(flag)|\n",
      "        +-------------+\n",
      "        |        false|\n",
      "        +-------------+\n",
      "    \n",
      "    broadcast(df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame\n",
      "        Marks a DataFrame as small enough for use in broadcast joins.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.DataFrame`\n",
      "            DataFrame marked as ready for broadcast join.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import types\n",
      "        >>> df = spark.createDataFrame([1, 2, 3, 3, 4], types.IntegerType())\n",
      "        >>> df_small = spark.range(3)\n",
      "        >>> df_b = broadcast(df_small)\n",
      "        >>> df.join(df_b, df.value == df_small.id).show()\n",
      "        +-----+---+\n",
      "        |value| id|\n",
      "        +-----+---+\n",
      "        |    1|  1|\n",
      "        |    2|  2|\n",
      "        +-----+---+\n",
      "    \n",
      "    bround(col: 'ColumnOrName', scale: int = 0) -> pyspark.sql.column.Column\n",
      "        Round the given value to `scale` decimal places using HALF_EVEN rounding mode if `scale` >= 0\n",
      "        or at integral part when `scale` < 0.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column to round.\n",
      "        scale : int optional default 0\n",
      "            scale value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            rounded values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(2.5,)], ['a']).select(bround('a', 0).alias('r')).collect()\n",
      "        [Row(r=2.0)]\n",
      "    \n",
      "    btrim(str: 'ColumnOrName', trim: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Remove the leading and trailing `trim` characters from `str`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        trim : :class:`~pyspark.sql.Column` or str\n",
      "            The trim string characters to trim, the default value is a single space\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"SSparkSQLS\", \"SL\", )], ['a', 'b'])\n",
      "        >>> df.select(btrim(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r='parkSQ')]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"    SparkSQL   \",)], ['a'])\n",
      "        >>> df.select(btrim(df.a).alias('r')).collect()\n",
      "        [Row(r='SparkSQL')]\n",
      "    \n",
      "    bucket(numBuckets: Union[pyspark.sql.column.Column, int], col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Partition transform function: A transform for any type that partitions\n",
      "        by a hash of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(  # doctest: +SKIP\n",
      "        ...     bucket(42, \"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date or timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            data partitioned by given columns.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    call_function(funcName: str, *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Call a SQL function.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        funcName : str\n",
      "            function name that follows the SQL identifier syntax (can be quoted, can be qualified)\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s to be used in the function\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            result of executed function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import call_udf, col\n",
      "        >>> from pyspark.sql.types import IntegerType, StringType\n",
      "        >>> df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"c\")],[\"id\", \"name\"])\n",
      "        >>> _ = spark.udf.register(\"intX2\", lambda i: i * 2, IntegerType())\n",
      "        >>> df.select(call_function(\"intX2\", \"id\")).show()\n",
      "        +---------+\n",
      "        |intX2(id)|\n",
      "        +---------+\n",
      "        |        2|\n",
      "        |        4|\n",
      "        |        6|\n",
      "        +---------+\n",
      "        >>> _ = spark.udf.register(\"strX2\", lambda s: s * 2, StringType())\n",
      "        >>> df.select(call_function(\"strX2\", col(\"name\"))).show()\n",
      "        +-----------+\n",
      "        |strX2(name)|\n",
      "        +-----------+\n",
      "        |         aa|\n",
      "        |         bb|\n",
      "        |         cc|\n",
      "        +-----------+\n",
      "        >>> df.select(call_function(\"avg\", col(\"id\"))).show()\n",
      "        +-------+\n",
      "        |avg(id)|\n",
      "        +-------+\n",
      "        |    2.0|\n",
      "        +-------+\n",
      "        >>> _ = spark.sql(\"CREATE FUNCTION custom_avg AS 'test.org.apache.spark.sql.MyDoubleAvg'\")\n",
      "        ... # doctest: +SKIP\n",
      "        >>> df.select(call_function(\"custom_avg\", col(\"id\"))).show()\n",
      "        ... # doctest: +SKIP\n",
      "        +------------------------------------+\n",
      "        |spark_catalog.default.custom_avg(id)|\n",
      "        +------------------------------------+\n",
      "        |                               102.0|\n",
      "        +------------------------------------+\n",
      "        >>> df.select(call_function(\"spark_catalog.default.custom_avg\", col(\"id\"))).show()\n",
      "        ... # doctest: +SKIP\n",
      "        +------------------------------------+\n",
      "        |spark_catalog.default.custom_avg(id)|\n",
      "        +------------------------------------+\n",
      "        |                               102.0|\n",
      "        +------------------------------------+\n",
      "    \n",
      "    call_udf(udfName: str, *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Call an user-defined function.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        udfName : str\n",
      "            name of the user defined function (UDF)\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s to be used in the UDF\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            result of executed udf.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import call_udf, col\n",
      "        >>> from pyspark.sql.types import IntegerType, StringType\n",
      "        >>> df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"c\")],[\"id\", \"name\"])\n",
      "        >>> _ = spark.udf.register(\"intX2\", lambda i: i * 2, IntegerType())\n",
      "        >>> df.select(call_udf(\"intX2\", \"id\")).show()\n",
      "        +---------+\n",
      "        |intX2(id)|\n",
      "        +---------+\n",
      "        |        2|\n",
      "        |        4|\n",
      "        |        6|\n",
      "        +---------+\n",
      "        >>> _ = spark.udf.register(\"strX2\", lambda s: s * 2, StringType())\n",
      "        >>> df.select(call_udf(\"strX2\", col(\"name\"))).show()\n",
      "        +-----------+\n",
      "        |strX2(name)|\n",
      "        +-----------+\n",
      "        |         aa|\n",
      "        |         bb|\n",
      "        |         cc|\n",
      "        +-----------+\n",
      "    \n",
      "    cardinality(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns the length of the array or map stored in the column.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            length of the array/map.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [([1, 2, 3],),([1],),([],)], ['data']\n",
      "        ... ).select(sf.cardinality(\"data\")).show()\n",
      "        +-----------------+\n",
      "        |cardinality(data)|\n",
      "        +-----------------+\n",
      "        |                3|\n",
      "        |                1|\n",
      "        |                0|\n",
      "        +-----------------+\n",
      "    \n",
      "    cbrt(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the cube-root of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(cbrt(lit(27))).show()\n",
      "        +--------+\n",
      "        |CBRT(27)|\n",
      "        +--------+\n",
      "        |     3.0|\n",
      "        +--------+\n",
      "    \n",
      "    ceil(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the ceiling of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(ceil(lit(-0.1))).show()\n",
      "        +----------+\n",
      "        |CEIL(-0.1)|\n",
      "        +----------+\n",
      "        |         0|\n",
      "        +----------+\n",
      "    \n",
      "    ceiling(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the ceiling of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.ceil(sf.lit(-0.1))).show()\n",
      "        +----------+\n",
      "        |CEIL(-0.1)|\n",
      "        +----------+\n",
      "        |         0|\n",
      "        +----------+\n",
      "    \n",
      "    char(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the ASCII character having the binary equivalent to `col`. If col is larger than 256 the\n",
      "        result is equivalent to char(col % 256)\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.char(sf.lit(65))).show()\n",
      "        +--------+\n",
      "        |char(65)|\n",
      "        +--------+\n",
      "        |       A|\n",
      "        +--------+\n",
      "    \n",
      "    char_length(str: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the character length of string data or number of bytes of binary data.\n",
      "        The length of string data includes the trailing spaces.\n",
      "        The length of binary data includes binary zeros.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.char_length(sf.lit(\"SparkSQL\"))).show()\n",
      "        +---------------------+\n",
      "        |char_length(SparkSQL)|\n",
      "        +---------------------+\n",
      "        |                    8|\n",
      "        +---------------------+\n",
      "    \n",
      "    character_length(str: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the character length of string data or number of bytes of binary data.\n",
      "        The length of string data includes the trailing spaces.\n",
      "        The length of binary data includes binary zeros.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.character_length(sf.lit(\"SparkSQL\"))).show()\n",
      "        +--------------------------+\n",
      "        |character_length(SparkSQL)|\n",
      "        +--------------------------+\n",
      "        |                         8|\n",
      "        +--------------------------+\n",
      "    \n",
      "    coalesce(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the first column that is not null.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            list of columns to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value of the first column that is not null.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
      "        >>> cDf.show()\n",
      "        +----+----+\n",
      "        |   a|   b|\n",
      "        +----+----+\n",
      "        |NULL|NULL|\n",
      "        |   1|NULL|\n",
      "        |NULL|   2|\n",
      "        +----+----+\n",
      "        \n",
      "        >>> cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()\n",
      "        +--------------+\n",
      "        |coalesce(a, b)|\n",
      "        +--------------+\n",
      "        |          NULL|\n",
      "        |             1|\n",
      "        |             2|\n",
      "        +--------------+\n",
      "        \n",
      "        >>> cDf.select('*', coalesce(cDf[\"a\"], lit(0.0))).show()\n",
      "        +----+----+----------------+\n",
      "        |   a|   b|coalesce(a, 0.0)|\n",
      "        +----+----+----------------+\n",
      "        |NULL|NULL|             0.0|\n",
      "        |   1|NULL|             1.0|\n",
      "        |NULL|   2|             0.0|\n",
      "        +----+----+----------------+\n",
      "    \n",
      "    col(col: str) -> pyspark.sql.column.Column\n",
      "        Returns a :class:`~pyspark.sql.Column` based on the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : str\n",
      "            the name for the column\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the corresponding column instance.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> col('x')\n",
      "        Column<'x'>\n",
      "        >>> column('x')\n",
      "        Column<'x'>\n",
      "    \n",
      "    collect_list(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns a list of objects with duplicates.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because the order of collected results depends\n",
      "        on the order of the rows which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            list of objects with duplicates.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
      "        >>> df2.agg(collect_list('age')).collect()\n",
      "        [Row(collect_list(age)=[2, 5, 5])]\n",
      "    \n",
      "    collect_set(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns a set of objects with duplicate elements eliminated.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because the order of collected results depends\n",
      "        on the order of the rows which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            list of objects with no duplicates.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
      "        >>> df2.agg(array_sort(collect_set('age')).alias('c')).collect()\n",
      "        [Row(c=[2, 5])]\n",
      "    \n",
      "    column = col(col: str) -> pyspark.sql.column.Column\n",
      "        Returns a :class:`~pyspark.sql.Column` based on the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : str\n",
      "            the name for the column\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the corresponding column instance.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> col('x')\n",
      "        Column<'x'>\n",
      "        >>> column('x')\n",
      "        Column<'x'>\n",
      "    \n",
      "    concat(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Concatenates multiple input columns together into a single column.\n",
      "        The function works with strings, numeric, binary and compatible array columns.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            target column or columns to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            concatenated values. Type of the `Column` depends on input columns' type.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`pyspark.sql.functions.array_join` : to concatenate string columns with delimiter\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "        >>> df = df.select(concat(df.s, df.d).alias('s'))\n",
      "        >>> df.collect()\n",
      "        [Row(s='abcd123')]\n",
      "        >>> df\n",
      "        DataFrame[s: string]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n",
      "        >>> df = df.select(concat(df.a, df.b, df.c).alias(\"arr\"))\n",
      "        >>> df.collect()\n",
      "        [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\n",
      "        >>> df\n",
      "        DataFrame[arr: array<bigint>]\n",
      "    \n",
      "    concat_ws(sep: str, *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Concatenates multiple input string columns together into a single string column,\n",
      "        using the given separator.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sep : str\n",
      "            words separator.\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            list of columns to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string of concatenated words.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "        >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n",
      "        [Row(s='abcd-123')]\n",
      "    \n",
      "    contains(left: 'ColumnOrName', right: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a boolean. The value is True if right is found inside left.\n",
      "        Returns NULL if either input expression is NULL. Otherwise, returns False.\n",
      "        Both left or right must be of STRING or BINARY type.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : :class:`~pyspark.sql.Column` or str\n",
      "            The input column or strings to check, may be NULL.\n",
      "        right : :class:`~pyspark.sql.Column` or str\n",
      "            The input column or strings to find, may be NULL.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Spark SQL\", \"Spark\")], ['a', 'b'])\n",
      "        >>> df.select(contains(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=True)]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"414243\", \"4243\",)], [\"c\", \"d\"])\n",
      "        >>> df = df.select(to_binary(\"c\").alias(\"c\"), to_binary(\"d\").alias(\"d\"))\n",
      "        >>> df.printSchema()\n",
      "        root\n",
      "         |-- c: binary (nullable = true)\n",
      "         |-- d: binary (nullable = true)\n",
      "        >>> df.select(contains(\"c\", \"d\"), contains(\"d\", \"c\")).show()\n",
      "        +--------------+--------------+\n",
      "        |contains(c, d)|contains(d, c)|\n",
      "        +--------------+--------------+\n",
      "        |          true|         false|\n",
      "        +--------------+--------------+\n",
      "    \n",
      "    conv(col: 'ColumnOrName', fromBase: int, toBase: int) -> pyspark.sql.column.Column\n",
      "        Convert a number in a string column from one base to another.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            a column to convert base for.\n",
      "        fromBase: int\n",
      "            from base number.\n",
      "        toBase: int\n",
      "            to base number.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            logariphm of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"010101\",)], ['n'])\n",
      "        >>> df.select(conv(df.n, 2, 16).alias('hex')).collect()\n",
      "        [Row(hex='15')]\n",
      "    \n",
      "    convert_timezone(sourceTz: Optional[pyspark.sql.column.Column], targetTz: pyspark.sql.column.Column, sourceTs: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Converts the timestamp without time zone `sourceTs`\n",
      "        from the `sourceTz` time zone to `targetTz`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sourceTz : :class:`~pyspark.sql.Column`\n",
      "            the time zone for the input timestamp. If it is missed,\n",
      "            the current session time zone is used as the source time zone.\n",
      "        targetTz : :class:`~pyspark.sql.Column`\n",
      "            the time zone to which the input timestamp should be converted.\n",
      "        sourceTs : :class:`~pyspark.sql.Column`\n",
      "            a timestamp without time zone.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            timestamp for converted time zone.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(convert_timezone(   # doctest: +SKIP\n",
      "        ...     None, lit('Asia/Hong_Kong'), 'dt').alias('ts')\n",
      "        ... ).show()\n",
      "        +-------------------+\n",
      "        |                 ts|\n",
      "        +-------------------+\n",
      "        |2015-04-08 00:00:00|\n",
      "        +-------------------+\n",
      "        >>> df.select(convert_timezone(\n",
      "        ...     lit('America/Los_Angeles'), lit('Asia/Hong_Kong'), 'dt').alias('ts')\n",
      "        ... ).show()\n",
      "        +-------------------+\n",
      "        |                 ts|\n",
      "        +-------------------+\n",
      "        |2015-04-08 15:00:00|\n",
      "        +-------------------+\n",
      "    \n",
      "    corr(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new :class:`~pyspark.sql.Column` for the Pearson Correlation Coefficient for\n",
      "        ``col1`` and ``col2``.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            first column to calculate correlation.\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            second column to calculate correlation.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            Pearson Correlation Coefficient of these two column values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = range(20)\n",
      "        >>> b = [2 * x for x in range(20)]\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(corr(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=1.0)]\n",
      "    \n",
      "    cos(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes cosine of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            cosine of the angle, as if computed by `java.lang.Math.cos()`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(cos(lit(math.pi))).first()\n",
      "        Row(COS(3.14159...)=-1.0)\n",
      "    \n",
      "    cosh(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes hyperbolic cosine of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            hyperbolic angle\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(cosh(lit(1))).first()\n",
      "        Row(COSH(1)=1.54308...)\n",
      "    \n",
      "    cot(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes cotangent of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            cotangent of the angle.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(cot(lit(math.radians(45)))).first()\n",
      "        Row(COT(0.78539...)=1.00000...)\n",
      "    \n",
      "    count(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the number of items in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Count by all columns (start), and by a column that does not count ``None``.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(None,), (\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n",
      "        >>> df.select(count(expr(\"*\")), count(df.alphabets)).show()\n",
      "        +--------+----------------+\n",
      "        |count(1)|count(alphabets)|\n",
      "        +--------+----------------+\n",
      "        |       4|               3|\n",
      "        +--------+----------------+\n",
      "    \n",
      "    countDistinct(col: 'ColumnOrName', *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new :class:`~pyspark.sql.Column` for distinct count of ``col`` or ``cols``.\n",
      "        \n",
      "        An alias of :func:`count_distinct`, and it is encouraged to use :func:`count_distinct`\n",
      "        directly.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "    \n",
      "    count_distinct(col: 'ColumnOrName', *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new :class:`Column` for distinct count of ``col`` or ``cols``.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            first column to compute on.\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            other columns to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            distinct values of these two column values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import types\n",
      "        >>> df1 = spark.createDataFrame([1, 1, 3], types.IntegerType())\n",
      "        >>> df2 = spark.createDataFrame([1, 2], types.IntegerType())\n",
      "        >>> df1.join(df2).show()\n",
      "        +-----+-----+\n",
      "        |value|value|\n",
      "        +-----+-----+\n",
      "        |    1|    1|\n",
      "        |    1|    2|\n",
      "        |    1|    1|\n",
      "        |    1|    2|\n",
      "        |    3|    1|\n",
      "        |    3|    2|\n",
      "        +-----+-----+\n",
      "        >>> df1.join(df2).select(count_distinct(df1.value, df2.value)).show()\n",
      "        +----------------------------+\n",
      "        |count(DISTINCT value, value)|\n",
      "        +----------------------------+\n",
      "        |                           4|\n",
      "        +----------------------------+\n",
      "    \n",
      "    count_if(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of `TRUE` values for the `col`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the number of `TRUE` values for the `col`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"a\", 1),\n",
      "        ...                             (\"a\", 2),\n",
      "        ...                             (\"a\", 3),\n",
      "        ...                             (\"b\", 8),\n",
      "        ...                             (\"b\", 2)], [\"c1\", \"c2\"])\n",
      "        >>> df.select(count_if(col('c2') % 2 == 0)).show()\n",
      "        +------------------------+\n",
      "        |count_if(((c2 % 2) = 0))|\n",
      "        +------------------------+\n",
      "        |                       3|\n",
      "        +------------------------+\n",
      "    \n",
      "    count_min_sketch(col: 'ColumnOrName', eps: 'ColumnOrName', confidence: 'ColumnOrName', seed: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a count-min sketch of a column with the given esp, confidence and seed.\n",
      "        The result is an array of bytes, which can be deserialized to a `CountMinSketch` before usage.\n",
      "        Count-min sketch is a probabilistic data structure used for cardinality estimation\n",
      "        using sub-linear space.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        eps : :class:`~pyspark.sql.Column` or str\n",
      "            relative error, must be positive\n",
      "        confidence : :class:`~pyspark.sql.Column` or str\n",
      "            confidence, must be positive and less than 1.0\n",
      "        seed : :class:`~pyspark.sql.Column` or str\n",
      "            random seed\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            count-min sketch of the column\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1], [2], [1]], ['data'])\n",
      "        >>> df = df.agg(count_min_sketch(df.data, lit(0.5), lit(0.5), lit(1)).alias('sketch'))\n",
      "        >>> df.select(hex(df.sketch).alias('r')).collect()\n",
      "        [Row(r='0000000100000000000000030000000100000004000000005D8D6AB90000000000000000000000000000000200000000000000010000000000000000')]\n",
      "    \n",
      "    covar_pop(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new :class:`~pyspark.sql.Column` for the population covariance of ``col1`` and\n",
      "        ``col2``.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            first column to calculate covariance.\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            second column to calculate covariance.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            covariance of these two column values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = [1] * 10\n",
      "        >>> b = [1] * 10\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(covar_pop(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=0.0)]\n",
      "    \n",
      "    covar_samp(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new :class:`~pyspark.sql.Column` for the sample covariance of ``col1`` and\n",
      "        ``col2``.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            first column to calculate covariance.\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            second column to calculate covariance.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            sample covariance of these two column values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = [1] * 10\n",
      "        >>> b = [1] * 10\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(covar_samp(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=0.0)]\n",
      "    \n",
      "    crc32(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calculates the cyclic redundancy check value  (CRC32) of a binary column and\n",
      "        returns the value as a bigint.\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(crc32('a').alias('crc32')).collect()\n",
      "        [Row(crc32=2743272264)]\n",
      "    \n",
      "    create_map(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')], Tuple[ForwardRef('ColumnOrName_'), ...]]) -> pyspark.sql.column.Column\n",
      "        Creates a new map column.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s that are\n",
      "            grouped as key-value pairs, e.g. (key1, value1, key2, value2, ...).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n",
      "        >>> df.select(create_map('name', 'age').alias(\"map\")).collect()\n",
      "        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n",
      "        >>> df.select(create_map([df.name, df.age]).alias(\"map\")).collect()\n",
      "        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n",
      "    \n",
      "    csc(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes cosecant of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            cosecant of the angle.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(csc(lit(math.radians(90)))).first()\n",
      "        Row(CSC(1.57079...)=1.0)\n",
      "    \n",
      "    cume_dist() -> pyspark.sql.column.Column\n",
      "        Window function: returns the cumulative distribution of values within a window partition,\n",
      "        i.e. the fraction of rows that are below the current row.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for calculating cumulative distribution.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window, types\n",
      "        >>> df = spark.createDataFrame([1, 2, 3, 3, 4], types.IntegerType())\n",
      "        >>> w = Window.orderBy(\"value\")\n",
      "        >>> df.withColumn(\"cd\", cume_dist().over(w)).show()\n",
      "        +-----+---+\n",
      "        |value| cd|\n",
      "        +-----+---+\n",
      "        |    1|0.2|\n",
      "        |    2|0.4|\n",
      "        |    3|0.8|\n",
      "        |    3|0.8|\n",
      "        |    4|1.0|\n",
      "        +-----+---+\n",
      "    \n",
      "    curdate() -> pyspark.sql.column.Column\n",
      "        Returns the current date at the start of query evaluation as a :class:`DateType` column.\n",
      "        All calls of current_date within the same query return the same value.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            current date.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.curdate()).show() # doctest: +SKIP\n",
      "        +--------------+\n",
      "        |current_date()|\n",
      "        +--------------+\n",
      "        |    2022-08-26|\n",
      "        +--------------+\n",
      "    \n",
      "    current_catalog() -> pyspark.sql.column.Column\n",
      "        Returns the current catalog.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.range(1).select(current_catalog()).show()\n",
      "        +-----------------+\n",
      "        |current_catalog()|\n",
      "        +-----------------+\n",
      "        |    spark_catalog|\n",
      "        +-----------------+\n",
      "    \n",
      "    current_database() -> pyspark.sql.column.Column\n",
      "        Returns the current database.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.range(1).select(current_database()).show()\n",
      "        +------------------+\n",
      "        |current_database()|\n",
      "        +------------------+\n",
      "        |           default|\n",
      "        +------------------+\n",
      "    \n",
      "    current_date() -> pyspark.sql.column.Column\n",
      "        Returns the current date at the start of query evaluation as a :class:`DateType` column.\n",
      "        All calls of current_date within the same query return the same value.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            current date.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(current_date()).show() # doctest: +SKIP\n",
      "        +--------------+\n",
      "        |current_date()|\n",
      "        +--------------+\n",
      "        |    2022-08-26|\n",
      "        +--------------+\n",
      "    \n",
      "    current_schema() -> pyspark.sql.column.Column\n",
      "        Returns the current database.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.current_schema()).show()\n",
      "        +------------------+\n",
      "        |current_database()|\n",
      "        +------------------+\n",
      "        |           default|\n",
      "        +------------------+\n",
      "    \n",
      "    current_timestamp() -> pyspark.sql.column.Column\n",
      "        Returns the current timestamp at the start of query evaluation as a :class:`TimestampType`\n",
      "        column. All calls of current_timestamp within the same query return the same value.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            current date and time.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(current_timestamp()).show(truncate=False) # doctest: +SKIP\n",
      "        +-----------------------+\n",
      "        |current_timestamp()    |\n",
      "        +-----------------------+\n",
      "        |2022-08-26 21:23:22.716|\n",
      "        +-----------------------+\n",
      "    \n",
      "    current_timezone() -> pyspark.sql.column.Column\n",
      "        Returns the current session local timezone.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            current session local timezone.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> spark.range(1).select(current_timezone()).show()\n",
      "        +-------------------+\n",
      "        | current_timezone()|\n",
      "        +-------------------+\n",
      "        |America/Los_Angeles|\n",
      "        +-------------------+\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    current_user() -> pyspark.sql.column.Column\n",
      "        Returns the current database.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.range(1).select(current_user()).show() # doctest: +SKIP\n",
      "        +--------------+\n",
      "        |current_user()|\n",
      "        +--------------+\n",
      "        | ruifeng.zheng|\n",
      "        +--------------+\n",
      "    \n",
      "    date_add(start: 'ColumnOrName', days: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Returns the date that is `days` days after `start`. If `days` is a negative value\n",
      "        then these amount of days will be deducted from `start`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            date column to work on.\n",
      "        days : :class:`~pyspark.sql.Column` or str or int\n",
      "            how many days after the given date to calculate.\n",
      "            Accepts negative value as well to calculate backwards in time.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a date after/before given number of days.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08', 2,)], ['dt', 'add'])\n",
      "        >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n",
      "        [Row(next_date=datetime.date(2015, 4, 9))]\n",
      "        >>> df.select(date_add(df.dt, df.add.cast('integer')).alias('next_date')).collect()\n",
      "        [Row(next_date=datetime.date(2015, 4, 10))]\n",
      "        >>> df.select(date_add('dt', -1).alias('prev_date')).collect()\n",
      "        [Row(prev_date=datetime.date(2015, 4, 7))]\n",
      "    \n",
      "    date_diff(end: 'ColumnOrName', start: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of days from `start` to `end`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        end : :class:`~pyspark.sql.Column` or str\n",
      "            to date column to work on.\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            from date column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            difference in days between two dates.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
      "        >>> df.select(date_diff(df.d2, df.d1).alias('diff')).collect()\n",
      "        [Row(diff=32)]\n",
      "    \n",
      "    date_format(date: 'ColumnOrName', format: str) -> pyspark.sql.column.Column\n",
      "        Converts a date/timestamp/string to a value of string in the format specified by the date\n",
      "        format given by the second argument.\n",
      "        \n",
      "        A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
      "        pattern letters of `datetime pattern`_. can be used.\n",
      "        \n",
      "        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Whenever possible, use specialized functions like `year`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        date : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to format.\n",
      "        format: str\n",
      "            format to use to represent datetime values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string value representing formatted datetime.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n",
      "        [Row(date='04/08/2015')]\n",
      "    \n",
      "    date_from_unix_date(days: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Create date from the number of `days` since 1970-01-01.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        days : :class:`~pyspark.sql.Column` or str\n",
      "            the target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the date from the number of days since 1970-01-01.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(date_from_unix_date(lit(1))).show()\n",
      "        +----------------------+\n",
      "        |date_from_unix_date(1)|\n",
      "        +----------------------+\n",
      "        |            1970-01-02|\n",
      "        +----------------------+\n",
      "    \n",
      "    date_part(field: 'ColumnOrName', source: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extracts a part of the date/timestamp or interval source.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        field : :class:`~pyspark.sql.Column` or str\n",
      "            selects which part of the source should be extracted, and supported string values\n",
      "            are as same as the fields of the equivalent function `extract`.\n",
      "        source : :class:`~pyspark.sql.Column` or str\n",
      "            a date/timestamp or interval column from where `field` should be extracted.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a part of the date/timestamp or interval source.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame([(datetime.datetime(2015, 4, 8, 13, 8, 15),)], ['ts'])\n",
      "        >>> df.select(\n",
      "        ...     date_part(lit('YEAR'), 'ts').alias('year'),\n",
      "        ...     date_part(lit('month'), 'ts').alias('month'),\n",
      "        ...     date_part(lit('WEEK'), 'ts').alias('week'),\n",
      "        ...     date_part(lit('D'), 'ts').alias('day'),\n",
      "        ...     date_part(lit('M'), 'ts').alias('minute'),\n",
      "        ...     date_part(lit('S'), 'ts').alias('second')\n",
      "        ... ).collect()\n",
      "        [Row(year=2015, month=4, week=15, day=8, minute=8, second=Decimal('15.000000'))]\n",
      "    \n",
      "    date_sub(start: 'ColumnOrName', days: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Returns the date that is `days` days before `start`. If `days` is a negative value\n",
      "        then these amount of days will be added to `start`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            date column to work on.\n",
      "        days : :class:`~pyspark.sql.Column` or str or int\n",
      "            how many days before the given date to calculate.\n",
      "            Accepts negative value as well to calculate forward in time.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a date before/after given number of days.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08', 2,)], ['dt', 'sub'])\n",
      "        >>> df.select(date_sub(df.dt, 1).alias('prev_date')).collect()\n",
      "        [Row(prev_date=datetime.date(2015, 4, 7))]\n",
      "        >>> df.select(date_sub(df.dt, df.sub.cast('integer')).alias('prev_date')).collect()\n",
      "        [Row(prev_date=datetime.date(2015, 4, 6))]\n",
      "        >>> df.select(date_sub('dt', -1).alias('next_date')).collect()\n",
      "        [Row(next_date=datetime.date(2015, 4, 9))]\n",
      "    \n",
      "    date_trunc(format: str, timestamp: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns timestamp truncated to the unit specified by the format.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        format : str\n",
      "            'year', 'yyyy', 'yy' to truncate by year,\n",
      "            'month', 'mon', 'mm' to truncate by month,\n",
      "            'day', 'dd' to truncate by day,\n",
      "            Other options are:\n",
      "            'microsecond', 'millisecond', 'second', 'minute', 'hour', 'week', 'quarter'\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to truncate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            truncated timestamp.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n",
      "        >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n",
      "        [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n",
      "        >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n",
      "        [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\n",
      "    \n",
      "    dateadd(start: 'ColumnOrName', days: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Returns the date that is `days` days after `start`. If `days` is a negative value\n",
      "        then these amount of days will be deducted from `start`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            date column to work on.\n",
      "        days : :class:`~pyspark.sql.Column` or str or int\n",
      "            how many days after the given date to calculate.\n",
      "            Accepts negative value as well to calculate backwards in time.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a date after/before given number of days.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [('2015-04-08', 2,)], ['dt', 'add']\n",
      "        ... ).select(sf.dateadd(\"dt\", 1)).show()\n",
      "        +---------------+\n",
      "        |date_add(dt, 1)|\n",
      "        +---------------+\n",
      "        |     2015-04-09|\n",
      "        +---------------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [('2015-04-08', 2,)], ['dt', 'add']\n",
      "        ... ).select(sf.dateadd(\"dt\", sf.lit(2))).show()\n",
      "        +---------------+\n",
      "        |date_add(dt, 2)|\n",
      "        +---------------+\n",
      "        |     2015-04-10|\n",
      "        +---------------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [('2015-04-08', 2,)], ['dt', 'add']\n",
      "        ... ).select(sf.dateadd(\"dt\", -1)).show()\n",
      "        +----------------+\n",
      "        |date_add(dt, -1)|\n",
      "        +----------------+\n",
      "        |      2015-04-07|\n",
      "        +----------------+\n",
      "    \n",
      "    datediff(end: 'ColumnOrName', start: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of days from `start` to `end`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        end : :class:`~pyspark.sql.Column` or str\n",
      "            to date column to work on.\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            from date column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            difference in days between two dates.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
      "        >>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()\n",
      "        [Row(diff=32)]\n",
      "    \n",
      "    datepart(field: 'ColumnOrName', source: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extracts a part of the date/timestamp or interval source.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        field : :class:`~pyspark.sql.Column` or str\n",
      "            selects which part of the source should be extracted, and supported string values\n",
      "            are as same as the fields of the equivalent function `extract`.\n",
      "        source : :class:`~pyspark.sql.Column` or str\n",
      "            a date/timestamp or interval column from where `field` should be extracted.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a part of the date/timestamp or interval source.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame([(datetime.datetime(2015, 4, 8, 13, 8, 15),)], ['ts'])\n",
      "        >>> df.select(\n",
      "        ...     datepart(lit('YEAR'), 'ts').alias('year'),\n",
      "        ...     datepart(lit('month'), 'ts').alias('month'),\n",
      "        ...     datepart(lit('WEEK'), 'ts').alias('week'),\n",
      "        ...     datepart(lit('D'), 'ts').alias('day'),\n",
      "        ...     datepart(lit('M'), 'ts').alias('minute'),\n",
      "        ...     datepart(lit('S'), 'ts').alias('second')\n",
      "        ... ).collect()\n",
      "        [Row(year=2015, month=4, week=15, day=8, minute=8, second=Decimal('15.000000'))]\n",
      "    \n",
      "    day(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the day of the month of a given date/timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            day of the month for given date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(day('dt').alias('day')).collect()\n",
      "        [Row(day=8)]\n",
      "    \n",
      "    dayofmonth(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the day of the month of a given date/timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            day of the month for given date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofmonth('dt').alias('day')).collect()\n",
      "        [Row(day=8)]\n",
      "    \n",
      "    dayofweek(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the day of the week of a given date/timestamp as integer.\n",
      "        Ranges from 1 for a Sunday through to 7 for a Saturday\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            day of the week for given date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofweek('dt').alias('day')).collect()\n",
      "        [Row(day=4)]\n",
      "    \n",
      "    dayofyear(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the day of the year of a given date/timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            day of the year for given date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofyear('dt').alias('day')).collect()\n",
      "        [Row(day=98)]\n",
      "    \n",
      "    days(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Partition transform function: A transform for timestamps and dates\n",
      "        to partition data into days.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date or timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            data partitioned by days.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(  # doctest: +SKIP\n",
      "        ...     days(\"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    decode(col: 'ColumnOrName', charset: str) -> pyspark.sql.column.Column\n",
      "        Computes the first argument into a string from a binary using the provided character set\n",
      "        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        charset : str\n",
      "            charset to use to decode to.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['a'])\n",
      "        >>> df.select(decode(\"a\", \"UTF-8\")).show()\n",
      "        +----------------+\n",
      "        |decode(a, UTF-8)|\n",
      "        +----------------+\n",
      "        |            abcd|\n",
      "        +----------------+\n",
      "    \n",
      "    degrees(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Converts an angle measured in radians to an approximately equivalent angle\n",
      "        measured in degrees.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            angle in degrees, as if computed by `java.lang.Math.toDegrees()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(degrees(lit(math.pi))).first()\n",
      "        Row(DEGREES(3.14159...)=180.0)\n",
      "    \n",
      "    dense_rank() -> pyspark.sql.column.Column\n",
      "        Window function: returns the rank of rows within a window partition, without any gaps.\n",
      "        \n",
      "        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
      "        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
      "        and had three people tie for second place, you would say that all three were in second\n",
      "        place and that the next person came in third. Rank would give me sequential numbers, making\n",
      "        the person that came in third place (after the ties) would register as coming in fifth.\n",
      "        \n",
      "        This is equivalent to the DENSE_RANK function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for calculating ranks.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window, types\n",
      "        >>> df = spark.createDataFrame([1, 1, 2, 3, 3, 4], types.IntegerType())\n",
      "        >>> w = Window.orderBy(\"value\")\n",
      "        >>> df.withColumn(\"drank\", dense_rank().over(w)).show()\n",
      "        +-----+-----+\n",
      "        |value|drank|\n",
      "        +-----+-----+\n",
      "        |    1|    1|\n",
      "        |    1|    1|\n",
      "        |    2|    2|\n",
      "        |    3|    3|\n",
      "        |    3|    3|\n",
      "        |    4|    4|\n",
      "        +-----+-----+\n",
      "    \n",
      "    desc(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sort expression based on the descending order of the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to sort by in the descending order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column specifying the order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Sort by the column 'id' in the descending order.\n",
      "        \n",
      "        >>> spark.range(5).orderBy(desc(\"id\")).show()\n",
      "        +---+\n",
      "        | id|\n",
      "        +---+\n",
      "        |  4|\n",
      "        |  3|\n",
      "        |  2|\n",
      "        |  1|\n",
      "        |  0|\n",
      "        +---+\n",
      "    \n",
      "    desc_nulls_first(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sort expression based on the descending order of the given\n",
      "        column name, and null values appear before non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to sort by in the descending order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column specifying the order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(0, None),\n",
      "        ...                              (1, \"Bob\"),\n",
      "        ...                              (2, \"Alice\")], [\"age\", \"name\"])\n",
      "        >>> df1.sort(desc_nulls_first(df1.name)).show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  0| NULL|\n",
      "        |  1|  Bob|\n",
      "        |  2|Alice|\n",
      "        +---+-----+\n",
      "    \n",
      "    desc_nulls_last(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sort expression based on the descending order of the given\n",
      "        column name, and null values appear after non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to sort by in the descending order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column specifying the order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(0, None),\n",
      "        ...                              (1, \"Bob\"),\n",
      "        ...                              (2, \"Alice\")], [\"age\", \"name\"])\n",
      "        >>> df1.sort(desc_nulls_last(df1.name)).show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  1|  Bob|\n",
      "        |  2|Alice|\n",
      "        |  0| NULL|\n",
      "        +---+-----+\n",
      "    \n",
      "    e() -> pyspark.sql.column.Column\n",
      "        Returns Euler's number.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.range(1).select(e()).show()\n",
      "        +-----------------+\n",
      "        |              E()|\n",
      "        +-----------------+\n",
      "        |2.718281828459045|\n",
      "        +-----------------+\n",
      "    \n",
      "    element_at(col: 'ColumnOrName', extraction: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: Returns element of array at given index in `extraction` if col is array.\n",
      "        Returns value for the given key in `extraction` if col is map. If position is negative\n",
      "        then location of the element will start from end, if number is outside the\n",
      "        array boundaries then None will be returned.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array or map\n",
      "        extraction :\n",
      "            index to check for in array or key to check for in map\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value at given position.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`get`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], ['data'])\n",
      "        >>> df.select(element_at(df.data, 1)).collect()\n",
      "        [Row(element_at(data, 1)='a')]\n",
      "        >>> df.select(element_at(df.data, -1)).collect()\n",
      "        [Row(element_at(data, -1)='c')]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},)], ['data'])\n",
      "        >>> df.select(element_at(df.data, lit(\"a\"))).collect()\n",
      "        [Row(element_at(data, a)=1.0)]\n",
      "    \n",
      "    elt(*inputs: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the `n`-th input, e.g., returns `input2` when `n` is 2.\n",
      "        The function returns NULL if the index exceeds the length of the array\n",
      "        and `spark.sql.ansi.enabled` is set to false. If `spark.sql.ansi.enabled` is set to true,\n",
      "        it throws ArrayIndexOutOfBoundsException for invalid indices.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        inputs : :class:`~pyspark.sql.Column` or str\n",
      "            Input columns or strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, \"scala\", \"java\")], ['a', 'b', 'c'])\n",
      "        >>> df.select(elt(df.a, df.b, df.c).alias('r')).collect()\n",
      "        [Row(r='scala')]\n",
      "    \n",
      "    encode(col: 'ColumnOrName', charset: str) -> pyspark.sql.column.Column\n",
      "        Computes the first argument into a binary from a string using the provided character set\n",
      "        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        charset : str\n",
      "            charset to use to encode.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['c'])\n",
      "        >>> df.select(encode(\"c\", \"UTF-8\")).show()\n",
      "        +----------------+\n",
      "        |encode(c, UTF-8)|\n",
      "        +----------------+\n",
      "        |   [61 62 63 64]|\n",
      "        +----------------+\n",
      "    \n",
      "    endswith(str: 'ColumnOrName', suffix: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a boolean. The value is True if str ends with suffix.\n",
      "        Returns NULL if either input expression is NULL. Otherwise, returns False.\n",
      "        Both str or suffix must be of STRING or BINARY type.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string.\n",
      "        suffix : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string, the suffix.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Spark SQL\", \"Spark\",)], [\"a\", \"b\"])\n",
      "        >>> df.select(endswith(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=False)]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"414243\", \"4243\",)], [\"e\", \"f\"])\n",
      "        >>> df = df.select(to_binary(\"e\").alias(\"e\"), to_binary(\"f\").alias(\"f\"))\n",
      "        >>> df.printSchema()\n",
      "        root\n",
      "         |-- e: binary (nullable = true)\n",
      "         |-- f: binary (nullable = true)\n",
      "        >>> df.select(endswith(\"e\", \"f\"), endswith(\"f\", \"e\")).show()\n",
      "        +--------------+--------------+\n",
      "        |endswith(e, f)|endswith(f, e)|\n",
      "        +--------------+--------------+\n",
      "        |          true|         false|\n",
      "        +--------------+--------------+\n",
      "    \n",
      "    equal_null(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns same result as the EQUAL(=) operator for non-null operands,\n",
      "        but returns true if both are null, false if one of the them is null.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None, None,), (1, 9,)], [\"a\", \"b\"])\n",
      "        >>> df.select(equal_null(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=True), Row(r=False)]\n",
      "    \n",
      "    every(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns true if all values of `col` are true.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to check if all values are true.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            true if all values of `col` are true, false otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [[True], [True], [True]], [\"flag\"]\n",
      "        ... ).select(sf.every(\"flag\")).show()\n",
      "        +-----------+\n",
      "        |every(flag)|\n",
      "        +-----------+\n",
      "        |       true|\n",
      "        +-----------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [[True], [False], [True]], [\"flag\"]\n",
      "        ... ).select(sf.every(\"flag\")).show()\n",
      "        +-----------+\n",
      "        |every(flag)|\n",
      "        +-----------+\n",
      "        |      false|\n",
      "        +-----------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [[False], [False], [False]], [\"flag\"]\n",
      "        ... ).select(sf.every(\"flag\")).show()\n",
      "        +-----------+\n",
      "        |every(flag)|\n",
      "        +-----------+\n",
      "        |      false|\n",
      "        +-----------+\n",
      "    \n",
      "    exists(col: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Returns whether a predicate holds for one or more elements in the array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            ``(x: Column) -> Column: ...``  returning the Boolean expression.\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            True if \"any\" element of an array evaluates to True when passed as an argument to\n",
      "            given function and False otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [1, 2, 3, 4]), (2, [3, -1, 0])],(\"key\", \"values\"))\n",
      "        >>> df.select(exists(\"values\", lambda x: x < 0).alias(\"any_negative\")).show()\n",
      "        +------------+\n",
      "        |any_negative|\n",
      "        +------------+\n",
      "        |       false|\n",
      "        |        true|\n",
      "        +------------+\n",
      "    \n",
      "    exp(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the exponential of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to calculate exponential for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            exponential of the given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(exp(lit(0))).show()\n",
      "        +------+\n",
      "        |EXP(0)|\n",
      "        +------+\n",
      "        |   1.0|\n",
      "        +------+\n",
      "    \n",
      "    explode(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new row for each element in the given array or map.\n",
      "        Uses the default column name `col` for elements in the array and\n",
      "        `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            one row per array item or map key value.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`pyspark.functions.posexplode`\n",
      "        :meth:`pyspark.functions.explode_outer`\n",
      "        :meth:`pyspark.functions.posexplode_outer`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "        >>> df.select(explode(df.intlist).alias(\"anInt\")).collect()\n",
      "        [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\n",
      "        \n",
      "        >>> df.select(explode(df.mapfield).alias(\"key\", \"value\")).show()\n",
      "        +---+-----+\n",
      "        |key|value|\n",
      "        +---+-----+\n",
      "        |  a|    b|\n",
      "        +---+-----+\n",
      "    \n",
      "    explode_outer(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new row for each element in the given array or map.\n",
      "        Unlike explode, if the array/map is null or empty then null is produced.\n",
      "        Uses the default column name `col` for elements in the array and\n",
      "        `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            one row per array item or map key value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
      "        ...     (\"id\", \"an_array\", \"a_map\")\n",
      "        ... )\n",
      "        >>> df.select(\"id\", \"an_array\", explode_outer(\"a_map\")).show()\n",
      "        +---+----------+----+-----+\n",
      "        | id|  an_array| key|value|\n",
      "        +---+----------+----+-----+\n",
      "        |  1|[foo, bar]|   x|  1.0|\n",
      "        |  2|        []|NULL| NULL|\n",
      "        |  3|      NULL|NULL| NULL|\n",
      "        +---+----------+----+-----+\n",
      "        \n",
      "        >>> df.select(\"id\", \"a_map\", explode_outer(\"an_array\")).show()\n",
      "        +---+----------+----+\n",
      "        | id|     a_map| col|\n",
      "        +---+----------+----+\n",
      "        |  1|{x -> 1.0}| foo|\n",
      "        |  1|{x -> 1.0}| bar|\n",
      "        |  2|        {}|NULL|\n",
      "        |  3|      NULL|NULL|\n",
      "        +---+----------+----+\n",
      "    \n",
      "    expm1(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the exponential of the given value minus one.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to calculate exponential for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            exponential less one.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(expm1(lit(1))).first()\n",
      "        Row(EXPM1(1)=1.71828...)\n",
      "    \n",
      "    expr(str: str) -> pyspark.sql.column.Column\n",
      "        Parses the expression string into the column that it represents\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : str\n",
      "            expression defined in string.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column representing the expression.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[\"Alice\"], [\"Bob\"]], [\"name\"])\n",
      "        >>> df.select(\"name\", expr(\"length(name)\")).show()\n",
      "        +-----+------------+\n",
      "        | name|length(name)|\n",
      "        +-----+------------+\n",
      "        |Alice|           5|\n",
      "        |  Bob|           3|\n",
      "        +-----+------------+\n",
      "    \n",
      "    extract(field: 'ColumnOrName', source: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extracts a part of the date/timestamp or interval source.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        field : :class:`~pyspark.sql.Column` or str\n",
      "            selects which part of the source should be extracted.\n",
      "        source : :class:`~pyspark.sql.Column` or str\n",
      "            a date/timestamp or interval column from where `field` should be extracted.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a part of the date/timestamp or interval source.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame([(datetime.datetime(2015, 4, 8, 13, 8, 15),)], ['ts'])\n",
      "        >>> df.select(\n",
      "        ...     extract(lit('YEAR'), 'ts').alias('year'),\n",
      "        ...     extract(lit('month'), 'ts').alias('month'),\n",
      "        ...     extract(lit('WEEK'), 'ts').alias('week'),\n",
      "        ...     extract(lit('D'), 'ts').alias('day'),\n",
      "        ...     extract(lit('M'), 'ts').alias('minute'),\n",
      "        ...     extract(lit('S'), 'ts').alias('second')\n",
      "        ... ).collect()\n",
      "        [Row(year=2015, month=4, week=15, day=8, minute=8, second=Decimal('15.000000'))]\n",
      "    \n",
      "    factorial(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the factorial of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            a column to calculate factorial for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            factorial of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(5,)], ['n'])\n",
      "        >>> df.select(factorial(df.n).alias('f')).collect()\n",
      "        [Row(f=120)]\n",
      "    \n",
      "    filter(col: 'ColumnOrName', f: Union[Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column], Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]]) -> pyspark.sql.column.Column\n",
      "        Returns an array of elements for which a predicate holds in a given array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            A function that returns the Boolean expression.\n",
      "            Can take one of the following forms:\n",
      "        \n",
      "            - Unary ``(x: Column) -> Column: ...``\n",
      "            - Binary ``(x: Column, i: Column) -> Column...``, where the second argument is\n",
      "                a 0-based index of the element.\n",
      "        \n",
      "            and can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            filtered array of elements where given function evaluated to True\n",
      "            when passed as an argument.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"2018-09-20\",  \"2019-02-03\", \"2019-07-01\", \"2020-06-01\"])],\n",
      "        ...     (\"key\", \"values\")\n",
      "        ... )\n",
      "        >>> def after_second_quarter(x):\n",
      "        ...     return month(to_date(x)) > 6\n",
      "        ...\n",
      "        >>> df.select(\n",
      "        ...     filter(\"values\", after_second_quarter).alias(\"after_second_quarter\")\n",
      "        ... ).show(truncate=False)\n",
      "        +------------------------+\n",
      "        |after_second_quarter    |\n",
      "        +------------------------+\n",
      "        |[2018-09-20, 2019-07-01]|\n",
      "        +------------------------+\n",
      "    \n",
      "    find_in_set(str: 'ColumnOrName', str_array: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the index (1-based) of the given string (`str`) in the comma-delimited\n",
      "        list (`strArray`). Returns 0, if the string was not found or if the given string (`str`)\n",
      "        contains a comma.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            The given string to be found.\n",
      "        str_array : :class:`~pyspark.sql.Column` or str\n",
      "            The comma-delimited list.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"ab\", \"abc,b,ab,c,def\")], ['a', 'b'])\n",
      "        >>> df.select(find_in_set(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=3)]\n",
      "    \n",
      "    first(col: 'ColumnOrName', ignorenulls: bool = False) -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the first value in a group.\n",
      "        \n",
      "        The function by default returns the first values it sees. It will return the first non-null\n",
      "        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because its results depends on the order of the\n",
      "        rows which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to fetch first value for.\n",
      "        ignorenulls : :class:`~pyspark.sql.Column` or str\n",
      "            if first value is null then look for first non-null value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            first value of the group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5), (\"Alice\", None)], (\"name\", \"age\"))\n",
      "        >>> df = df.orderBy(df.age)\n",
      "        >>> df.groupby(\"name\").agg(first(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+----------+\n",
      "        | name|first(age)|\n",
      "        +-----+----------+\n",
      "        |Alice|      NULL|\n",
      "        |  Bob|         5|\n",
      "        +-----+----------+\n",
      "        \n",
      "        Now, to ignore any nulls we needs to set ``ignorenulls`` to `True`\n",
      "        \n",
      "        >>> df.groupby(\"name\").agg(first(\"age\", ignorenulls=True)).orderBy(\"name\").show()\n",
      "        +-----+----------+\n",
      "        | name|first(age)|\n",
      "        +-----+----------+\n",
      "        |Alice|         2|\n",
      "        |  Bob|         5|\n",
      "        +-----+----------+\n",
      "    \n",
      "    first_value(col: 'ColumnOrName', ignoreNulls: Union[bool, pyspark.sql.column.Column, NoneType] = None) -> pyspark.sql.column.Column\n",
      "        Returns the first value of `col` for a group of rows. It will return the first non-null\n",
      "        value it sees when `ignoreNulls` is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        ignorenulls : :class:`~pyspark.sql.Column` or bool\n",
      "            if first value is null then look for first non-null value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            some value of `col` for a group of rows.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(None, 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (\"b\", 2)], [\"a\", \"b\"]\n",
      "        ... ).select(sf.first_value('a'), sf.first_value('b')).show()\n",
      "        +--------------+--------------+\n",
      "        |first_value(a)|first_value(b)|\n",
      "        +--------------+--------------+\n",
      "        |          NULL|             1|\n",
      "        +--------------+--------------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(None, 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (\"b\", 2)], [\"a\", \"b\"]\n",
      "        ... ).select(sf.first_value('a', True), sf.first_value('b', True)).show()\n",
      "        +--------------+--------------+\n",
      "        |first_value(a)|first_value(b)|\n",
      "        +--------------+--------------+\n",
      "        |             a|             1|\n",
      "        +--------------+--------------+\n",
      "    \n",
      "    flatten(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: creates a single array from an array of arrays.\n",
      "        If a structure of nested arrays is deeper than two levels,\n",
      "        only one level of nesting is removed.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            flattened array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([[1, 2, 3], [4, 5], [6]],), ([None, [4, 5]],)], ['data'])\n",
      "        >>> df.show(truncate=False)\n",
      "        +------------------------+\n",
      "        |data                    |\n",
      "        +------------------------+\n",
      "        |[[1, 2, 3], [4, 5], [6]]|\n",
      "        |[NULL, [4, 5]]          |\n",
      "        +------------------------+\n",
      "        >>> df.select(flatten(df.data).alias('r')).show()\n",
      "        +------------------+\n",
      "        |                 r|\n",
      "        +------------------+\n",
      "        |[1, 2, 3, 4, 5, 6]|\n",
      "        |              NULL|\n",
      "        +------------------+\n",
      "    \n",
      "    floor(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the floor of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to find floor for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            nearest integer that is less than or equal to given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(floor(lit(2.5))).show()\n",
      "        +----------+\n",
      "        |FLOOR(2.5)|\n",
      "        +----------+\n",
      "        |         2|\n",
      "        +----------+\n",
      "    \n",
      "    forall(col: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Returns whether a predicate holds for every element in the array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            ``(x: Column) -> Column: ...``  returning the Boolean expression.\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            True if \"all\" elements of an array evaluates to True when passed as an argument to\n",
      "            given function and False otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"bar\"]), (2, [\"foo\", \"bar\"]), (3, [\"foobar\", \"foo\"])],\n",
      "        ...     (\"key\", \"values\")\n",
      "        ... )\n",
      "        >>> df.select(forall(\"values\", lambda x: x.rlike(\"foo\")).alias(\"all_foo\")).show()\n",
      "        +-------+\n",
      "        |all_foo|\n",
      "        +-------+\n",
      "        |  false|\n",
      "        |  false|\n",
      "        |   true|\n",
      "        +-------+\n",
      "    \n",
      "    format_number(col: 'ColumnOrName', d: int) -> pyspark.sql.column.Column\n",
      "        Formats the number X to a format like '#,--#,--#.--', rounded to d decimal places\n",
      "        with HALF_EVEN round mode, and returns the result as a string.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            the column name of the numeric value to be formatted\n",
      "        d : int\n",
      "            the N decimal places\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column of formatted results.\n",
      "        \n",
      "        >>> spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()\n",
      "        [Row(v='5.0000')]\n",
      "    \n",
      "    format_string(format: str, *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Formats the arguments in printf-style and returns the result as a string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        format : str\n",
      "            string that can contain embedded format tags and used as result column's value\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s to be used in formatting\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column of formatted results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(5, \"hello\")], ['a', 'b'])\n",
      "        >>> df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()\n",
      "        [Row(v='5 hello')]\n",
      "    \n",
      "    from_csv(col: 'ColumnOrName', schema: Union[pyspark.sql.column.Column, str], options: Optional[Dict[str, str]] = None) -> pyspark.sql.column.Column\n",
      "        Parses a column containing a CSV string to a row with the specified schema.\n",
      "        Returns `null`, in the case of an unparseable string.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            a column or column name in CSV format\n",
      "        schema :class:`~pyspark.sql.Column` or str\n",
      "            a column, or Python string literal with schema in DDL format, to use when parsing the CSV column.\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the CSV datasource.\n",
      "            See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "            for the version you use.\n",
      "        \n",
      "            .. # noqa\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of parsed CSV values\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> data = [(\"1,2,3\",)]\n",
      "        >>> df = spark.createDataFrame(data, (\"value\",))\n",
      "        >>> df.select(from_csv(df.value, \"a INT, b INT, c INT\").alias(\"csv\")).collect()\n",
      "        [Row(csv=Row(a=1, b=2, c=3))]\n",
      "        >>> value = data[0][0]\n",
      "        >>> df.select(from_csv(df.value, schema_of_csv(value)).alias(\"csv\")).collect()\n",
      "        [Row(csv=Row(_c0=1, _c1=2, _c2=3))]\n",
      "        >>> data = [(\"   abc\",)]\n",
      "        >>> df = spark.createDataFrame(data, (\"value\",))\n",
      "        >>> options = {'ignoreLeadingWhiteSpace': True}\n",
      "        >>> df.select(from_csv(df.value, \"s string\", options).alias(\"csv\")).collect()\n",
      "        [Row(csv=Row(s='abc'))]\n",
      "    \n",
      "    from_json(col: 'ColumnOrName', schema: Union[pyspark.sql.types.ArrayType, pyspark.sql.types.StructType, pyspark.sql.column.Column, str], options: Optional[Dict[str, str]] = None) -> pyspark.sql.column.Column\n",
      "        Parses a column containing a JSON string into a :class:`MapType` with :class:`StringType`\n",
      "        as keys type, :class:`StructType` or :class:`ArrayType` with\n",
      "        the specified schema. Returns `null`, in the case of an unparseable string.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            a column or column name in JSON format\n",
      "        schema : :class:`DataType` or str\n",
      "            a StructType, ArrayType of StructType or Python string literal with a DDL-formatted string\n",
      "            to use when parsing the json column\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the json datasource.\n",
      "            See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      "            for the version you use.\n",
      "        \n",
      "            .. # noqa\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a new column of complex type from given JSON object.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> data = [(1, '''{\"a\": 1}''')]\n",
      "        >>> schema = StructType([StructField(\"a\", IntegerType())])\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=1))]\n",
      "        >>> df.select(from_json(df.value, \"a INT\").alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=1))]\n",
      "        >>> df.select(from_json(df.value, \"MAP<STRING,INT>\").alias(\"json\")).collect()\n",
      "        [Row(json={'a': 1})]\n",
      "        >>> data = [(1, '''[{\"a\": 1}]''')]\n",
      "        >>> schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=[Row(a=1)])]\n",
      "        >>> schema = schema_of_json(lit('''{\"a\": 0}'''))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=None))]\n",
      "        >>> data = [(1, '''[1, 2, 3]''')]\n",
      "        >>> schema = ArrayType(IntegerType())\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=[1, 2, 3])]\n",
      "    \n",
      "    from_unixtime(timestamp: 'ColumnOrName', format: str = 'yyyy-MM-dd HH:mm:ss') -> pyspark.sql.column.Column\n",
      "        Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string\n",
      "        representing the timestamp of that moment in the current system time zone in the given\n",
      "        format.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            column of unix time values.\n",
      "        format : str, optional\n",
      "            format to use to convert to (default: yyyy-MM-dd HH:mm:ss)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            formatted timestamp as string.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> time_df = spark.createDataFrame([(1428476400,)], ['unix_time'])\n",
      "        >>> time_df.select(from_unixtime('unix_time').alias('ts')).collect()\n",
      "        [Row(ts='2015-04-08 00:00:00')]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    from_utc_timestamp(timestamp: 'ColumnOrName', tz: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n",
      "        takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and\n",
      "        renders that timestamp as a timestamp in the given time zone.\n",
      "        \n",
      "        However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n",
      "        timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to\n",
      "        the given timezone.\n",
      "        \n",
      "        This function may return confusing result if the input is a string with timezone, e.g.\n",
      "        '2018-03-13T06:18:23+00:00'. The reason is that, Spark firstly cast the string to timestamp\n",
      "        according to the timezone in the string, and finally display the result by converting the\n",
      "        timestamp to string according to the session local timezone.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            the column that contains timestamps\n",
      "        tz : :class:`~pyspark.sql.Column` or str\n",
      "            A string detailing the time zone ID that the input should be adjusted to. It should\n",
      "            be in the format of either region-based zone IDs or zone offsets. Region IDs must\n",
      "            have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in\n",
      "            the format '(+|-)HH:mm', for example '-08:00' or '+01:00'. Also 'UTC' and 'Z' are\n",
      "            supported as aliases of '+00:00'. Other short names are not recommended to use\n",
      "            because they can be ambiguous.\n",
      "        \n",
      "            .. versionchanged:: 2.4\n",
      "               `tz` can take a :class:`~pyspark.sql.Column` containing timezone ID strings.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            timestamp value represented in given timezone.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n",
      "        >>> df.select(from_utc_timestamp(df.ts, \"PST\").alias('local_time')).collect()\n",
      "        [Row(local_time=datetime.datetime(1997, 2, 28, 2, 30))]\n",
      "        >>> df.select(from_utc_timestamp(df.ts, df.tz).alias('local_time')).collect()\n",
      "        [Row(local_time=datetime.datetime(1997, 2, 28, 19, 30))]\n",
      "    \n",
      "    get(col: 'ColumnOrName', index: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Collection function: Returns element of array at given (0-based) index.\n",
      "        If the index points outside of the array boundaries, then this function\n",
      "        returns NULL.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        index : :class:`~pyspark.sql.Column` or str or int\n",
      "            index to check for in array\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value at given position.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not 1 based, but 0 based index.\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`element_at`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"], 1)], ['data', 'index'])\n",
      "        >>> df.select(get(df.data, 1)).show()\n",
      "        +------------+\n",
      "        |get(data, 1)|\n",
      "        +------------+\n",
      "        |           b|\n",
      "        +------------+\n",
      "        \n",
      "        >>> df.select(get(df.data, -1)).show()\n",
      "        +-------------+\n",
      "        |get(data, -1)|\n",
      "        +-------------+\n",
      "        |         NULL|\n",
      "        +-------------+\n",
      "        \n",
      "        >>> df.select(get(df.data, 3)).show()\n",
      "        +------------+\n",
      "        |get(data, 3)|\n",
      "        +------------+\n",
      "        |        NULL|\n",
      "        +------------+\n",
      "        \n",
      "        >>> df.select(get(df.data, \"index\")).show()\n",
      "        +----------------+\n",
      "        |get(data, index)|\n",
      "        +----------------+\n",
      "        |               b|\n",
      "        +----------------+\n",
      "        \n",
      "        >>> df.select(get(df.data, col(\"index\") - 1)).show()\n",
      "        +----------------------+\n",
      "        |get(data, (index - 1))|\n",
      "        +----------------------+\n",
      "        |                     a|\n",
      "        +----------------------+\n",
      "    \n",
      "    get_json_object(col: 'ColumnOrName', path: str) -> pyspark.sql.column.Column\n",
      "        Extracts json object from a json string based on json `path` specified, and returns json string\n",
      "        of the extracted json object. It will return null if the input json string is invalid.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            string column in json format\n",
      "        path : str\n",
      "            path to the json object to extract\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string representation of given JSON object value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
      "        >>> df.select(df.key, get_json_object(df.jstring, '$.f1').alias(\"c0\"), \\\n",
      "        ...                   get_json_object(df.jstring, '$.f2').alias(\"c1\") ).collect()\n",
      "        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n",
      "    \n",
      "    getbit(col: 'ColumnOrName', pos: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the value of the bit (0 or 1) at the specified position.\n",
      "        The positions are numbered from right to left, starting at zero.\n",
      "        The position argument cannot be negative.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        pos : :class:`~pyspark.sql.Column` or str\n",
      "            The positions are numbered from right to left, starting at zero.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the value of the bit (0 or 1) at the specified position.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [[1], [1], [2]], [\"c\"]\n",
      "        ... ).select(sf.getbit(\"c\", sf.lit(1))).show()\n",
      "        +------------+\n",
      "        |getbit(c, 1)|\n",
      "        +------------+\n",
      "        |           0|\n",
      "        |           0|\n",
      "        |           1|\n",
      "        +------------+\n",
      "    \n",
      "    greatest(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the greatest value of the list of column names, skipping null values.\n",
      "        This function takes at least 2 parameters. It will return null if all parameters are null.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            columns to check for gratest value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            gratest value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(greatest(df.a, df.b, df.c).alias(\"greatest\")).collect()\n",
      "        [Row(greatest=4)]\n",
      "    \n",
      "    grouping(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated\n",
      "        or not, returns 1 for aggregated or 0 for not aggregated in the result set.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to check if it's aggregated.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            returns 1 for aggregated or 0 for not aggregated in the result set.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n",
      "        >>> df.cube(\"name\").agg(grouping(\"name\"), sum(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+--------------+--------+\n",
      "        | name|grouping(name)|sum(age)|\n",
      "        +-----+--------------+--------+\n",
      "        | NULL|             1|       7|\n",
      "        |Alice|             0|       2|\n",
      "        |  Bob|             0|       5|\n",
      "        +-----+--------------+--------+\n",
      "    \n",
      "    grouping_id(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the level of grouping, equals to\n",
      "        \n",
      "           (grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + ... + grouping(cn)\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The list of columns should match with grouping columns exactly, or empty (means all\n",
      "        the grouping columns).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            columns to check for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            returns level of the grouping it relates to.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, \"a\", \"a\"),\n",
      "        ...                             (3, \"a\", \"a\"),\n",
      "        ...                             (4, \"b\", \"c\")], [\"c1\", \"c2\", \"c3\"])\n",
      "        >>> df.cube(\"c2\", \"c3\").agg(grouping_id(), sum(\"c1\")).orderBy(\"c2\", \"c3\").show()\n",
      "        +----+----+-------------+-------+\n",
      "        |  c2|  c3|grouping_id()|sum(c1)|\n",
      "        +----+----+-------------+-------+\n",
      "        |NULL|NULL|            3|      8|\n",
      "        |NULL|   a|            2|      4|\n",
      "        |NULL|   c|            2|      4|\n",
      "        |   a|NULL|            1|      4|\n",
      "        |   a|   a|            0|      4|\n",
      "        |   b|NULL|            1|      4|\n",
      "        |   b|   c|            0|      4|\n",
      "        +----+----+-------------+-------+\n",
      "    \n",
      "    hash(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calculates the hash code of given columns, and returns the result as an int column.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            one or more columns to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hash value as int column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('ABC', 'DEF')], ['c1', 'c2'])\n",
      "        \n",
      "        Hash for one column\n",
      "        \n",
      "        >>> df.select(hash('c1').alias('hash')).show()\n",
      "        +----------+\n",
      "        |      hash|\n",
      "        +----------+\n",
      "        |-757602832|\n",
      "        +----------+\n",
      "        \n",
      "        Two or more columns\n",
      "        \n",
      "        >>> df.select(hash('c1', 'c2').alias('hash')).show()\n",
      "        +---------+\n",
      "        |     hash|\n",
      "        +---------+\n",
      "        |599895104|\n",
      "        +---------+\n",
      "    \n",
      "    hex(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes hex value of the given column, which could be :class:`pyspark.sql.types.StringType`,\n",
      "        :class:`pyspark.sql.types.BinaryType`, :class:`pyspark.sql.types.IntegerType` or\n",
      "        :class:`pyspark.sql.types.LongType`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hexadecimal representation of given value as string.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC', 3)], ['a', 'b']).select(hex('a'), hex('b')).collect()\n",
      "        [Row(hex(a)='414243', hex(b)='3')]\n",
      "    \n",
      "    histogram_numeric(col: 'ColumnOrName', nBins: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes a histogram on numeric 'col' using nb bins.\n",
      "        The return value is an array of (x,y) pairs representing the centers of the\n",
      "        histogram's bins. As the value of 'nb' is increased, the histogram approximation\n",
      "        gets finer-grained, but may yield artifacts around outliers. In practice, 20-40\n",
      "        histogram bins appear to work well, with more bins being required for skewed or\n",
      "        smaller datasets. Note that this function creates a histogram with non-uniform\n",
      "        bin widths. It offers no guarantees in terms of the mean-squared-error of the\n",
      "        histogram, but in practice is comparable to the histograms produced by the R/S-Plus\n",
      "        statistical computing packages. Note: the output type of the 'x' field in the return value is\n",
      "        propagated from the input value consumed in the aggregate function.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        nBins : :class:`~pyspark.sql.Column` or str\n",
      "            number of Histogram columns.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a histogram on numeric 'col' using nb bins.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"a\", 1),\n",
      "        ...                             (\"a\", 2),\n",
      "        ...                             (\"a\", 3),\n",
      "        ...                             (\"b\", 8),\n",
      "        ...                             (\"b\", 2)], [\"c1\", \"c2\"])\n",
      "        >>> df.select(histogram_numeric('c2', lit(5))).show()\n",
      "        +------------------------+\n",
      "        |histogram_numeric(c2, 5)|\n",
      "        +------------------------+\n",
      "        |    [{1, 1.0}, {2, 1....|\n",
      "        +------------------------+\n",
      "    \n",
      "    hll_sketch_agg(col: 'ColumnOrName', lgConfigK: Union[int, pyspark.sql.column.Column, NoneType] = None) -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the updatable binary representation of the Datasketches\n",
      "        HllSketch configured with lgConfigK arg.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str or int\n",
      "        lgConfigK : int, optional\n",
      "            The log-base-2 of K, where K is the number of buckets or slots for the HllSketch\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            The binary representation of the HllSketch.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([1,2,2,3], \"INT\")\n",
      "        >>> df1 = df.agg(hll_sketch_estimate(hll_sketch_agg(\"value\")).alias(\"distinct_cnt\"))\n",
      "        >>> df1.show()\n",
      "        +------------+\n",
      "        |distinct_cnt|\n",
      "        +------------+\n",
      "        |           3|\n",
      "        +------------+\n",
      "        >>> df2 = df.agg(hll_sketch_estimate(\n",
      "        ...     hll_sketch_agg(\"value\", lit(12))\n",
      "        ... ).alias(\"distinct_cnt\"))\n",
      "        >>> df2.show()\n",
      "        +------------+\n",
      "        |distinct_cnt|\n",
      "        +------------+\n",
      "        |           3|\n",
      "        +------------+\n",
      "        >>> df3 = df.agg(hll_sketch_estimate(\n",
      "        ...     hll_sketch_agg(col(\"value\"), lit(12))).alias(\"distinct_cnt\"))\n",
      "        >>> df3.show()\n",
      "        +------------+\n",
      "        |distinct_cnt|\n",
      "        +------------+\n",
      "        |           3|\n",
      "        +------------+\n",
      "    \n",
      "    hll_sketch_estimate(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the estimated number of unique values given the binary representation\n",
      "        of a Datasketches HllSketch.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            The estimated number of unique values for the HllSketch.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([1,2,2,3], \"INT\")\n",
      "        >>> df = df.agg(hll_sketch_estimate(hll_sketch_agg(\"value\")).alias(\"distinct_cnt\"))\n",
      "        >>> df.show()\n",
      "        +------------+\n",
      "        |distinct_cnt|\n",
      "        +------------+\n",
      "        |           3|\n",
      "        +------------+\n",
      "    \n",
      "    hll_union(col1: 'ColumnOrName', col2: 'ColumnOrName', allowDifferentLgConfigK: Optional[bool] = None) -> pyspark.sql.column.Column\n",
      "        Merges two binary representations of Datasketches HllSketch objects, using a\n",
      "        Datasketches Union object.  Throws an exception if sketches have different\n",
      "        lgConfigK values and allowDifferentLgConfigK is unset or set to false.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "        allowDifferentLgConfigK : bool, optional\n",
      "            Allow sketches with different lgConfigK values to be merged (defaults to false).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            The binary representation of the merged HllSketch.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1,4),(2,5),(2,5),(3,6)], \"struct<v1:int,v2:int>\")\n",
      "        >>> df = df.agg(hll_sketch_agg(\"v1\").alias(\"sketch1\"), hll_sketch_agg(\"v2\").alias(\"sketch2\"))\n",
      "        >>> df = df.withColumn(\"distinct_cnt\", hll_sketch_estimate(hll_union(\"sketch1\", \"sketch2\")))\n",
      "        >>> df.drop(\"sketch1\", \"sketch2\").show()\n",
      "        +------------+\n",
      "        |distinct_cnt|\n",
      "        +------------+\n",
      "        |           6|\n",
      "        +------------+\n",
      "    \n",
      "    hll_union_agg(col: 'ColumnOrName', allowDifferentLgConfigK: Union[bool, pyspark.sql.column.Column, NoneType] = None) -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the updatable binary representation of the Datasketches\n",
      "        HllSketch, generated by merging previously created Datasketches HllSketch instances\n",
      "        via a Datasketches Union instance. Throws an exception if sketches have different\n",
      "        lgConfigK values and allowDifferentLgConfigK is unset or set to false.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str or bool\n",
      "        allowDifferentLgConfigK : bool, optional\n",
      "            Allow sketches with different lgConfigK values to be merged (defaults to false).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            The binary representation of the merged HllSketch.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([1,2,2,3], \"INT\")\n",
      "        >>> df1 = df1.agg(hll_sketch_agg(\"value\").alias(\"sketch\"))\n",
      "        >>> df2 = spark.createDataFrame([4,5,5,6], \"INT\")\n",
      "        >>> df2 = df2.agg(hll_sketch_agg(\"value\").alias(\"sketch\"))\n",
      "        >>> df3 = df1.union(df2).agg(hll_sketch_estimate(\n",
      "        ...     hll_union_agg(\"sketch\")\n",
      "        ... ).alias(\"distinct_cnt\"))\n",
      "        >>> df3.drop(\"sketch\").show()\n",
      "        +------------+\n",
      "        |distinct_cnt|\n",
      "        +------------+\n",
      "        |           6|\n",
      "        +------------+\n",
      "        >>> df4 = df1.union(df2).agg(hll_sketch_estimate(\n",
      "        ...     hll_union_agg(\"sketch\", lit(False))\n",
      "        ... ).alias(\"distinct_cnt\"))\n",
      "        >>> df4.drop(\"sketch\").show()\n",
      "        +------------+\n",
      "        |distinct_cnt|\n",
      "        +------------+\n",
      "        |           6|\n",
      "        +------------+\n",
      "        >>> df5 = df1.union(df2).agg(hll_sketch_estimate(\n",
      "        ...     hll_union_agg(col(\"sketch\"), lit(False))\n",
      "        ... ).alias(\"distinct_cnt\"))\n",
      "        >>> df5.drop(\"sketch\").show()\n",
      "        +------------+\n",
      "        |distinct_cnt|\n",
      "        +------------+\n",
      "        |           6|\n",
      "        +------------+\n",
      "    \n",
      "    hour(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the hours of a given timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hour part of the timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame([(datetime.datetime(2015, 4, 8, 13, 8, 15),)], ['ts'])\n",
      "        >>> df.select(hour('ts').alias('hour')).collect()\n",
      "        [Row(hour=13)]\n",
      "    \n",
      "    hours(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Partition transform function: A transform for timestamps\n",
      "        to partition data into hours.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date or timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            data partitioned by hours.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(   # doctest: +SKIP\n",
      "        ...     hours(\"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    hypot(col1: Union[ForwardRef('ColumnOrName'), float], col2: Union[ForwardRef('ColumnOrName'), float]) -> pyspark.sql.column.Column\n",
      "        Computes ``sqrt(a^2 + b^2)`` without intermediate overflow or underflow.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str, :class:`~pyspark.sql.Column` or float\n",
      "            a leg.\n",
      "        col2 : str, :class:`~pyspark.sql.Column` or float\n",
      "            b leg.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            length of the hypotenuse.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(hypot(lit(1), lit(2))).first()\n",
      "        Row(HYPOT(1, 2)=2.23606...)\n",
      "    \n",
      "    ifnull(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns `col2` if `col1` is null, or `col1` otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> df = spark.createDataFrame([(None,), (1,)], [\"e\"])\n",
      "        >>> df.select(sf.ifnull(df.e, sf.lit(8))).show()\n",
      "        +------------+\n",
      "        |ifnull(e, 8)|\n",
      "        +------------+\n",
      "        |           8|\n",
      "        |           1|\n",
      "        +------------+\n",
      "    \n",
      "    ilike(str: 'ColumnOrName', pattern: 'ColumnOrName', escapeChar: Optional[ForwardRef('Column')] = None) -> pyspark.sql.column.Column\n",
      "        Returns true if str matches `pattern` with `escape` case-insensitively,\n",
      "        null if any arguments are null, false otherwise.\n",
      "        The default escape character is the ''.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            A string.\n",
      "        pattern : :class:`~pyspark.sql.Column` or str\n",
      "            A string. The pattern is a string which is matched literally, with\n",
      "            exception to the following special symbols:\n",
      "            _ matches any one character in the input (similar to . in posix regular expressions)\n",
      "            % matches zero or more characters in the input (similar to .* in posix regular\n",
      "            expressions)\n",
      "            Since Spark 2.0, string literals are unescaped in our SQL parser. For example, in order\n",
      "            to match \"\u0007bc\", the pattern should be \"\\abc\".\n",
      "            When SQL config 'spark.sql.parser.escapedStringLiterals' is enabled, it falls back\n",
      "            to Spark 1.6 behavior regarding string literal parsing. For example, if the config is\n",
      "            enabled, the pattern to match \"\u0007bc\" should be \"\u0007bc\".\n",
      "        escape : :class:`~pyspark.sql.Column`\n",
      "            An character added since Spark 3.0. The default escape character is the ''.\n",
      "            If an escape character precedes a special symbol or another escape character, the\n",
      "            following character is matched literally. It is invalid to escape any other character.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Spark\", \"_park\")], ['a', 'b'])\n",
      "        >>> df.select(ilike(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=True)]\n",
      "        \n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(\"%SystemDrive%/Users/John\", \"/%SystemDrive/%//Users%\")],\n",
      "        ...     ['a', 'b']\n",
      "        ... )\n",
      "        >>> df.select(ilike(df.a, df.b, lit('/')).alias('r')).collect()\n",
      "        [Row(r=True)]\n",
      "    \n",
      "    initcap(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Translate the first letter of each word to upper case in the sentence.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string with all first letters are uppercase in each word.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ab cd',)], ['a']).select(initcap(\"a\").alias('v')).collect()\n",
      "        [Row(v='Ab Cd')]\n",
      "    \n",
      "    inline(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Explodes an array of structs into a table.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to explode.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            generator expression with the inline exploded result.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`explode`\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(structlist=[Row(a=1, b=2), Row(a=3, b=4)])])\n",
      "        >>> df.select(inline(df.structlist)).show()\n",
      "        +---+---+\n",
      "        |  a|  b|\n",
      "        +---+---+\n",
      "        |  1|  2|\n",
      "        |  3|  4|\n",
      "        +---+---+\n",
      "    \n",
      "    inline_outer(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Explodes an array of structs into a table.\n",
      "        Unlike inline, if the array is null or empty then null is produced for each nested column.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to explode.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            generator expression with the inline exploded result.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`explode_outer`\n",
      "        :meth:`inline`\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     Row(id=1, structlist=[Row(a=1, b=2), Row(a=3, b=4)]),\n",
      "        ...     Row(id=2, structlist=[])\n",
      "        ... ])\n",
      "        >>> df.select('id', inline_outer(df.structlist)).show()\n",
      "        +---+----+----+\n",
      "        | id|   a|   b|\n",
      "        +---+----+----+\n",
      "        |  1|   1|   2|\n",
      "        |  1|   3|   4|\n",
      "        |  2|NULL|NULL|\n",
      "        +---+----+----+\n",
      "    \n",
      "    input_file_block_length() -> pyspark.sql.column.Column\n",
      "        Returns the length of the block being read, or -1 if not available.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.read.text(\"python/test_support/sql/ages_newlines.csv\", lineSep=\",\")\n",
      "        >>> df.select(input_file_block_length().alias('r')).first()\n",
      "        Row(r=87)\n",
      "    \n",
      "    input_file_block_start() -> pyspark.sql.column.Column\n",
      "        Returns the start offset of the block being read, or -1 if not available.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.read.text(\"python/test_support/sql/ages_newlines.csv\", lineSep=\",\")\n",
      "        >>> df.select(input_file_block_start().alias('r')).first()\n",
      "        Row(r=0)\n",
      "    \n",
      "    input_file_name() -> pyspark.sql.column.Column\n",
      "        Creates a string column for the file name of the current Spark task.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            file names.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import os\n",
      "        >>> path = os.path.abspath(__file__)\n",
      "        >>> df = spark.read.text(path)\n",
      "        >>> df.select(input_file_name()).first()\n",
      "        Row(input_file_name()='file:///...')\n",
      "    \n",
      "    instr(str: 'ColumnOrName', substr: str) -> pyspark.sql.column.Column\n",
      "        Locate the position of the first occurrence of substr column in the given string.\n",
      "        Returns null if either of the arguments are null.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index. Returns 0 if substr\n",
      "        could not be found in str.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        substr : str\n",
      "            substring to look for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            location of the first occurrence of the substring as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(instr(df.s, 'b').alias('s')).collect()\n",
      "        [Row(s=2)]\n",
      "    \n",
      "    isnan(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        An expression that returns true if the column is NaN.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            True if value is NaN and False otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
      "        >>> df.select(\"a\", \"b\", isnan(\"a\").alias(\"r1\"), isnan(df.b).alias(\"r2\")).show()\n",
      "        +---+---+-----+-----+\n",
      "        |  a|  b|   r1|   r2|\n",
      "        +---+---+-----+-----+\n",
      "        |1.0|NaN|false| true|\n",
      "        |NaN|2.0| true|false|\n",
      "        +---+---+-----+-----+\n",
      "    \n",
      "    isnotnull(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns true if `col` is not null, or false otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None,), (1,)], [\"e\"])\n",
      "        >>> df.select(isnotnull(df.e).alias('r')).collect()\n",
      "        [Row(r=False), Row(r=True)]\n",
      "    \n",
      "    isnull(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        An expression that returns true if the column is null.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            True if value is null and False otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, None), (None, 2)], (\"a\", \"b\"))\n",
      "        >>> df.select(\"a\", \"b\", isnull(\"a\").alias(\"r1\"), isnull(df.b).alias(\"r2\")).show()\n",
      "        +----+----+-----+-----+\n",
      "        |   a|   b|   r1|   r2|\n",
      "        +----+----+-----+-----+\n",
      "        |   1|NULL|false| true|\n",
      "        |NULL|   2| true|false|\n",
      "        +----+----+-----+-----+\n",
      "    \n",
      "    java_method(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calls a method with reflection.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            the first element should be a literal string for the class name,\n",
      "            and the second element should be a literal string for the method name,\n",
      "            and the remaining are input arguments to the Java method.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(\n",
      "        ...     sf.java_method(\n",
      "        ...         sf.lit(\"java.util.UUID\"),\n",
      "        ...         sf.lit(\"fromString\"),\n",
      "        ...         sf.lit(\"a5cf6c42-0c85-418f-af6c-3e4e5b1328f2\")\n",
      "        ...     )\n",
      "        ... ).show(truncate=False)\n",
      "        +-----------------------------------------------------------------------------+\n",
      "        |java_method(java.util.UUID, fromString, a5cf6c42-0c85-418f-af6c-3e4e5b1328f2)|\n",
      "        +-----------------------------------------------------------------------------+\n",
      "        |a5cf6c42-0c85-418f-af6c-3e4e5b1328f2                                         |\n",
      "        +-----------------------------------------------------------------------------+\n",
      "    \n",
      "    json_array_length(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of elements in the outermost JSON array. `NULL` is returned in case of\n",
      "        any other valid JSON string, `NULL` or an invalid JSON.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col: :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            length of json array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None,), ('[1, 2, 3]',), ('[]',)], ['data'])\n",
      "        >>> df.select(json_array_length(df.data).alias('r')).collect()\n",
      "        [Row(r=None), Row(r=3), Row(r=0)]\n",
      "    \n",
      "    json_object_keys(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns all the keys of the outermost JSON object as an array. If a valid JSON object is\n",
      "        given, all the keys of the outermost object will be returned as an array. If it is any\n",
      "        other valid JSON string, an invalid JSON string or an empty string, the function returns null.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col: :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            all the keys of the outermost JSON object.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None,), ('{}',), ('{\"key1\":1, \"key2\":2}',)], ['data'])\n",
      "        >>> df.select(json_object_keys(df.data).alias('r')).collect()\n",
      "        [Row(r=None), Row(r=[]), Row(r=['key1', 'key2'])]\n",
      "    \n",
      "    json_tuple(col: 'ColumnOrName', *fields: str) -> pyspark.sql.column.Column\n",
      "        Creates a new row for a json column according to the given field names.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            string column in json format\n",
      "        fields : str\n",
      "            a field or fields to extract\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a new row for each given field value from json object\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
      "        >>> df.select(df.key, json_tuple(df.jstring, 'f1', 'f2')).collect()\n",
      "        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n",
      "    \n",
      "    kurtosis(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the kurtosis of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            kurtosis of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
      "        >>> df.select(kurtosis(df.c)).show()\n",
      "        +-----------+\n",
      "        |kurtosis(c)|\n",
      "        +-----------+\n",
      "        |       -1.5|\n",
      "        +-----------+\n",
      "    \n",
      "    lag(col: 'ColumnOrName', offset: int = 1, default: Optional[Any] = None) -> pyspark.sql.column.Column\n",
      "        Window function: returns the value that is `offset` rows before the current row, and\n",
      "        `default` if there is less than `offset` rows before the current row. For example,\n",
      "        an `offset` of one will return the previous row at any given point in the window partition.\n",
      "        \n",
      "        This is equivalent to the LAG function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        offset : int, optional default 1\n",
      "            number of row to extend\n",
      "        default : optional\n",
      "            default value\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value before current row based on `offset`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> df = spark.createDataFrame([(\"a\", 1),\n",
      "        ...                             (\"a\", 2),\n",
      "        ...                             (\"a\", 3),\n",
      "        ...                             (\"b\", 8),\n",
      "        ...                             (\"b\", 2)], [\"c1\", \"c2\"])\n",
      "        >>> df.show()\n",
      "        +---+---+\n",
      "        | c1| c2|\n",
      "        +---+---+\n",
      "        |  a|  1|\n",
      "        |  a|  2|\n",
      "        |  a|  3|\n",
      "        |  b|  8|\n",
      "        |  b|  2|\n",
      "        +---+---+\n",
      "        >>> w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
      "        >>> df.withColumn(\"previos_value\", lag(\"c2\").over(w)).show()\n",
      "        +---+---+-------------+\n",
      "        | c1| c2|previos_value|\n",
      "        +---+---+-------------+\n",
      "        |  a|  1|         NULL|\n",
      "        |  a|  2|            1|\n",
      "        |  a|  3|            2|\n",
      "        |  b|  2|         NULL|\n",
      "        |  b|  8|            2|\n",
      "        +---+---+-------------+\n",
      "        >>> df.withColumn(\"previos_value\", lag(\"c2\", 1, 0).over(w)).show()\n",
      "        +---+---+-------------+\n",
      "        | c1| c2|previos_value|\n",
      "        +---+---+-------------+\n",
      "        |  a|  1|            0|\n",
      "        |  a|  2|            1|\n",
      "        |  a|  3|            2|\n",
      "        |  b|  2|            0|\n",
      "        |  b|  8|            2|\n",
      "        +---+---+-------------+\n",
      "        >>> df.withColumn(\"previos_value\", lag(\"c2\", 2, -1).over(w)).show()\n",
      "        +---+---+-------------+\n",
      "        | c1| c2|previos_value|\n",
      "        +---+---+-------------+\n",
      "        |  a|  1|           -1|\n",
      "        |  a|  2|           -1|\n",
      "        |  a|  3|            1|\n",
      "        |  b|  2|           -1|\n",
      "        |  b|  8|           -1|\n",
      "        +---+---+-------------+\n",
      "    \n",
      "    last(col: 'ColumnOrName', ignorenulls: bool = False) -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the last value in a group.\n",
      "        \n",
      "        The function by default returns the last values it sees. It will return the last non-null\n",
      "        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because its results depends on the order of the\n",
      "        rows which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to fetch last value for.\n",
      "        ignorenulls : :class:`~pyspark.sql.Column` or str\n",
      "            if last value is null then look for non-null value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            last value of the group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5), (\"Alice\", None)], (\"name\", \"age\"))\n",
      "        >>> df = df.orderBy(df.age.desc())\n",
      "        >>> df.groupby(\"name\").agg(last(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+---------+\n",
      "        | name|last(age)|\n",
      "        +-----+---------+\n",
      "        |Alice|     NULL|\n",
      "        |  Bob|        5|\n",
      "        +-----+---------+\n",
      "        \n",
      "        Now, to ignore any nulls we needs to set ``ignorenulls`` to `True`\n",
      "        \n",
      "        >>> df.groupby(\"name\").agg(last(\"age\", ignorenulls=True)).orderBy(\"name\").show()\n",
      "        +-----+---------+\n",
      "        | name|last(age)|\n",
      "        +-----+---------+\n",
      "        |Alice|        2|\n",
      "        |  Bob|        5|\n",
      "        +-----+---------+\n",
      "    \n",
      "    last_day(date: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the last day of the month which the given date belongs to.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        date : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            last day of the month.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-10',)], ['d'])\n",
      "        >>> df.select(last_day(df.d).alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "    \n",
      "    last_value(col: 'ColumnOrName', ignoreNulls: Union[bool, pyspark.sql.column.Column, NoneType] = None) -> pyspark.sql.column.Column\n",
      "        Returns the last value of `col` for a group of rows. It will return the last non-null\n",
      "        value it sees when `ignoreNulls` is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        ignorenulls : :class:`~pyspark.sql.Column` or bool\n",
      "            if first value is null then look for first non-null value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            some value of `col` for a group of rows.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"a\", 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (None, 2)], [\"a\", \"b\"]\n",
      "        ... ).select(sf.last_value('a'), sf.last_value('b')).show()\n",
      "        +-------------+-------------+\n",
      "        |last_value(a)|last_value(b)|\n",
      "        +-------------+-------------+\n",
      "        |         NULL|            2|\n",
      "        +-------------+-------------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"a\", 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (None, 2)], [\"a\", \"b\"]\n",
      "        ... ).select(sf.last_value('a', True), sf.last_value('b', True)).show()\n",
      "        +-------------+-------------+\n",
      "        |last_value(a)|last_value(b)|\n",
      "        +-------------+-------------+\n",
      "        |            b|            2|\n",
      "        +-------------+-------------+\n",
      "    \n",
      "    lcase(str: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns `str` with all characters changed to lowercase.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.lcase(sf.lit(\"Spark\"))).show()\n",
      "        +------------+\n",
      "        |lcase(Spark)|\n",
      "        +------------+\n",
      "        |       spark|\n",
      "        +------------+\n",
      "    \n",
      "    lead(col: 'ColumnOrName', offset: int = 1, default: Optional[Any] = None) -> pyspark.sql.column.Column\n",
      "        Window function: returns the value that is `offset` rows after the current row, and\n",
      "        `default` if there is less than `offset` rows after the current row. For example,\n",
      "        an `offset` of one will return the next row at any given point in the window partition.\n",
      "        \n",
      "        This is equivalent to the LEAD function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        offset : int, optional default 1\n",
      "            number of row to extend\n",
      "        default : optional\n",
      "            default value\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value after current row based on `offset`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> df = spark.createDataFrame([(\"a\", 1),\n",
      "        ...                             (\"a\", 2),\n",
      "        ...                             (\"a\", 3),\n",
      "        ...                             (\"b\", 8),\n",
      "        ...                             (\"b\", 2)], [\"c1\", \"c2\"])\n",
      "        >>> df.show()\n",
      "        +---+---+\n",
      "        | c1| c2|\n",
      "        +---+---+\n",
      "        |  a|  1|\n",
      "        |  a|  2|\n",
      "        |  a|  3|\n",
      "        |  b|  8|\n",
      "        |  b|  2|\n",
      "        +---+---+\n",
      "        >>> w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
      "        >>> df.withColumn(\"next_value\", lead(\"c2\").over(w)).show()\n",
      "        +---+---+----------+\n",
      "        | c1| c2|next_value|\n",
      "        +---+---+----------+\n",
      "        |  a|  1|         2|\n",
      "        |  a|  2|         3|\n",
      "        |  a|  3|      NULL|\n",
      "        |  b|  2|         8|\n",
      "        |  b|  8|      NULL|\n",
      "        +---+---+----------+\n",
      "        >>> df.withColumn(\"next_value\", lead(\"c2\", 1, 0).over(w)).show()\n",
      "        +---+---+----------+\n",
      "        | c1| c2|next_value|\n",
      "        +---+---+----------+\n",
      "        |  a|  1|         2|\n",
      "        |  a|  2|         3|\n",
      "        |  a|  3|         0|\n",
      "        |  b|  2|         8|\n",
      "        |  b|  8|         0|\n",
      "        +---+---+----------+\n",
      "        >>> df.withColumn(\"next_value\", lead(\"c2\", 2, -1).over(w)).show()\n",
      "        +---+---+----------+\n",
      "        | c1| c2|next_value|\n",
      "        +---+---+----------+\n",
      "        |  a|  1|         3|\n",
      "        |  a|  2|        -1|\n",
      "        |  a|  3|        -1|\n",
      "        |  b|  2|        -1|\n",
      "        |  b|  8|        -1|\n",
      "        +---+---+----------+\n",
      "    \n",
      "    least(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the least value of the list of column names, skipping null values.\n",
      "        This function takes at least 2 parameters. It will return null if all parameters are null.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or columns to be compared\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            least value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(least(df.a, df.b, df.c).alias(\"least\")).collect()\n",
      "        [Row(least=1)]\n",
      "    \n",
      "    left(str: 'ColumnOrName', len: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the leftmost `len`(`len` can be string type) characters from the string `str`,\n",
      "        if `len` is less or equal than 0 the result is an empty string.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        len : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings, the leftmost `len`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Spark SQL\", 3,)], ['a', 'b'])\n",
      "        >>> df.select(left(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r='Spa')]\n",
      "    \n",
      "    length(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the character length of string data or number of bytes of binary data.\n",
      "        The length of character data includes the trailing spaces. The length of binary data\n",
      "        includes binary zeros.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            length of the value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC ',)], ['a']).select(length('a').alias('length')).collect()\n",
      "        [Row(length=4)]\n",
      "    \n",
      "    levenshtein(left: 'ColumnOrName', right: 'ColumnOrName', threshold: Optional[int] = None) -> pyspark.sql.column.Column\n",
      "        Computes the Levenshtein distance of the two given strings.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : :class:`~pyspark.sql.Column` or str\n",
      "            first column value.\n",
      "        right : :class:`~pyspark.sql.Column` or str\n",
      "            second column value.\n",
      "        threshold : int, optional\n",
      "            if set when the levenshtein distance of the two given strings\n",
      "            less than or equal to a given threshold then return result distance, or -1\n",
      "        \n",
      "            .. versionchanged: 3.5.0\n",
      "                Added ``threshold`` argument.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            Levenshtein distance as integer value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n",
      "        >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\n",
      "        [Row(d=3)]\n",
      "        >>> df0.select(levenshtein('l', 'r', 2).alias('d')).collect()\n",
      "        [Row(d=-1)]\n",
      "    \n",
      "    like(str: 'ColumnOrName', pattern: 'ColumnOrName', escapeChar: Optional[ForwardRef('Column')] = None) -> pyspark.sql.column.Column\n",
      "        Returns true if str matches `pattern` with `escape`,\n",
      "        null if any arguments are null, false otherwise.\n",
      "        The default escape character is the ''.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            A string.\n",
      "        pattern : :class:`~pyspark.sql.Column` or str\n",
      "            A string. The pattern is a string which is matched literally, with\n",
      "            exception to the following special symbols:\n",
      "            _ matches any one character in the input (similar to . in posix regular expressions)\n",
      "            % matches zero or more characters in the input (similar to .* in posix regular\n",
      "            expressions)\n",
      "            Since Spark 2.0, string literals are unescaped in our SQL parser. For example, in order\n",
      "            to match \"\u0007bc\", the pattern should be \"\\abc\".\n",
      "            When SQL config 'spark.sql.parser.escapedStringLiterals' is enabled, it falls back\n",
      "            to Spark 1.6 behavior regarding string literal parsing. For example, if the config is\n",
      "            enabled, the pattern to match \"\u0007bc\" should be \"\u0007bc\".\n",
      "        escape : :class:`~pyspark.sql.Column`\n",
      "            An character added since Spark 3.0. The default escape character is the ''.\n",
      "            If an escape character precedes a special symbol or another escape character, the\n",
      "            following character is matched literally. It is invalid to escape any other character.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Spark\", \"_park\")], ['a', 'b'])\n",
      "        >>> df.select(like(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=True)]\n",
      "        \n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(\"%SystemDrive%/Users/John\", \"/%SystemDrive/%//Users%\")],\n",
      "        ...     ['a', 'b']\n",
      "        ... )\n",
      "        >>> df.select(like(df.a, df.b, lit('/')).alias('r')).collect()\n",
      "        [Row(r=True)]\n",
      "    \n",
      "    lit(col: Any) -> pyspark.sql.column.Column\n",
      "        Creates a :class:`~pyspark.sql.Column` of literal value.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column`, str, int, float, bool or list, NumPy literals or ndarray.\n",
      "            the value to make it as a PySpark literal. If a column is passed,\n",
      "            it returns the column as is.\n",
      "        \n",
      "            .. versionchanged:: 3.4.0\n",
      "                Since 3.4.0, it supports the list type.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the literal instance.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(lit(5).alias('height'), df.id).show()\n",
      "        +------+---+\n",
      "        |height| id|\n",
      "        +------+---+\n",
      "        |     5|  0|\n",
      "        +------+---+\n",
      "        \n",
      "        Create a literal from a list.\n",
      "        \n",
      "        >>> spark.range(1).select(lit([1, 2, 3])).show()\n",
      "        +--------------+\n",
      "        |array(1, 2, 3)|\n",
      "        +--------------+\n",
      "        |     [1, 2, 3]|\n",
      "        +--------------+\n",
      "    \n",
      "    ln(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the natural logarithm of the argument.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            a column to calculate logariphm for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            natural logarithm of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(4,)], ['a'])\n",
      "        >>> df.select(ln('a')).show()\n",
      "        +------------------+\n",
      "        |             ln(a)|\n",
      "        +------------------+\n",
      "        |1.3862943611198906|\n",
      "        +------------------+\n",
      "    \n",
      "    localtimestamp() -> pyspark.sql.column.Column\n",
      "        Returns the current timestamp without time zone at the start of query evaluation\n",
      "        as a timestamp without time zone column. All calls of localtimestamp within the\n",
      "        same query return the same value.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            current local date and time.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(localtimestamp()).show(truncate=False) # doctest: +SKIP\n",
      "        +-----------------------+\n",
      "        |localtimestamp()       |\n",
      "        +-----------------------+\n",
      "        |2022-08-26 21:28:34.639|\n",
      "        +-----------------------+\n",
      "    \n",
      "    locate(substr: str, str: 'ColumnOrName', pos: int = 1) -> pyspark.sql.column.Column\n",
      "        Locate the position of the first occurrence of substr in a string column, after position pos.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        substr : str\n",
      "            a string\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            a Column of :class:`pyspark.sql.types.StringType`\n",
      "        pos : int, optional\n",
      "            start position (zero based)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            position of the substring.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index. Returns 0 if substr\n",
      "        could not be found in str.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(locate('b', df.s, 1).alias('s')).collect()\n",
      "        [Row(s=2)]\n",
      "    \n",
      "    log(arg1: Union[ForwardRef('ColumnOrName'), float], arg2: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Returns the first argument-based logarithm of the second argument.\n",
      "        \n",
      "        If there is only one argument, then this takes the natural logarithm of the argument.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        arg1 : :class:`~pyspark.sql.Column`, str or float\n",
      "            base number or actual number (in this case base is `e`)\n",
      "        arg2 : :class:`~pyspark.sql.Column`, str or float\n",
      "            number to calculate logariphm for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            logariphm of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import functions as sf\n",
      "        >>> df = spark.sql(\"SELECT * FROM VALUES (1), (2), (4) AS t(value)\")\n",
      "        >>> df.select(sf.log(2.0, df.value).alias('log2_value')).show()\n",
      "        +----------+\n",
      "        |log2_value|\n",
      "        +----------+\n",
      "        |       0.0|\n",
      "        |       1.0|\n",
      "        |       2.0|\n",
      "        +----------+\n",
      "        \n",
      "        And Natural logarithm\n",
      "        \n",
      "        >>> df.select(sf.log(df.value).alias('ln_value')).show()\n",
      "        +------------------+\n",
      "        |          ln_value|\n",
      "        +------------------+\n",
      "        |               0.0|\n",
      "        |0.6931471805599453|\n",
      "        |1.3862943611198906|\n",
      "        +------------------+\n",
      "    \n",
      "    log10(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the logarithm of the given value in Base 10.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to calculate logarithm for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            logarithm of the given value in Base 10.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(log10(lit(100))).show()\n",
      "        +----------+\n",
      "        |LOG10(100)|\n",
      "        +----------+\n",
      "        |       2.0|\n",
      "        +----------+\n",
      "    \n",
      "    log1p(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the natural logarithm of the \"given value plus one\".\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to calculate natural logarithm for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            natural logarithm of the \"given value plus one\".\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(log1p(lit(math.e))).first()\n",
      "        Row(LOG1P(2.71828...)=1.31326...)\n",
      "        \n",
      "        Same as:\n",
      "        \n",
      "        >>> df.select(log(lit(math.e+1))).first()\n",
      "        Row(ln(3.71828...)=1.31326...)\n",
      "    \n",
      "    log2(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the base-2 logarithm of the argument.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            a column to calculate logariphm for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            logariphm of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(4,)], ['a'])\n",
      "        >>> df.select(log2('a').alias('log2')).show()\n",
      "        +----+\n",
      "        |log2|\n",
      "        +----+\n",
      "        | 2.0|\n",
      "        +----+\n",
      "    \n",
      "    lower(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Converts a string expression to lower case.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            lower case values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
      "        >>> df.select(lower(\"value\")).show()\n",
      "        +------------+\n",
      "        |lower(value)|\n",
      "        +------------+\n",
      "        |       spark|\n",
      "        |     pyspark|\n",
      "        |  pandas api|\n",
      "        +------------+\n",
      "    \n",
      "    lpad(col: 'ColumnOrName', len: int, pad: str) -> pyspark.sql.column.Column\n",
      "        Left-pad the string column to width `len` with `pad`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        len : int\n",
      "            length of the final string.\n",
      "        pad : str\n",
      "            chars to prepend.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            left padded result.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()\n",
      "        [Row(s='##abcd')]\n",
      "    \n",
      "    ltrim(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Trim the spaces from left end for the specified string value.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            left trimmed values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n",
      "        >>> df.select(ltrim(\"value\").alias(\"r\")).withColumn(\"length\", length(\"r\")).show()\n",
      "        +-------+------+\n",
      "        |      r|length|\n",
      "        +-------+------+\n",
      "        |  Spark|     5|\n",
      "        |Spark  |     7|\n",
      "        |  Spark|     5|\n",
      "        +-------+------+\n",
      "    \n",
      "    make_date(year: 'ColumnOrName', month: 'ColumnOrName', day: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a column with a date built from the year, month and day columns.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        year : :class:`~pyspark.sql.Column` or str\n",
      "            The year to build the date\n",
      "        month : :class:`~pyspark.sql.Column` or str\n",
      "            The month to build the date\n",
      "        day : :class:`~pyspark.sql.Column` or str\n",
      "            The day to build the date\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a date built from given parts.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(2020, 6, 26)], ['Y', 'M', 'D'])\n",
      "        >>> df.select(make_date(df.Y, df.M, df.D).alias(\"datefield\")).collect()\n",
      "        [Row(datefield=datetime.date(2020, 6, 26))]\n",
      "    \n",
      "    make_dt_interval(days: Optional[ForwardRef('ColumnOrName')] = None, hours: Optional[ForwardRef('ColumnOrName')] = None, mins: Optional[ForwardRef('ColumnOrName')] = None, secs: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Make DayTimeIntervalType duration from days, hours, mins and secs.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        days : :class:`~pyspark.sql.Column` or str\n",
      "            the number of days, positive or negative\n",
      "        hours : :class:`~pyspark.sql.Column` or str\n",
      "            the number of hours, positive or negative\n",
      "        mins : :class:`~pyspark.sql.Column` or str\n",
      "            the number of minutes, positive or negative\n",
      "        secs : :class:`~pyspark.sql.Column` or str\n",
      "            the number of seconds with the fractional part in microsecond precision.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1, 12, 30, 01.001001]],\n",
      "        ...     [\"day\", \"hour\", \"min\", \"sec\"])\n",
      "        >>> df.select(make_dt_interval(\n",
      "        ...     df.day, df.hour, df.min, df.sec).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +------------------------------------------+\n",
      "        |r                                         |\n",
      "        +------------------------------------------+\n",
      "        |INTERVAL '1 12:30:01.001001' DAY TO SECOND|\n",
      "        +------------------------------------------+\n",
      "        \n",
      "        >>> df.select(make_dt_interval(\n",
      "        ...     df.day, df.hour, df.min).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +-----------------------------------+\n",
      "        |r                                  |\n",
      "        +-----------------------------------+\n",
      "        |INTERVAL '1 12:30:00' DAY TO SECOND|\n",
      "        +-----------------------------------+\n",
      "        \n",
      "        >>> df.select(make_dt_interval(\n",
      "        ...     df.day, df.hour).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +-----------------------------------+\n",
      "        |r                                  |\n",
      "        +-----------------------------------+\n",
      "        |INTERVAL '1 12:00:00' DAY TO SECOND|\n",
      "        +-----------------------------------+\n",
      "        \n",
      "        >>> df.select(make_dt_interval(df.day).alias('r')).show(truncate=False)\n",
      "        +-----------------------------------+\n",
      "        |r                                  |\n",
      "        +-----------------------------------+\n",
      "        |INTERVAL '1 00:00:00' DAY TO SECOND|\n",
      "        +-----------------------------------+\n",
      "        \n",
      "        >>> df.select(make_dt_interval().alias('r')).show(truncate=False)\n",
      "        +-----------------------------------+\n",
      "        |r                                  |\n",
      "        +-----------------------------------+\n",
      "        |INTERVAL '0 00:00:00' DAY TO SECOND|\n",
      "        +-----------------------------------+\n",
      "    \n",
      "    make_interval(years: Optional[ForwardRef('ColumnOrName')] = None, months: Optional[ForwardRef('ColumnOrName')] = None, weeks: Optional[ForwardRef('ColumnOrName')] = None, days: Optional[ForwardRef('ColumnOrName')] = None, hours: Optional[ForwardRef('ColumnOrName')] = None, mins: Optional[ForwardRef('ColumnOrName')] = None, secs: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Make interval from years, months, weeks, days, hours, mins and secs.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        years : :class:`~pyspark.sql.Column` or str\n",
      "            the number of years, positive or negative\n",
      "        months : :class:`~pyspark.sql.Column` or str\n",
      "            the number of months, positive or negative\n",
      "        weeks : :class:`~pyspark.sql.Column` or str\n",
      "            the number of weeks, positive or negative\n",
      "        days : :class:`~pyspark.sql.Column` or str\n",
      "            the number of days, positive or negative\n",
      "        hours : :class:`~pyspark.sql.Column` or str\n",
      "            the number of hours, positive or negative\n",
      "        mins : :class:`~pyspark.sql.Column` or str\n",
      "            the number of minutes, positive or negative\n",
      "        secs : :class:`~pyspark.sql.Column` or str\n",
      "            the number of seconds with the fractional part in microsecond precision.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[100, 11, 1, 1, 12, 30, 01.001001]],\n",
      "        ...     [\"year\", \"month\", \"week\", \"day\", \"hour\", \"min\", \"sec\"])\n",
      "        >>> df.select(make_interval(\n",
      "        ...     df.year, df.month, df.week, df.day, df.hour, df.min, df.sec).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +---------------------------------------------------------------+\n",
      "        |r                                                              |\n",
      "        +---------------------------------------------------------------+\n",
      "        |100 years 11 months 8 days 12 hours 30 minutes 1.001001 seconds|\n",
      "        +---------------------------------------------------------------+\n",
      "        \n",
      "        >>> df.select(make_interval(\n",
      "        ...     df.year, df.month, df.week, df.day, df.hour, df.min).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +----------------------------------------------+\n",
      "        |r                                             |\n",
      "        +----------------------------------------------+\n",
      "        |100 years 11 months 8 days 12 hours 30 minutes|\n",
      "        +----------------------------------------------+\n",
      "        \n",
      "        >>> df.select(make_interval(\n",
      "        ...     df.year, df.month, df.week, df.day, df.hour).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +-----------------------------------+\n",
      "        |r                                  |\n",
      "        +-----------------------------------+\n",
      "        |100 years 11 months 8 days 12 hours|\n",
      "        +-----------------------------------+\n",
      "        \n",
      "        >>> df.select(make_interval(\n",
      "        ...     df.year, df.month, df.week, df.day).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +--------------------------+\n",
      "        |r                         |\n",
      "        +--------------------------+\n",
      "        |100 years 11 months 8 days|\n",
      "        +--------------------------+\n",
      "        \n",
      "        >>> df.select(make_interval(\n",
      "        ...     df.year, df.month, df.week).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +--------------------------+\n",
      "        |r                         |\n",
      "        +--------------------------+\n",
      "        |100 years 11 months 7 days|\n",
      "        +--------------------------+\n",
      "        \n",
      "        >>> df.select(make_interval(df.year, df.month).alias('r')).show(truncate=False)\n",
      "        +-------------------+\n",
      "        |r                  |\n",
      "        +-------------------+\n",
      "        |100 years 11 months|\n",
      "        +-------------------+\n",
      "        \n",
      "        >>> df.select(make_interval(df.year).alias('r')).show(truncate=False)\n",
      "        +---------+\n",
      "        |r        |\n",
      "        +---------+\n",
      "        |100 years|\n",
      "        +---------+\n",
      "    \n",
      "    make_timestamp(years: 'ColumnOrName', months: 'ColumnOrName', days: 'ColumnOrName', hours: 'ColumnOrName', mins: 'ColumnOrName', secs: 'ColumnOrName', timezone: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Create timestamp from years, months, days, hours, mins, secs and timezone fields.\n",
      "        The result data type is consistent with the value of configuration `spark.sql.timestampType`.\n",
      "        If the configuration `spark.sql.ansi.enabled` is false, the function returns NULL\n",
      "        on invalid inputs. Otherwise, it will throw an error instead.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        years : :class:`~pyspark.sql.Column` or str\n",
      "            the year to represent, from 1 to 9999\n",
      "        months : :class:`~pyspark.sql.Column` or str\n",
      "            the month-of-year to represent, from 1 (January) to 12 (December)\n",
      "        days : :class:`~pyspark.sql.Column` or str\n",
      "            the day-of-month to represent, from 1 to 31\n",
      "        hours : :class:`~pyspark.sql.Column` or str\n",
      "            the hour-of-day to represent, from 0 to 23\n",
      "        mins : :class:`~pyspark.sql.Column` or str\n",
      "            the minute-of-hour to represent, from 0 to 59\n",
      "        secs : :class:`~pyspark.sql.Column` or str\n",
      "            the second-of-minute and its micro-fraction to represent, from 0 to 60.\n",
      "            The value can be either an integer like 13 , or a fraction like 13.123.\n",
      "            If the sec argument equals to 60, the seconds field is set\n",
      "            to 0 and 1 minute is added to the final timestamp.\n",
      "        timezone : :class:`~pyspark.sql.Column` or str\n",
      "            the time zone identifier. For example, CET, UTC and etc.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([[2014, 12, 28, 6, 30, 45.887, 'CET']],\n",
      "        ...     [\"year\", \"month\", \"day\", \"hour\", \"min\", \"sec\", \"timezone\"])\n",
      "        >>> df.select(make_timestamp(\n",
      "        ...     df.year, df.month, df.day, df.hour, df.min, df.sec, df.timezone).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +-----------------------+\n",
      "        |r                      |\n",
      "        +-----------------------+\n",
      "        |2014-12-27 21:30:45.887|\n",
      "        +-----------------------+\n",
      "        \n",
      "        >>> df.select(make_timestamp(\n",
      "        ...     df.year, df.month, df.day, df.hour, df.min, df.sec).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +-----------------------+\n",
      "        |r                      |\n",
      "        +-----------------------+\n",
      "        |2014-12-28 06:30:45.887|\n",
      "        +-----------------------+\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    make_timestamp_ltz(years: 'ColumnOrName', months: 'ColumnOrName', days: 'ColumnOrName', hours: 'ColumnOrName', mins: 'ColumnOrName', secs: 'ColumnOrName', timezone: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Create the current timestamp with local time zone from years, months, days, hours, mins,\n",
      "        secs and timezone fields. If the configuration `spark.sql.ansi.enabled` is false,\n",
      "        the function returns NULL on invalid inputs. Otherwise, it will throw an error instead.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        years : :class:`~pyspark.sql.Column` or str\n",
      "            the year to represent, from 1 to 9999\n",
      "        months : :class:`~pyspark.sql.Column` or str\n",
      "            the month-of-year to represent, from 1 (January) to 12 (December)\n",
      "        days : :class:`~pyspark.sql.Column` or str\n",
      "            the day-of-month to represent, from 1 to 31\n",
      "        hours : :class:`~pyspark.sql.Column` or str\n",
      "            the hour-of-day to represent, from 0 to 23\n",
      "        mins : :class:`~pyspark.sql.Column` or str\n",
      "            the minute-of-hour to represent, from 0 to 59\n",
      "        secs : :class:`~pyspark.sql.Column` or str\n",
      "            the second-of-minute and its micro-fraction to represent, from 0 to 60.\n",
      "            The value can be either an integer like 13 , or a fraction like 13.123.\n",
      "            If the sec argument equals to 60, the seconds field is set\n",
      "            to 0 and 1 minute is added to the final timestamp.\n",
      "        timezone : :class:`~pyspark.sql.Column` or str\n",
      "            the time zone identifier. For example, CET, UTC and etc.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([[2014, 12, 28, 6, 30, 45.887, 'CET']],\n",
      "        ...     [\"year\", \"month\", \"day\", \"hour\", \"min\", \"sec\", \"timezone\"])\n",
      "        >>> df.select(sf.make_timestamp_ltz(\n",
      "        ...     df.year, df.month, df.day, df.hour, df.min, df.sec, df.timezone)\n",
      "        ... ).show(truncate=False)\n",
      "        +--------------------------------------------------------------+\n",
      "        |make_timestamp_ltz(year, month, day, hour, min, sec, timezone)|\n",
      "        +--------------------------------------------------------------+\n",
      "        |2014-12-27 21:30:45.887                                       |\n",
      "        +--------------------------------------------------------------+\n",
      "        \n",
      "        >>> df.select(sf.make_timestamp_ltz(\n",
      "        ...     df.year, df.month, df.day, df.hour, df.min, df.sec)\n",
      "        ... ).show(truncate=False)\n",
      "        +----------------------------------------------------+\n",
      "        |make_timestamp_ltz(year, month, day, hour, min, sec)|\n",
      "        +----------------------------------------------------+\n",
      "        |2014-12-28 06:30:45.887                             |\n",
      "        +----------------------------------------------------+\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    make_timestamp_ntz(years: 'ColumnOrName', months: 'ColumnOrName', days: 'ColumnOrName', hours: 'ColumnOrName', mins: 'ColumnOrName', secs: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Create local date-time from years, months, days, hours, mins, secs fields.\n",
      "        If the configuration `spark.sql.ansi.enabled` is false, the function returns NULL\n",
      "        on invalid inputs. Otherwise, it will throw an error instead.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        years : :class:`~pyspark.sql.Column` or str\n",
      "            the year to represent, from 1 to 9999\n",
      "        months : :class:`~pyspark.sql.Column` or str\n",
      "            the month-of-year to represent, from 1 (January) to 12 (December)\n",
      "        days : :class:`~pyspark.sql.Column` or str\n",
      "            the day-of-month to represent, from 1 to 31\n",
      "        hours : :class:`~pyspark.sql.Column` or str\n",
      "            the hour-of-day to represent, from 0 to 23\n",
      "        mins : :class:`~pyspark.sql.Column` or str\n",
      "            the minute-of-hour to represent, from 0 to 59\n",
      "        secs : :class:`~pyspark.sql.Column` or str\n",
      "            the second-of-minute and its micro-fraction to represent, from 0 to 60.\n",
      "            The value can be either an integer like 13 , or a fraction like 13.123.\n",
      "            If the sec argument equals to 60, the seconds field is set\n",
      "            to 0 and 1 minute is added to the final timestamp.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([[2014, 12, 28, 6, 30, 45.887]],\n",
      "        ...     [\"year\", \"month\", \"day\", \"hour\", \"min\", \"sec\"])\n",
      "        >>> df.select(sf.make_timestamp_ntz(\n",
      "        ...     df.year, df.month, df.day, df.hour, df.min, df.sec)\n",
      "        ... ).show(truncate=False)\n",
      "        +----------------------------------------------------+\n",
      "        |make_timestamp_ntz(year, month, day, hour, min, sec)|\n",
      "        +----------------------------------------------------+\n",
      "        |2014-12-28 06:30:45.887                             |\n",
      "        +----------------------------------------------------+\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    make_ym_interval(years: Optional[ForwardRef('ColumnOrName')] = None, months: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Make year-month interval from years, months.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        years : :class:`~pyspark.sql.Column` or str\n",
      "            the number of years, positive or negative\n",
      "        months : :class:`~pyspark.sql.Column` or str\n",
      "            the number of months, positive or negative\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([[2014, 12]], [\"year\", \"month\"])\n",
      "        >>> df.select(make_ym_interval(df.year, df.month).alias('r')).show(truncate=False)\n",
      "        +-------------------------------+\n",
      "        |r                              |\n",
      "        +-------------------------------+\n",
      "        |INTERVAL '2015-0' YEAR TO MONTH|\n",
      "        +-------------------------------+\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    map_concat(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')], Tuple[ForwardRef('ColumnOrName_'), ...]]) -> pyspark.sql.column.Column\n",
      "        Returns the union of all the given maps.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a map of merged entries from other maps.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_concat\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as map1, map(3, 'c') as map2\")\n",
      "        >>> df.select(map_concat(\"map1\", \"map2\").alias(\"map3\")).show(truncate=False)\n",
      "        +------------------------+\n",
      "        |map3                    |\n",
      "        +------------------------+\n",
      "        |{1 -> a, 2 -> b, 3 -> c}|\n",
      "        +------------------------+\n",
      "    \n",
      "    map_contains_key(col: 'ColumnOrName', value: Any) -> pyspark.sql.column.Column\n",
      "        Returns true if the map contains the key.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        value :\n",
      "            a literal value\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            True if key is in the map and False otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_contains_key\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_contains_key(\"data\", 1)).show()\n",
      "        +---------------------------------+\n",
      "        |array_contains(map_keys(data), 1)|\n",
      "        +---------------------------------+\n",
      "        |                             true|\n",
      "        +---------------------------------+\n",
      "        >>> df.select(map_contains_key(\"data\", -1)).show()\n",
      "        +----------------------------------+\n",
      "        |array_contains(map_keys(data), -1)|\n",
      "        +----------------------------------+\n",
      "        |                             false|\n",
      "        +----------------------------------+\n",
      "    \n",
      "    map_entries(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: Returns an unordered array of all entries in the given map.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of key value pairs as a struct type\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_entries\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df = df.select(map_entries(\"data\").alias(\"entries\"))\n",
      "        >>> df.show()\n",
      "        +----------------+\n",
      "        |         entries|\n",
      "        +----------------+\n",
      "        |[{1, a}, {2, b}]|\n",
      "        +----------------+\n",
      "        >>> df.printSchema()\n",
      "        root\n",
      "         |-- entries: array (nullable = false)\n",
      "         |    |-- element: struct (containsNull = false)\n",
      "         |    |    |-- key: integer (nullable = false)\n",
      "         |    |    |-- value: string (nullable = false)\n",
      "    \n",
      "    map_filter(col: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Returns a map whose key-value pairs satisfy a predicate.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a binary function ``(k: Column, v: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            filtered map.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, {\"foo\": 42.0, \"bar\": 1.0, \"baz\": 32.0})], (\"id\", \"data\"))\n",
      "        >>> row = df.select(map_filter(\n",
      "        ...     \"data\", lambda _, v: v > 30.0).alias(\"data_filtered\")\n",
      "        ... ).head()\n",
      "        >>> sorted(row[\"data_filtered\"].items())\n",
      "        [('baz', 32.0), ('foo', 42.0)]\n",
      "    \n",
      "    map_from_arrays(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Creates a new map from two arrays.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a set of keys. All elements should not be null\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a set of values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of map type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 5], ['a', 'b'])], ['k', 'v'])\n",
      "        >>> df = df.select(map_from_arrays(df.k, df.v).alias(\"col\"))\n",
      "        >>> df.show()\n",
      "        +----------------+\n",
      "        |             col|\n",
      "        +----------------+\n",
      "        |{2 -> a, 5 -> b}|\n",
      "        +----------------+\n",
      "        >>> df.printSchema()\n",
      "        root\n",
      "         |-- col: map (nullable = true)\n",
      "         |    |-- key: long\n",
      "         |    |-- value: string (valueContainsNull = true)\n",
      "    \n",
      "    map_from_entries(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: Converts an array of entries (key value struct types) to a map\n",
      "        of values.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a map created from the given array of entries.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_from_entries\n",
      "        >>> df = spark.sql(\"SELECT array(struct(1, 'a'), struct(2, 'b')) as data\")\n",
      "        >>> df.select(map_from_entries(\"data\").alias(\"map\")).show()\n",
      "        +----------------+\n",
      "        |             map|\n",
      "        +----------------+\n",
      "        |{1 -> a, 2 -> b}|\n",
      "        +----------------+\n",
      "    \n",
      "    map_keys(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: Returns an unordered array containing the keys of the map.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            keys of the map as an array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_keys\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_keys(\"data\").alias(\"keys\")).show()\n",
      "        +------+\n",
      "        |  keys|\n",
      "        +------+\n",
      "        |[1, 2]|\n",
      "        +------+\n",
      "    \n",
      "    map_values(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: Returns an unordered array containing the values of the map.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            values of the map as an array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_values\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_values(\"data\").alias(\"values\")).show()\n",
      "        +------+\n",
      "        |values|\n",
      "        +------+\n",
      "        |[a, b]|\n",
      "        +------+\n",
      "    \n",
      "    map_zip_with(col1: 'ColumnOrName', col2: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Merge two given maps, key-wise into a single map using a function.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of the first column or expression\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of the second column or expression\n",
      "        f : function\n",
      "            a ternary function ``(k: Column, v1: Column, v2: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            zipped map where entries are calculated by applying given function to each\n",
      "            pair of arguments.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (1, {\"IT\": 24.0, \"SALES\": 12.00}, {\"IT\": 2.0, \"SALES\": 1.4})],\n",
      "        ...     (\"id\", \"base\", \"ratio\")\n",
      "        ... )\n",
      "        >>> row = df.select(map_zip_with(\n",
      "        ...     \"base\", \"ratio\", lambda k, v1, v2: round(v1 * v2, 2)).alias(\"updated_data\")\n",
      "        ... ).head()\n",
      "        >>> sorted(row[\"updated_data\"].items())\n",
      "        [('IT', 48.0), ('SALES', 16.8)]\n",
      "    \n",
      "    mask(col: 'ColumnOrName', upperChar: Optional[ForwardRef('ColumnOrName')] = None, lowerChar: Optional[ForwardRef('ColumnOrName')] = None, digitChar: Optional[ForwardRef('ColumnOrName')] = None, otherChar: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Masks the given string value. This can be useful for creating copies of tables with sensitive\n",
      "        information removed.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col: :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        upperChar: :class:`~pyspark.sql.Column` or str\n",
      "            character to replace upper-case characters with. Specify NULL to retain original character.\n",
      "        lowerChar: :class:`~pyspark.sql.Column` or str\n",
      "            character to replace lower-case characters with. Specify NULL to retain original character.\n",
      "        digitChar: :class:`~pyspark.sql.Column` or str\n",
      "            character to replace digit characters with. Specify NULL to retain original character.\n",
      "        otherChar: :class:`~pyspark.sql.Column` or str\n",
      "            character to replace all other characters with. Specify NULL to retain original character.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"AbCD123-@$#\",), (\"abcd-EFGH-8765-4321\",)], ['data'])\n",
      "        >>> df.select(mask(df.data).alias('r')).collect()\n",
      "        [Row(r='XxXXnnn-@$#'), Row(r='xxxx-XXXX-nnnn-nnnn')]\n",
      "        >>> df.select(mask(df.data, lit('Y')).alias('r')).collect()\n",
      "        [Row(r='YxYYnnn-@$#'), Row(r='xxxx-YYYY-nnnn-nnnn')]\n",
      "        >>> df.select(mask(df.data, lit('Y'), lit('y')).alias('r')).collect()\n",
      "        [Row(r='YyYYnnn-@$#'), Row(r='yyyy-YYYY-nnnn-nnnn')]\n",
      "        >>> df.select(mask(df.data, lit('Y'), lit('y'), lit('d')).alias('r')).collect()\n",
      "        [Row(r='YyYYddd-@$#'), Row(r='yyyy-YYYY-dddd-dddd')]\n",
      "        >>> df.select(mask(df.data, lit('Y'), lit('y'), lit('d'), lit('*')).alias('r')).collect()\n",
      "        [Row(r='YyYYddd****'), Row(r='yyyy*YYYY*dddd*dddd')]\n",
      "    \n",
      "    max(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the maximum value of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(10)\n",
      "        >>> df.select(max(col(\"id\"))).show()\n",
      "        +-------+\n",
      "        |max(id)|\n",
      "        +-------+\n",
      "        |      9|\n",
      "        +-------+\n",
      "    \n",
      "    max_by(col: 'ColumnOrName', ord: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the value associated with the maximum value of ord.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        ord : :class:`~pyspark.sql.Column` or str\n",
      "            column to be maximized\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value associated with the maximum value of ord.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
      "        ...     (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n",
      "        ...     schema=(\"course\", \"year\", \"earnings\"))\n",
      "        >>> df.groupby(\"course\").agg(max_by(\"year\", \"earnings\")).show()\n",
      "        +------+----------------------+\n",
      "        |course|max_by(year, earnings)|\n",
      "        +------+----------------------+\n",
      "        |  Java|                  2013|\n",
      "        |dotNET|                  2013|\n",
      "        +------+----------------------+\n",
      "    \n",
      "    md5(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calculates the MD5 digest and returns the value as a 32 character hex string.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(md5('a').alias('hash')).collect()\n",
      "        [Row(hash='902fbdd2b1df0c4f70b4a5d23525e932')]\n",
      "    \n",
      "    mean(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the average of the values in a group.\n",
      "        An alias of :func:`avg`.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(10)\n",
      "        >>> df.select(mean(df.id)).show()\n",
      "        +-------+\n",
      "        |avg(id)|\n",
      "        +-------+\n",
      "        |    4.5|\n",
      "        +-------+\n",
      "    \n",
      "    median(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the median of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the median of the values in a group.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
      "        ...     (\"Java\", 2012, 22000), (\"dotNET\", 2012, 10000),\n",
      "        ...     (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n",
      "        ...     schema=(\"course\", \"year\", \"earnings\"))\n",
      "        >>> df.groupby(\"course\").agg(median(\"earnings\")).show()\n",
      "        +------+----------------+\n",
      "        |course|median(earnings)|\n",
      "        +------+----------------+\n",
      "        |  Java|         22000.0|\n",
      "        |dotNET|         10000.0|\n",
      "        +------+----------------+\n",
      "    \n",
      "    min(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the minimum value of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(10)\n",
      "        >>> df.select(min(df.id)).show()\n",
      "        +-------+\n",
      "        |min(id)|\n",
      "        +-------+\n",
      "        |      0|\n",
      "        +-------+\n",
      "    \n",
      "    min_by(col: 'ColumnOrName', ord: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the value associated with the minimum value of ord.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        ord : :class:`~pyspark.sql.Column` or str\n",
      "            column to be minimized\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value associated with the minimum value of ord.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
      "        ...     (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n",
      "        ...     schema=(\"course\", \"year\", \"earnings\"))\n",
      "        >>> df.groupby(\"course\").agg(min_by(\"year\", \"earnings\")).show()\n",
      "        +------+----------------------+\n",
      "        |course|min_by(year, earnings)|\n",
      "        +------+----------------------+\n",
      "        |  Java|                  2012|\n",
      "        |dotNET|                  2012|\n",
      "        +------+----------------------+\n",
      "    \n",
      "    minute(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the minutes of a given timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            minutes part of the timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame([(datetime.datetime(2015, 4, 8, 13, 8, 15),)], ['ts'])\n",
      "        >>> df.select(minute('ts').alias('minute')).collect()\n",
      "        [Row(minute=8)]\n",
      "    \n",
      "    mode(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the most frequent value in a group.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the most frequent value in a group.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
      "        ...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
      "        ...     (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n",
      "        ...     schema=(\"course\", \"year\", \"earnings\"))\n",
      "        >>> df.groupby(\"course\").agg(mode(\"year\")).show()\n",
      "        +------+----------+\n",
      "        |course|mode(year)|\n",
      "        +------+----------+\n",
      "        |  Java|      2012|\n",
      "        |dotNET|      2012|\n",
      "        +------+----------+\n",
      "    \n",
      "    monotonically_increasing_id() -> pyspark.sql.column.Column\n",
      "        A column that generates monotonically increasing 64-bit integers.\n",
      "        \n",
      "        The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.\n",
      "        The current implementation puts the partition ID in the upper 31 bits, and the record number\n",
      "        within each partition in the lower 33 bits. The assumption is that the data frame has\n",
      "        less than 1 billion partitions, and each partition has less than 8 billion records.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because its result depends on partition IDs.\n",
      "        \n",
      "        As an example, consider a :class:`DataFrame` with two partitions, each with 3 records.\n",
      "        This expression would return the following IDs:\n",
      "        0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            last value of the group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import functions as sf\n",
      "        >>> spark.range(0, 10, 1, 2).select(sf.monotonically_increasing_id()).show()\n",
      "        +-----------------------------+\n",
      "        |monotonically_increasing_id()|\n",
      "        +-----------------------------+\n",
      "        |                            0|\n",
      "        |                            1|\n",
      "        |                            2|\n",
      "        |                            3|\n",
      "        |                            4|\n",
      "        |                   8589934592|\n",
      "        |                   8589934593|\n",
      "        |                   8589934594|\n",
      "        |                   8589934595|\n",
      "        |                   8589934596|\n",
      "        +-----------------------------+\n",
      "    \n",
      "    month(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the month of a given date/timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            month part of the date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(month('dt').alias('month')).collect()\n",
      "        [Row(month=4)]\n",
      "    \n",
      "    months(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Partition transform function: A transform for timestamps and dates\n",
      "        to partition data into months.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date or timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            data partitioned by months.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(\n",
      "        ...     months(\"ts\")\n",
      "        ... ).createOrReplace()  # doctest: +SKIP\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    months_between(date1: 'ColumnOrName', date2: 'ColumnOrName', roundOff: bool = True) -> pyspark.sql.column.Column\n",
      "        Returns number of months between dates date1 and date2.\n",
      "        If date1 is later than date2, then the result is positive.\n",
      "        A whole number is returned if both inputs have the same day of month or both are the last day\n",
      "        of their respective months. Otherwise, the difference is calculated assuming 31 days per month.\n",
      "        The result is rounded off to 8 digits unless `roundOff` is set to `False`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        date1 : :class:`~pyspark.sql.Column` or str\n",
      "            first date column.\n",
      "        date2 : :class:`~pyspark.sql.Column` or str\n",
      "            second date column.\n",
      "        roundOff : bool, optional\n",
      "            whether to round (to 8 digits) the final value or not (default: True).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            number of months between two dates.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n",
      "        >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()\n",
      "        [Row(months=3.94959677)]\n",
      "        >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()\n",
      "        [Row(months=3.9495967741935485)]\n",
      "    \n",
      "    named_struct(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Creates a struct with the given field names and values.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            list of columns to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, 2, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(named_struct(lit('x'), df.a, lit('y'), df.b).alias('r')).collect()\n",
      "        [Row(r=Row(x=1, y=2))]\n",
      "    \n",
      "    nanvl(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns col1 if it is not NaN, or col2 if col1 is NaN.\n",
      "        \n",
      "        Both inputs should be floating point columns (:class:`DoubleType` or :class:`FloatType`).\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            first column to check.\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            second column to return if first is NaN.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value from first column or second if first is NaN .\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
      "        >>> df.select(nanvl(\"a\", \"b\").alias(\"r1\"), nanvl(df.a, df.b).alias(\"r2\")).collect()\n",
      "        [Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]\n",
      "    \n",
      "    negate = negative(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the negative value.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to calculate negative value for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            negative value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(3).select(sf.negative(\"id\")).show()\n",
      "        +------------+\n",
      "        |negative(id)|\n",
      "        +------------+\n",
      "        |           0|\n",
      "        |          -1|\n",
      "        |          -2|\n",
      "        +------------+\n",
      "    \n",
      "    negative(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the negative value.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to calculate negative value for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            negative value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(3).select(sf.negative(\"id\")).show()\n",
      "        +------------+\n",
      "        |negative(id)|\n",
      "        +------------+\n",
      "        |           0|\n",
      "        |          -1|\n",
      "        |          -2|\n",
      "        +------------+\n",
      "    \n",
      "    next_day(date: 'ColumnOrName', dayOfWeek: str) -> pyspark.sql.column.Column\n",
      "        Returns the first date which is later than the value of the date column\n",
      "        based on second `week day` argument.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        date : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        dayOfWeek : str\n",
      "            day of the week, case-insensitive, accepts:\n",
      "                \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column of computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-07-27',)], ['d'])\n",
      "        >>> df.select(next_day(df.d, 'Sun').alias('date')).collect()\n",
      "        [Row(date=datetime.date(2015, 8, 2))]\n",
      "    \n",
      "    now() -> pyspark.sql.column.Column\n",
      "        Returns the current timestamp at the start of query evaluation.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            current timestamp at the start of query evaluation.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(now()).show(truncate=False) # doctest: +SKIP\n",
      "        +-----------------------+\n",
      "        |now()    |\n",
      "        +-----------------------+\n",
      "        |2022-08-26 21:23:22.716|\n",
      "        +-----------------------+\n",
      "    \n",
      "    nth_value(col: 'ColumnOrName', offset: int, ignoreNulls: Optional[bool] = False) -> pyspark.sql.column.Column\n",
      "        Window function: returns the value that is the `offset`\\th row of the window frame\n",
      "        (counting from 1), and `null` if the size of window frame is less than `offset` rows.\n",
      "        \n",
      "        It will return the `offset`\\th non-null value it sees when `ignoreNulls` is set to\n",
      "        true. If all values are null, then null is returned.\n",
      "        \n",
      "        This is equivalent to the nth_value function in SQL.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        offset : int\n",
      "            number of row to use as the value\n",
      "        ignoreNulls : bool, optional\n",
      "            indicates the Nth value should skip null in the\n",
      "            determination of which row to use\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value of nth row.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> df = spark.createDataFrame([(\"a\", 1),\n",
      "        ...                             (\"a\", 2),\n",
      "        ...                             (\"a\", 3),\n",
      "        ...                             (\"b\", 8),\n",
      "        ...                             (\"b\", 2)], [\"c1\", \"c2\"])\n",
      "        >>> df.show()\n",
      "        +---+---+\n",
      "        | c1| c2|\n",
      "        +---+---+\n",
      "        |  a|  1|\n",
      "        |  a|  2|\n",
      "        |  a|  3|\n",
      "        |  b|  8|\n",
      "        |  b|  2|\n",
      "        +---+---+\n",
      "        >>> w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
      "        >>> df.withColumn(\"nth_value\", nth_value(\"c2\", 1).over(w)).show()\n",
      "        +---+---+---------+\n",
      "        | c1| c2|nth_value|\n",
      "        +---+---+---------+\n",
      "        |  a|  1|        1|\n",
      "        |  a|  2|        1|\n",
      "        |  a|  3|        1|\n",
      "        |  b|  2|        2|\n",
      "        |  b|  8|        2|\n",
      "        +---+---+---------+\n",
      "        >>> df.withColumn(\"nth_value\", nth_value(\"c2\", 2).over(w)).show()\n",
      "        +---+---+---------+\n",
      "        | c1| c2|nth_value|\n",
      "        +---+---+---------+\n",
      "        |  a|  1|     NULL|\n",
      "        |  a|  2|        2|\n",
      "        |  a|  3|        2|\n",
      "        |  b|  2|     NULL|\n",
      "        |  b|  8|        8|\n",
      "        +---+---+---------+\n",
      "    \n",
      "    ntile(n: int) -> pyspark.sql.column.Column\n",
      "        Window function: returns the ntile group id (from 1 to `n` inclusive)\n",
      "        in an ordered window partition. For example, if `n` is 4, the first\n",
      "        quarter of the rows will get value 1, the second quarter will get 2,\n",
      "        the third quarter will get 3, and the last quarter will get 4.\n",
      "        \n",
      "        This is equivalent to the NTILE function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            an integer\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            portioned group id.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> df = spark.createDataFrame([(\"a\", 1),\n",
      "        ...                             (\"a\", 2),\n",
      "        ...                             (\"a\", 3),\n",
      "        ...                             (\"b\", 8),\n",
      "        ...                             (\"b\", 2)], [\"c1\", \"c2\"])\n",
      "        >>> df.show()\n",
      "        +---+---+\n",
      "        | c1| c2|\n",
      "        +---+---+\n",
      "        |  a|  1|\n",
      "        |  a|  2|\n",
      "        |  a|  3|\n",
      "        |  b|  8|\n",
      "        |  b|  2|\n",
      "        +---+---+\n",
      "        >>> w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
      "        >>> df.withColumn(\"ntile\", ntile(2).over(w)).show()\n",
      "        +---+---+-----+\n",
      "        | c1| c2|ntile|\n",
      "        +---+---+-----+\n",
      "        |  a|  1|    1|\n",
      "        |  a|  2|    1|\n",
      "        |  a|  3|    2|\n",
      "        |  b|  2|    1|\n",
      "        |  b|  8|    2|\n",
      "        +---+---+-----+\n",
      "    \n",
      "    nullif(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns null if `col1` equals to `col2`, or `col1` otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None, None,), (1, 9,)], [\"a\", \"b\"])\n",
      "        >>> df.select(nullif(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=None), Row(r=1)]\n",
      "    \n",
      "    nvl(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns `col2` if `col1` is null, or `col1` otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None, 8,), (1, 9,)], [\"a\", \"b\"])\n",
      "        >>> df.select(nvl(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=8), Row(r=1)]\n",
      "    \n",
      "    nvl2(col1: 'ColumnOrName', col2: 'ColumnOrName', col3: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns `col2` if `col1` is not null, or `col3` otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "        col3 : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None, 8, 6,), (1, 9, 9,)], [\"a\", \"b\", \"c\"])\n",
      "        >>> df.select(nvl2(df.a, df.b, df.c).alias('r')).collect()\n",
      "        [Row(r=6), Row(r=9)]\n",
      "    \n",
      "    octet_length(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calculates the byte length for the specified string column.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Source column or strings\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            Byte length of the col\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import octet_length\n",
      "        >>> spark.createDataFrame([('cat',), ( '🐈',)], ['cat']) \\\n",
      "        ...      .select(octet_length('cat')).collect()\n",
      "            [Row(octet_length(cat)=3), Row(octet_length(cat)=4)]\n",
      "    \n",
      "    overlay(src: 'ColumnOrName', replace: 'ColumnOrName', pos: Union[ForwardRef('ColumnOrName'), int], len: Union[ForwardRef('ColumnOrName'), int] = -1) -> pyspark.sql.column.Column\n",
      "        Overlay the specified portion of `src` with `replace`,\n",
      "        starting from byte position `pos` of `src` and proceeding for `len` bytes.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        src : :class:`~pyspark.sql.Column` or str\n",
      "            column name or column containing the string that will be replaced\n",
      "        replace : :class:`~pyspark.sql.Column` or str\n",
      "            column name or column containing the substitution string\n",
      "        pos : :class:`~pyspark.sql.Column` or str or int\n",
      "            column name, column, or int containing the starting position in src\n",
      "        len : :class:`~pyspark.sql.Column` or str or int, optional\n",
      "            column name, column, or int containing the number of bytes to replace in src\n",
      "            string by 'replace' defaults to -1, which represents the length of the 'replace' string\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string with replaced values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"SPARK_SQL\", \"CORE\")], (\"x\", \"y\"))\n",
      "        >>> df.select(overlay(\"x\", \"y\", 7).alias(\"overlayed\")).collect()\n",
      "        [Row(overlayed='SPARK_CORE')]\n",
      "        >>> df.select(overlay(\"x\", \"y\", 7, 0).alias(\"overlayed\")).collect()\n",
      "        [Row(overlayed='SPARK_CORESQL')]\n",
      "        >>> df.select(overlay(\"x\", \"y\", 7, 2).alias(\"overlayed\")).collect()\n",
      "        [Row(overlayed='SPARK_COREL')]\n",
      "    \n",
      "    parse_url(url: 'ColumnOrName', partToExtract: 'ColumnOrName', key: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Extracts a part from a URL.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        url : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string.\n",
      "        partToExtract : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string, the path.\n",
      "        key : :class:`~pyspark.sql.Column` or str, optional\n",
      "            A column of string, the key.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(\"http://spark.apache.org/path?query=1\", \"QUERY\", \"query\",)],\n",
      "        ...     [\"a\", \"b\", \"c\"]\n",
      "        ... )\n",
      "        >>> df.select(parse_url(df.a, df.b, df.c).alias('r')).collect()\n",
      "        [Row(r='1')]\n",
      "        \n",
      "        >>> df.select(parse_url(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r='query=1')]\n",
      "    \n",
      "    percent_rank() -> pyspark.sql.column.Column\n",
      "        Window function: returns the relative rank (i.e. percentile) of rows within a window partition.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for calculating relative rank.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window, types\n",
      "        >>> df = spark.createDataFrame([1, 1, 2, 3, 3, 4], types.IntegerType())\n",
      "        >>> w = Window.orderBy(\"value\")\n",
      "        >>> df.withColumn(\"pr\", percent_rank().over(w)).show()\n",
      "        +-----+---+\n",
      "        |value| pr|\n",
      "        +-----+---+\n",
      "        |    1|0.0|\n",
      "        |    1|0.0|\n",
      "        |    2|0.4|\n",
      "        |    3|0.6|\n",
      "        |    3|0.6|\n",
      "        |    4|1.0|\n",
      "        +-----+---+\n",
      "    \n",
      "    percentile(col: 'ColumnOrName', percentage: Union[pyspark.sql.column.Column, float, List[float], Tuple[float]], frequency: Union[pyspark.sql.column.Column, int] = 1) -> pyspark.sql.column.Column\n",
      "        Returns the exact percentile(s) of numeric column `expr` at the given percentage(s)\n",
      "        with value range in [0.0, 1.0].\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str input column.\n",
      "        percentage : :class:`~pyspark.sql.Column`, float, list of floats or tuple of floats\n",
      "            percentage in decimal (must be between 0.0 and 1.0).\n",
      "        frequency : :class:`~pyspark.sql.Column` or int is a positive numeric literal which\n",
      "            controls frequency.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the exact `percentile` of the numeric column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> key = (col(\"id\") % 3).alias(\"key\")\n",
      "        >>> value = (randn(42) + key * 10).alias(\"value\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(key, value)\n",
      "        >>> df.select(\n",
      "        ...     percentile(\"value\", [0.25, 0.5, 0.75], lit(1)).alias(\"quantiles\")\n",
      "        ... ).show()\n",
      "        +--------------------+\n",
      "        |           quantiles|\n",
      "        +--------------------+\n",
      "        |[0.74419914941216...|\n",
      "        +--------------------+\n",
      "        \n",
      "        >>> df.groupBy(\"key\").agg(\n",
      "        ...     percentile(\"value\", 0.5, lit(1)).alias(\"median\")\n",
      "        ... ).show()\n",
      "        +---+--------------------+\n",
      "        |key|              median|\n",
      "        +---+--------------------+\n",
      "        |  0|-0.03449962216667901|\n",
      "        |  1|   9.990389751837329|\n",
      "        |  2|  19.967859769284075|\n",
      "        +---+--------------------+\n",
      "    \n",
      "    percentile_approx(col: 'ColumnOrName', percentage: Union[pyspark.sql.column.Column, float, List[float], Tuple[float]], accuracy: Union[pyspark.sql.column.Column, float] = 10000) -> pyspark.sql.column.Column\n",
      "        Returns the approximate `percentile` of the numeric column `col` which is the smallest value\n",
      "        in the ordered `col` values (sorted from least to greatest) such that no more than `percentage`\n",
      "        of `col` values is less than the value or equal to that value.\n",
      "        \n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column.\n",
      "        percentage : :class:`~pyspark.sql.Column`, float, list of floats or tuple of floats\n",
      "            percentage in decimal (must be between 0.0 and 1.0).\n",
      "            When percentage is an array, each value of the percentage array must be between 0.0 and 1.0.\n",
      "            In this case, returns the approximate percentile array of column col\n",
      "            at the given percentage array.\n",
      "        accuracy : :class:`~pyspark.sql.Column` or float\n",
      "            is a positive numeric literal which controls approximation accuracy\n",
      "            at the cost of memory. Higher value of accuracy yields better accuracy,\n",
      "            1.0/accuracy is the relative error of the approximation. (default: 10000).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            approximate `percentile` of the numeric column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> key = (col(\"id\") % 3).alias(\"key\")\n",
      "        >>> value = (randn(42) + key * 10).alias(\"value\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(key, value)\n",
      "        >>> df.select(\n",
      "        ...     percentile_approx(\"value\", [0.25, 0.5, 0.75], 1000000).alias(\"quantiles\")\n",
      "        ... ).printSchema()\n",
      "        root\n",
      "         |-- quantiles: array (nullable = true)\n",
      "         |    |-- element: double (containsNull = false)\n",
      "        \n",
      "        >>> df.groupBy(\"key\").agg(\n",
      "        ...     percentile_approx(\"value\", 0.5, lit(1000000)).alias(\"median\")\n",
      "        ... ).printSchema()\n",
      "        root\n",
      "         |-- key: long (nullable = true)\n",
      "         |-- median: double (nullable = true)\n",
      "    \n",
      "    pi() -> pyspark.sql.column.Column\n",
      "        Returns Pi.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.range(1).select(pi()).show()\n",
      "        +-----------------+\n",
      "        |             PI()|\n",
      "        +-----------------+\n",
      "        |3.141592653589793|\n",
      "        +-----------------+\n",
      "    \n",
      "    pmod(dividend: Union[ForwardRef('ColumnOrName'), float], divisor: Union[ForwardRef('ColumnOrName'), float]) -> pyspark.sql.column.Column\n",
      "        Returns the positive value of dividend mod divisor.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        dividend : str, :class:`~pyspark.sql.Column` or float\n",
      "            the column that contains dividend, or the specified dividend value\n",
      "        divisor : str, :class:`~pyspark.sql.Column` or float\n",
      "            the column that contains divisor, or the specified divisor value\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            positive value of dividend mod divisor.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import pmod\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (1.0, float('nan')), (float('nan'), 2.0), (10.0, 3.0),\n",
      "        ...     (float('nan'), float('nan')), (-3.0, 4.0), (-10.0, 3.0),\n",
      "        ...     (-5.0, -6.0), (7.0, -8.0), (1.0, 2.0)],\n",
      "        ...     (\"a\", \"b\"))\n",
      "        >>> df.select(pmod(\"a\", \"b\")).show()\n",
      "        +----------+\n",
      "        |pmod(a, b)|\n",
      "        +----------+\n",
      "        |       NaN|\n",
      "        |       NaN|\n",
      "        |       1.0|\n",
      "        |       NaN|\n",
      "        |       1.0|\n",
      "        |       2.0|\n",
      "        |      -5.0|\n",
      "        |       7.0|\n",
      "        |       1.0|\n",
      "        +----------+\n",
      "    \n",
      "    posexplode(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new row for each element with position in the given array or map.\n",
      "        Uses the default column name `pos` for position, and `col` for elements in the\n",
      "        array and `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            one row per array item or map key value including positions as a separate column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "        >>> df.select(posexplode(df.intlist)).collect()\n",
      "        [Row(pos=0, col=1), Row(pos=1, col=2), Row(pos=2, col=3)]\n",
      "        \n",
      "        >>> df.select(posexplode(df.mapfield)).show()\n",
      "        +---+---+-----+\n",
      "        |pos|key|value|\n",
      "        +---+---+-----+\n",
      "        |  0|  a|    b|\n",
      "        +---+---+-----+\n",
      "    \n",
      "    posexplode_outer(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new row for each element with position in the given array or map.\n",
      "        Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced.\n",
      "        Uses the default column name `pos` for position, and `col` for elements in the\n",
      "        array and `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            one row per array item or map key value including positions as a separate column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
      "        ...     (\"id\", \"an_array\", \"a_map\")\n",
      "        ... )\n",
      "        >>> df.select(\"id\", \"an_array\", posexplode_outer(\"a_map\")).show()\n",
      "        +---+----------+----+----+-----+\n",
      "        | id|  an_array| pos| key|value|\n",
      "        +---+----------+----+----+-----+\n",
      "        |  1|[foo, bar]|   0|   x|  1.0|\n",
      "        |  2|        []|NULL|NULL| NULL|\n",
      "        |  3|      NULL|NULL|NULL| NULL|\n",
      "        +---+----------+----+----+-----+\n",
      "        >>> df.select(\"id\", \"a_map\", posexplode_outer(\"an_array\")).show()\n",
      "        +---+----------+----+----+\n",
      "        | id|     a_map| pos| col|\n",
      "        +---+----------+----+----+\n",
      "        |  1|{x -> 1.0}|   0| foo|\n",
      "        |  1|{x -> 1.0}|   1| bar|\n",
      "        |  2|        {}|NULL|NULL|\n",
      "        |  3|      NULL|NULL|NULL|\n",
      "        +---+----------+----+----+\n",
      "    \n",
      "    position(substr: 'ColumnOrName', str: 'ColumnOrName', start: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Returns the position of the first occurrence of `substr` in `str` after position `start`.\n",
      "        The given `start` and return value are 1-based.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        substr : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string, substring.\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string.\n",
      "        start : :class:`~pyspark.sql.Column` or str, optional\n",
      "            A column of string, start position.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"bar\", \"foobarbar\", 5,)], [\"a\", \"b\", \"c\"]\n",
      "        ... ).select(sf.position(\"a\", \"b\", \"c\")).show()\n",
      "        +-----------------+\n",
      "        |position(a, b, c)|\n",
      "        +-----------------+\n",
      "        |                7|\n",
      "        +-----------------+\n",
      "        \n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"bar\", \"foobarbar\", 5,)], [\"a\", \"b\", \"c\"]\n",
      "        ... ).select(sf.position(\"a\", \"b\")).show()\n",
      "        +-----------------+\n",
      "        |position(a, b, 1)|\n",
      "        +-----------------+\n",
      "        |                4|\n",
      "        +-----------------+\n",
      "    \n",
      "    positive(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the value.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input value column.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(-1,), (0,), (1,)], ['v'])\n",
      "        >>> df.select(positive(\"v\").alias(\"p\")).show()\n",
      "        +---+\n",
      "        |  p|\n",
      "        +---+\n",
      "        | -1|\n",
      "        |  0|\n",
      "        |  1|\n",
      "        +---+\n",
      "    \n",
      "    pow(col1: Union[ForwardRef('ColumnOrName'), float], col2: Union[ForwardRef('ColumnOrName'), float]) -> pyspark.sql.column.Column\n",
      "        Returns the value of the first argument raised to the power of the second argument.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str, :class:`~pyspark.sql.Column` or float\n",
      "            the base number.\n",
      "        col2 : str, :class:`~pyspark.sql.Column` or float\n",
      "            the exponent number.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the base rased to the power the argument.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(pow(lit(3), lit(2))).first()\n",
      "        Row(POWER(3, 2)=9.0)\n",
      "    \n",
      "    power = pow(col1: Union[ForwardRef('ColumnOrName'), float], col2: Union[ForwardRef('ColumnOrName'), float]) -> pyspark.sql.column.Column\n",
      "        Returns the value of the first argument raised to the power of the second argument.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str, :class:`~pyspark.sql.Column` or float\n",
      "            the base number.\n",
      "        col2 : str, :class:`~pyspark.sql.Column` or float\n",
      "            the exponent number.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the base rased to the power the argument.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(pow(lit(3), lit(2))).first()\n",
      "        Row(POWER(3, 2)=9.0)\n",
      "    \n",
      "    printf(format: 'ColumnOrName', *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Formats the arguments in printf-style and returns the result as a string column.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        format : :class:`~pyspark.sql.Column` or str\n",
      "            string that can contain embedded format tags and used as result column's value\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s to be used in formatting\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"aa%d%s\", 123, \"cc\",)], [\"a\", \"b\", \"c\"]\n",
      "        ... ).select(sf.printf(\"a\", \"b\", \"c\")).show()\n",
      "        +---------------+\n",
      "        |printf(a, b, c)|\n",
      "        +---------------+\n",
      "        |        aa123cc|\n",
      "        +---------------+\n",
      "    \n",
      "    product(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the product of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : str, :class:`Column`\n",
      "            column containing values to be multiplied together\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1, 10).toDF('x').withColumn('mod3', col('x') % 3)\n",
      "        >>> prods = df.groupBy('mod3').agg(product('x').alias('product'))\n",
      "        >>> prods.orderBy('mod3').show()\n",
      "        +----+-------+\n",
      "        |mod3|product|\n",
      "        +----+-------+\n",
      "        |   0|  162.0|\n",
      "        |   1|   28.0|\n",
      "        |   2|   80.0|\n",
      "        +----+-------+\n",
      "    \n",
      "    quarter(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the quarter of a given date/timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            quarter of the date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(quarter('dt').alias('quarter')).collect()\n",
      "        [Row(quarter=2)]\n",
      "    \n",
      "    radians(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Converts an angle measured in degrees to an approximately equivalent angle\n",
      "        measured in radians.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in degrees\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            angle in radians, as if computed by `java.lang.Math.toRadians()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(radians(lit(180))).first()\n",
      "        Row(RADIANS(180)=3.14159...)\n",
      "    \n",
      "    raise_error(errMsg: Union[pyspark.sql.column.Column, str]) -> pyspark.sql.column.Column\n",
      "        Throws an exception with the provided error message.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        errMsg : :class:`~pyspark.sql.Column` or str\n",
      "            A Python string literal or column containing the error message\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            throws an error with specified message.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(raise_error(\"My error message\")).show() # doctest: +SKIP\n",
      "        ...\n",
      "        java.lang.RuntimeException: My error message\n",
      "        ...\n",
      "    \n",
      "    rand(seed: Optional[int] = None) -> pyspark.sql.column.Column\n",
      "        Generates a random column with independent and identically distributed (i.i.d.) samples\n",
      "        uniformly distributed in [0.0, 1.0).\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic in general case.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        seed : int (default: None)\n",
      "            seed value for random generator.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            random values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import functions as sf\n",
      "        >>> spark.range(0, 2, 1, 1).withColumn('rand', sf.rand(seed=42) * 3).show()\n",
      "        +---+------------------+\n",
      "        | id|              rand|\n",
      "        +---+------------------+\n",
      "        |  0|1.8575681106759028|\n",
      "        |  1|1.5288056527339444|\n",
      "        +---+------------------+\n",
      "    \n",
      "    randn(seed: Optional[int] = None) -> pyspark.sql.column.Column\n",
      "        Generates a column with independent and identically distributed (i.i.d.) samples from\n",
      "        the standard normal distribution.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic in general case.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        seed : int (default: None)\n",
      "            seed value for random generator.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            random values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import functions as sf\n",
      "        >>> spark.range(0, 2, 1, 1).withColumn('randn', sf.randn(seed=42)).show()\n",
      "        +---+------------------+\n",
      "        | id|             randn|\n",
      "        +---+------------------+\n",
      "        |  0| 2.384479054241165|\n",
      "        |  1|0.1920934041293524|\n",
      "        +---+------------------+\n",
      "    \n",
      "    rank() -> pyspark.sql.column.Column\n",
      "        Window function: returns the rank of rows within a window partition.\n",
      "        \n",
      "        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
      "        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
      "        and had three people tie for second place, you would say that all three were in second\n",
      "        place and that the next person came in third. Rank would give me sequential numbers, making\n",
      "        the person that came in third place (after the ties) would register as coming in fifth.\n",
      "        \n",
      "        This is equivalent to the RANK function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for calculating ranks.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window, types\n",
      "        >>> df = spark.createDataFrame([1, 1, 2, 3, 3, 4], types.IntegerType())\n",
      "        >>> w = Window.orderBy(\"value\")\n",
      "        >>> df.withColumn(\"drank\", rank().over(w)).show()\n",
      "        +-----+-----+\n",
      "        |value|drank|\n",
      "        +-----+-----+\n",
      "        |    1|    1|\n",
      "        |    1|    1|\n",
      "        |    2|    3|\n",
      "        |    3|    4|\n",
      "        |    3|    4|\n",
      "        |    4|    6|\n",
      "        +-----+-----+\n",
      "    \n",
      "    reduce(col: 'ColumnOrName', initialValue: 'ColumnOrName', merge: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column], finish: Optional[Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column]] = None) -> pyspark.sql.column.Column\n",
      "        Applies a binary operator to an initial state and all elements in the array,\n",
      "        and reduces this to a single state. The final state is converted into the final result\n",
      "        by applying a finish function.\n",
      "        \n",
      "        Both functions can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "        Python ``UserDefinedFunctions`` are not supported\n",
      "        (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        initialValue : :class:`~pyspark.sql.Column` or str\n",
      "            initial value. Name of column or expression\n",
      "        merge : function\n",
      "            a binary function ``(acc: Column, x: Column) -> Column...`` returning expression\n",
      "            of the same type as ``zero``\n",
      "        finish : function\n",
      "            an optional unary function ``(x: Column) -> Column: ...``\n",
      "            used to convert accumulated value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            final value after aggregate function is applied.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\"))\n",
      "        >>> df.select(reduce(\"values\", lit(0.0), lambda acc, x: acc + x).alias(\"sum\")).show()\n",
      "        +----+\n",
      "        | sum|\n",
      "        +----+\n",
      "        |42.0|\n",
      "        +----+\n",
      "        \n",
      "        >>> def merge(acc, x):\n",
      "        ...     count = acc.count + 1\n",
      "        ...     sum = acc.sum + x\n",
      "        ...     return struct(count.alias(\"count\"), sum.alias(\"sum\"))\n",
      "        ...\n",
      "        >>> df.select(\n",
      "        ...     reduce(\n",
      "        ...         \"values\",\n",
      "        ...         struct(lit(0).alias(\"count\"), lit(0.0).alias(\"sum\")),\n",
      "        ...         merge,\n",
      "        ...         lambda acc: acc.sum / acc.count,\n",
      "        ...     ).alias(\"mean\")\n",
      "        ... ).show()\n",
      "        +----+\n",
      "        |mean|\n",
      "        +----+\n",
      "        | 8.4|\n",
      "        +----+\n",
      "    \n",
      "    reflect(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calls a method with reflection.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            the first element should be a literal string for the class name,\n",
      "            and the second element should be a literal string for the method name,\n",
      "            and the remaining are input arguments to the Java method.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"a5cf6c42-0c85-418f-af6c-3e4e5b1328f2\",)], [\"a\"])\n",
      "        >>> df.select(\n",
      "        ...     reflect(lit(\"java.util.UUID\"), lit(\"fromString\"), df.a).alias('r')\n",
      "        ... ).collect()\n",
      "        [Row(r='a5cf6c42-0c85-418f-af6c-3e4e5b1328f2')]\n",
      "    \n",
      "    regexp(str: 'ColumnOrName', regexp: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns true if `str` matches the Java regex `regexp`, or false otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        regexp : :class:`~pyspark.sql.Column` or str\n",
      "            regex pattern to apply.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            true if `str` matches a Java regex, or false otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
      "        ... ).select(sf.regexp('str', sf.lit(r'(\\d+)'))).show()\n",
      "        +------------------+\n",
      "        |REGEXP(str, (\\d+))|\n",
      "        +------------------+\n",
      "        |              true|\n",
      "        +------------------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
      "        ... ).select(sf.regexp('str', sf.lit(r'\\d{2}b'))).show()\n",
      "        +-------------------+\n",
      "        |REGEXP(str, \\d{2}b)|\n",
      "        +-------------------+\n",
      "        |              false|\n",
      "        +-------------------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
      "        ... ).select(sf.regexp('str', sf.col(\"regexp\"))).show()\n",
      "        +-------------------+\n",
      "        |REGEXP(str, regexp)|\n",
      "        +-------------------+\n",
      "        |               true|\n",
      "        +-------------------+\n",
      "    \n",
      "    regexp_count(str: 'ColumnOrName', regexp: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a count of the number of times that the Java regex pattern `regexp` is matched\n",
      "        in the string `str`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        regexp : :class:`~pyspark.sql.Column` or str\n",
      "            regex pattern to apply.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the number of times that a Java regex pattern is matched in the string.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"1a 2b 14m\", r\"\\d+\")], [\"str\", \"regexp\"])\n",
      "        >>> df.select(regexp_count('str', lit(r'\\d+')).alias('d')).collect()\n",
      "        [Row(d=3)]\n",
      "        >>> df.select(regexp_count('str', lit(r'mmm')).alias('d')).collect()\n",
      "        [Row(d=0)]\n",
      "        >>> df.select(regexp_count(\"str\", col(\"regexp\")).alias('d')).collect()\n",
      "        [Row(d=3)]\n",
      "    \n",
      "    regexp_extract(str: 'ColumnOrName', pattern: str, idx: int) -> pyspark.sql.column.Column\n",
      "        Extract a specific group matched by the Java regex `regexp`, from the specified string column.\n",
      "        If the regex did not match, or the specified group did not match, an empty string is returned.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        pattern : str\n",
      "            regex pattern to apply.\n",
      "        idx : int\n",
      "            matched group id.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            matched value specified by `idx` group id.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('100-200',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', r'(\\d+)-(\\d+)', 1).alias('d')).collect()\n",
      "        [Row(d='100')]\n",
      "        >>> df = spark.createDataFrame([('foo',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', r'(\\d+)', 1).alias('d')).collect()\n",
      "        [Row(d='')]\n",
      "        >>> df = spark.createDataFrame([('aaaac',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', '(a+)(b)?(c)', 2).alias('d')).collect()\n",
      "        [Row(d='')]\n",
      "    \n",
      "    regexp_extract_all(str: 'ColumnOrName', regexp: 'ColumnOrName', idx: Union[int, pyspark.sql.column.Column, NoneType] = None) -> pyspark.sql.column.Column\n",
      "        Extract all strings in the `str` that match the Java regex `regexp`\n",
      "        and corresponding to the regex group index.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        regexp : :class:`~pyspark.sql.Column` or str\n",
      "            regex pattern to apply.\n",
      "        idx : int\n",
      "            matched group id.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            all strings in the `str` that match a Java regex and corresponding to the regex group index.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"100-200, 300-400\", r\"(\\d+)-(\\d+)\")], [\"str\", \"regexp\"])\n",
      "        >>> df.select(regexp_extract_all('str', lit(r'(\\d+)-(\\d+)')).alias('d')).collect()\n",
      "        [Row(d=['100', '300'])]\n",
      "        >>> df.select(regexp_extract_all('str', lit(r'(\\d+)-(\\d+)'), 1).alias('d')).collect()\n",
      "        [Row(d=['100', '300'])]\n",
      "        >>> df.select(regexp_extract_all('str', lit(r'(\\d+)-(\\d+)'), 2).alias('d')).collect()\n",
      "        [Row(d=['200', '400'])]\n",
      "        >>> df.select(regexp_extract_all('str', col(\"regexp\")).alias('d')).collect()\n",
      "        [Row(d=['100', '300'])]\n",
      "    \n",
      "    regexp_instr(str: 'ColumnOrName', regexp: 'ColumnOrName', idx: Union[int, pyspark.sql.column.Column, NoneType] = None) -> pyspark.sql.column.Column\n",
      "        Extract all strings in the `str` that match the Java regex `regexp`\n",
      "        and corresponding to the regex group index.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        regexp : :class:`~pyspark.sql.Column` or str\n",
      "            regex pattern to apply.\n",
      "        idx : int\n",
      "            matched group id.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            all strings in the `str` that match a Java regex and corresponding to the regex group index.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"1a 2b 14m\", r\"\\d+(a|b|m)\")], [\"str\", \"regexp\"])\n",
      "        >>> df.select(regexp_instr('str', lit(r'\\d+(a|b|m)')).alias('d')).collect()\n",
      "        [Row(d=1)]\n",
      "        >>> df.select(regexp_instr('str', lit(r'\\d+(a|b|m)'), 1).alias('d')).collect()\n",
      "        [Row(d=1)]\n",
      "        >>> df.select(regexp_instr('str', lit(r'\\d+(a|b|m)'), 2).alias('d')).collect()\n",
      "        [Row(d=1)]\n",
      "        >>> df.select(regexp_instr('str', col(\"regexp\")).alias('d')).collect()\n",
      "        [Row(d=1)]\n",
      "    \n",
      "    regexp_like(str: 'ColumnOrName', regexp: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns true if `str` matches the Java regex `regexp`, or false otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        regexp : :class:`~pyspark.sql.Column` or str\n",
      "            regex pattern to apply.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            true if `str` matches a Java regex, or false otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
      "        ... ).select(sf.regexp_like('str', sf.lit(r'(\\d+)'))).show()\n",
      "        +-----------------------+\n",
      "        |REGEXP_LIKE(str, (\\d+))|\n",
      "        +-----------------------+\n",
      "        |                   true|\n",
      "        +-----------------------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
      "        ... ).select(sf.regexp_like('str', sf.lit(r'\\d{2}b'))).show()\n",
      "        +------------------------+\n",
      "        |REGEXP_LIKE(str, \\d{2}b)|\n",
      "        +------------------------+\n",
      "        |                   false|\n",
      "        +------------------------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
      "        ... ).select(sf.regexp_like('str', sf.col(\"regexp\"))).show()\n",
      "        +------------------------+\n",
      "        |REGEXP_LIKE(str, regexp)|\n",
      "        +------------------------+\n",
      "        |                    true|\n",
      "        +------------------------+\n",
      "    \n",
      "    regexp_replace(string: 'ColumnOrName', pattern: Union[str, pyspark.sql.column.Column], replacement: Union[str, pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Replace all substrings of the specified string value that match regexp with replacement.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        string : :class:`~pyspark.sql.Column` or str\n",
      "            column name or column containing the string value\n",
      "        pattern : :class:`~pyspark.sql.Column` or str\n",
      "            column object or str containing the regexp pattern\n",
      "        replacement : :class:`~pyspark.sql.Column` or str\n",
      "            column object or str containing the replacement\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string with all substrings replaced.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"100-200\", r\"(\\d+)\", \"--\")], [\"str\", \"pattern\", \"replacement\"])\n",
      "        >>> df.select(regexp_replace('str', r'(\\d+)', '--').alias('d')).collect()\n",
      "        [Row(d='-----')]\n",
      "        >>> df.select(regexp_replace(\"str\", col(\"pattern\"), col(\"replacement\")).alias('d')).collect()\n",
      "        [Row(d='-----')]\n",
      "    \n",
      "    regexp_substr(str: 'ColumnOrName', regexp: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the substring that matches the Java regex `regexp` within the string `str`.\n",
      "        If the regular expression is not found, the result is null.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        regexp : :class:`~pyspark.sql.Column` or str\n",
      "            regex pattern to apply.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the substring that matches a Java regex within the string `str`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"1a 2b 14m\", r\"\\d+\")], [\"str\", \"regexp\"])\n",
      "        >>> df.select(regexp_substr('str', lit(r'\\d+')).alias('d')).collect()\n",
      "        [Row(d='1')]\n",
      "        >>> df.select(regexp_substr('str', lit(r'mmm')).alias('d')).collect()\n",
      "        [Row(d=None)]\n",
      "        >>> df.select(regexp_substr(\"str\", col(\"regexp\")).alias('d')).collect()\n",
      "        [Row(d='1')]\n",
      "    \n",
      "    regr_avgx(y: 'ColumnOrName', x: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the average of the independent variable for non-null pairs\n",
      "        in a group, where `y` is the dependent variable and `x` is the independent variable.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : :class:`~pyspark.sql.Column` or str\n",
      "            the dependent variable.\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the independent variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the average of the independent variable for non-null pairs in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> x = (col(\"id\") % 3).alias(\"x\")\n",
      "        >>> y = (randn(42) + x * 10).alias(\"y\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(x, y)\n",
      "        >>> df.select(regr_avgx(\"y\", \"x\")).first()\n",
      "        Row(regr_avgx(y, x)=0.999)\n",
      "    \n",
      "    regr_avgy(y: 'ColumnOrName', x: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the average of the dependent variable for non-null pairs\n",
      "        in a group, where `y` is the dependent variable and `x` is the independent variable.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : :class:`~pyspark.sql.Column` or str\n",
      "            the dependent variable.\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the independent variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the average of the dependent variable for non-null pairs in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> x = (col(\"id\") % 3).alias(\"x\")\n",
      "        >>> y = (randn(42) + x * 10).alias(\"y\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(x, y)\n",
      "        >>> df.select(regr_avgy(\"y\", \"x\")).first()\n",
      "        Row(regr_avgy(y, x)=9.980732994136464)\n",
      "    \n",
      "    regr_count(y: 'ColumnOrName', x: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the number of non-null number pairs\n",
      "        in a group, where `y` is the dependent variable and `x` is the independent variable.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : :class:`~pyspark.sql.Column` or str\n",
      "            the dependent variable.\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the independent variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the number of non-null number pairs in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> x = (col(\"id\") % 3).alias(\"x\")\n",
      "        >>> y = (randn(42) + x * 10).alias(\"y\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(x, y)\n",
      "        >>> df.select(regr_count(\"y\", \"x\")).first()\n",
      "        Row(regr_count(y, x)=1000)\n",
      "    \n",
      "    regr_intercept(y: 'ColumnOrName', x: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the intercept of the univariate linear regression line\n",
      "        for non-null pairs in a group, where `y` is the dependent variable and\n",
      "        `x` is the independent variable.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : :class:`~pyspark.sql.Column` or str\n",
      "            the dependent variable.\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the independent variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the intercept of the univariate linear regression line for non-null pairs in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> x = (col(\"id\") % 3).alias(\"x\")\n",
      "        >>> y = (randn(42) + x * 10).alias(\"y\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(x, y)\n",
      "        >>> df.select(regr_intercept(\"y\", \"x\")).first()\n",
      "        Row(regr_intercept(y, x)=-0.04961745990969568)\n",
      "    \n",
      "    regr_r2(y: 'ColumnOrName', x: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the coefficient of determination for non-null pairs\n",
      "        in a group, where `y` is the dependent variable and `x` is the independent variable.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : :class:`~pyspark.sql.Column` or str\n",
      "            the dependent variable.\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the independent variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the coefficient of determination for non-null pairs in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> x = (col(\"id\") % 3).alias(\"x\")\n",
      "        >>> y = (randn(42) + x * 10).alias(\"y\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(x, y)\n",
      "        >>> df.select(regr_r2(\"y\", \"x\")).first()\n",
      "        Row(regr_r2(y, x)=0.9851908293645436)\n",
      "    \n",
      "    regr_slope(y: 'ColumnOrName', x: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the slope of the linear regression line for non-null pairs\n",
      "        in a group, where `y` is the dependent variable and `x` is the independent variable.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : :class:`~pyspark.sql.Column` or str\n",
      "            the dependent variable.\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the independent variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the slope of the linear regression line for non-null pairs in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> x = (col(\"id\") % 3).alias(\"x\")\n",
      "        >>> y = (randn(42) + x * 10).alias(\"y\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(x, y)\n",
      "        >>> df.select(regr_slope(\"y\", \"x\")).first()\n",
      "        Row(regr_slope(y, x)=10.040390844891048)\n",
      "    \n",
      "    regr_sxx(y: 'ColumnOrName', x: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns REGR_COUNT(y, x) * VAR_POP(x) for non-null pairs\n",
      "        in a group, where `y` is the dependent variable and `x` is the independent variable.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : :class:`~pyspark.sql.Column` or str\n",
      "            the dependent variable.\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the independent variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            REGR_COUNT(y, x) * VAR_POP(x) for non-null pairs in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> x = (col(\"id\") % 3).alias(\"x\")\n",
      "        >>> y = (randn(42) + x * 10).alias(\"y\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(x, y)\n",
      "        >>> df.select(regr_sxx(\"y\", \"x\")).first()\n",
      "        Row(regr_sxx(y, x)=666.9989999999996)\n",
      "    \n",
      "    regr_sxy(y: 'ColumnOrName', x: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns REGR_COUNT(y, x) * COVAR_POP(y, x) for non-null pairs\n",
      "        in a group, where `y` is the dependent variable and `x` is the independent variable.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : :class:`~pyspark.sql.Column` or str\n",
      "            the dependent variable.\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the independent variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            REGR_COUNT(y, x) * COVAR_POP(y, x) for non-null pairs in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> x = (col(\"id\") % 3).alias(\"x\")\n",
      "        >>> y = (randn(42) + x * 10).alias(\"y\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(x, y)\n",
      "        >>> df.select(regr_sxy(\"y\", \"x\")).first()\n",
      "        Row(regr_sxy(y, x)=6696.93065315148)\n",
      "    \n",
      "    regr_syy(y: 'ColumnOrName', x: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns REGR_COUNT(y, x) * VAR_POP(y) for non-null pairs\n",
      "        in a group, where `y` is the dependent variable and `x` is the independent variable.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : :class:`~pyspark.sql.Column` or str\n",
      "            the dependent variable.\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the independent variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            REGR_COUNT(y, x) * VAR_POP(y) for non-null pairs in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> x = (col(\"id\") % 3).alias(\"x\")\n",
      "        >>> y = (randn(42) + x * 10).alias(\"y\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(x, y)\n",
      "        >>> df.select(regr_syy(\"y\", \"x\")).first()\n",
      "        Row(regr_syy(y, x)=68250.53503811295)\n",
      "    \n",
      "    repeat(col: 'ColumnOrName', n: int) -> pyspark.sql.column.Column\n",
      "        Repeats a string column n times, and returns it as a new string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        n : int\n",
      "            number of times to repeat value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string with repeated values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('ab',)], ['s',])\n",
      "        >>> df.select(repeat(df.s, 3).alias('s')).collect()\n",
      "        [Row(s='ababab')]\n",
      "    \n",
      "    replace(src: 'ColumnOrName', search: 'ColumnOrName', replace: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Replaces all occurrences of `search` with `replace`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        src : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string to be replaced.\n",
      "        search : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string, If `search` is not found in `str`, `str` is returned unchanged.\n",
      "        replace : :class:`~pyspark.sql.Column` or str, optional\n",
      "            A column of string, If `replace` is not specified or is an empty string,\n",
      "            nothing replaces the string that is removed from `str`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"ABCabc\", \"abc\", \"DEF\",)], [\"a\", \"b\", \"c\"])\n",
      "        >>> df.select(replace(df.a, df.b, df.c).alias('r')).collect()\n",
      "        [Row(r='ABCDEF')]\n",
      "        \n",
      "        >>> df.select(replace(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r='ABC')]\n",
      "    \n",
      "    reverse(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns a reversed string or an array with reverse order of elements.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            array of elements in reverse order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('Spark SQL',)], ['data'])\n",
      "        >>> df.select(reverse(df.data).alias('s')).collect()\n",
      "        [Row(s='LQS krapS')]\n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],) ,([1],) ,([],)], ['data'])\n",
      "        >>> df.select(reverse(df.data).alias('r')).collect()\n",
      "        [Row(r=[3, 1, 2]), Row(r=[1]), Row(r=[])]\n",
      "    \n",
      "    right(str: 'ColumnOrName', len: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the rightmost `len`(`len` can be string type) characters from the string `str`,\n",
      "        if `len` is less or equal than 0 the result is an empty string.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        len : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings, the rightmost `len`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Spark SQL\", 3,)], ['a', 'b'])\n",
      "        >>> df.select(right(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r='SQL')]\n",
      "    \n",
      "    rint(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the double value that is closest in value to the argument and\n",
      "        is equal to a mathematical integer.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(rint(lit(10.6))).show()\n",
      "        +----------+\n",
      "        |rint(10.6)|\n",
      "        +----------+\n",
      "        |      11.0|\n",
      "        +----------+\n",
      "        \n",
      "        >>> df.select(rint(lit(10.3))).show()\n",
      "        +----------+\n",
      "        |rint(10.3)|\n",
      "        +----------+\n",
      "        |      10.0|\n",
      "        +----------+\n",
      "    \n",
      "    rlike(str: 'ColumnOrName', regexp: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns true if `str` matches the Java regex `regexp`, or false otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        regexp : :class:`~pyspark.sql.Column` or str\n",
      "            regex pattern to apply.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            true if `str` matches a Java regex, or false otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"])\n",
      "        >>> df.select(rlike('str', lit(r'(\\d+)')).alias('d')).collect()\n",
      "        [Row(d=True)]\n",
      "        >>> df.select(rlike('str', lit(r'\\d{2}b')).alias('d')).collect()\n",
      "        [Row(d=False)]\n",
      "        >>> df.select(rlike(\"str\", col(\"regexp\")).alias('d')).collect()\n",
      "        [Row(d=True)]\n",
      "    \n",
      "    round(col: 'ColumnOrName', scale: int = 0) -> pyspark.sql.column.Column\n",
      "        Round the given value to `scale` decimal places using HALF_UP rounding mode if `scale` >= 0\n",
      "        or at integral part when `scale` < 0.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column to round.\n",
      "        scale : int optional default 0\n",
      "            scale value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            rounded values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(2.5,)], ['a']).select(round('a', 0).alias('r')).collect()\n",
      "        [Row(r=3.0)]\n",
      "    \n",
      "    row_number() -> pyspark.sql.column.Column\n",
      "        Window function: returns a sequential number starting at 1 within a window partition.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for calculating row numbers.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> df = spark.range(3)\n",
      "        >>> w = Window.orderBy(df.id.desc())\n",
      "        >>> df.withColumn(\"desc_order\", row_number().over(w)).show()\n",
      "        +---+----------+\n",
      "        | id|desc_order|\n",
      "        +---+----------+\n",
      "        |  2|         1|\n",
      "        |  1|         2|\n",
      "        |  0|         3|\n",
      "        +---+----------+\n",
      "    \n",
      "    rpad(col: 'ColumnOrName', len: int, pad: str) -> pyspark.sql.column.Column\n",
      "        Right-pad the string column to width `len` with `pad`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        len : int\n",
      "            length of the final string.\n",
      "        pad : str\n",
      "            chars to append.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            right padded result.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(rpad(df.s, 6, '#').alias('s')).collect()\n",
      "        [Row(s='abcd##')]\n",
      "    \n",
      "    rtrim(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Trim the spaces from right end for the specified string value.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            right trimmed values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n",
      "        >>> df.select(rtrim(\"value\").alias(\"r\")).withColumn(\"length\", length(\"r\")).show()\n",
      "        +--------+------+\n",
      "        |       r|length|\n",
      "        +--------+------+\n",
      "        |   Spark|     8|\n",
      "        |   Spark|     5|\n",
      "        |   Spark|     6|\n",
      "        +--------+------+\n",
      "    \n",
      "    schema_of_csv(csv: 'ColumnOrName', options: Optional[Dict[str, str]] = None) -> pyspark.sql.column.Column\n",
      "        Parses a CSV string and infers its schema in DDL format.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        csv : :class:`~pyspark.sql.Column` or str\n",
      "            a CSV string or a foldable string column containing a CSV string.\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the CSV datasource.\n",
      "            See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "            for the version you use.\n",
      "        \n",
      "            .. # noqa\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a string representation of a :class:`StructType` parsed from given CSV.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(schema_of_csv(lit('1|a'), {'sep':'|'}).alias(\"csv\")).collect()\n",
      "        [Row(csv='STRUCT<_c0: INT, _c1: STRING>')]\n",
      "        >>> df.select(schema_of_csv('1|a', {'sep':'|'}).alias(\"csv\")).collect()\n",
      "        [Row(csv='STRUCT<_c0: INT, _c1: STRING>')]\n",
      "    \n",
      "    schema_of_json(json: 'ColumnOrName', options: Optional[Dict[str, str]] = None) -> pyspark.sql.column.Column\n",
      "        Parses a JSON string and infers its schema in DDL format.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        json : :class:`~pyspark.sql.Column` or str\n",
      "            a JSON string or a foldable string column containing a JSON string.\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the JSON datasource.\n",
      "            See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      "            for the version you use.\n",
      "        \n",
      "            .. # noqa\n",
      "        \n",
      "            .. versionchanged:: 3.0.0\n",
      "               It accepts `options` parameter to control schema inferring.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a string representation of a :class:`StructType` parsed from given JSON.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(schema_of_json(lit('{\"a\": 0}')).alias(\"json\")).collect()\n",
      "        [Row(json='STRUCT<a: BIGINT>')]\n",
      "        >>> schema = schema_of_json('{a: 1}', {'allowUnquotedFieldNames':'true'})\n",
      "        >>> df.select(schema.alias(\"json\")).collect()\n",
      "        [Row(json='STRUCT<a: BIGINT>')]\n",
      "    \n",
      "    sec(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes secant of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Angle in radians\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            Secant of the angle.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(sec(lit(1.5))).first()\n",
      "        Row(SEC(1.5)=14.13683...)\n",
      "    \n",
      "    second(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the seconds of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            `seconds` part of the timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame([(datetime.datetime(2015, 4, 8, 13, 8, 15),)], ['ts'])\n",
      "        >>> df.select(second('ts').alias('second')).collect()\n",
      "        [Row(second=15)]\n",
      "    \n",
      "    sentences(string: 'ColumnOrName', language: Optional[ForwardRef('ColumnOrName')] = None, country: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Splits a string into arrays of sentences, where each sentence is an array of words.\n",
      "        The 'language' and 'country' arguments are optional, and if omitted, the default locale is used.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        string : :class:`~pyspark.sql.Column` or str\n",
      "            a string to be split\n",
      "        language : :class:`~pyspark.sql.Column` or str, optional\n",
      "            a language of the locale\n",
      "        country : :class:`~pyspark.sql.Column` or str, optional\n",
      "            a country of the locale\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            arrays of split sentences.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[\"This is an example sentence.\"]], [\"string\"])\n",
      "        >>> df.select(sentences(df.string, lit(\"en\"), lit(\"US\"))).show(truncate=False)\n",
      "        +-----------------------------------+\n",
      "        |sentences(string, en, US)          |\n",
      "        +-----------------------------------+\n",
      "        |[[This, is, an, example, sentence]]|\n",
      "        +-----------------------------------+\n",
      "        >>> df = spark.createDataFrame([[\"Hello world. How are you?\"]], [\"s\"])\n",
      "        >>> df.select(sentences(\"s\")).show(truncate=False)\n",
      "        +---------------------------------+\n",
      "        |sentences(s, , )                 |\n",
      "        +---------------------------------+\n",
      "        |[[Hello, world], [How, are, you]]|\n",
      "        +---------------------------------+\n",
      "    \n",
      "    sequence(start: 'ColumnOrName', stop: 'ColumnOrName', step: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Generate a sequence of integers from `start` to `stop`, incrementing by `step`.\n",
      "        If `step` is not set, incrementing by 1 if `start` is less than or equal to `stop`,\n",
      "        otherwise -1.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            starting value (inclusive)\n",
      "        stop : :class:`~pyspark.sql.Column` or str\n",
      "            last values (inclusive)\n",
      "        step : :class:`~pyspark.sql.Column` or str, optional\n",
      "            value to add to current to get next element (default is 1)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of sequence values\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(-2, 2)], ('C1', 'C2'))\n",
      "        >>> df1.select(sequence('C1', 'C2').alias('r')).collect()\n",
      "        [Row(r=[-2, -1, 0, 1, 2])]\n",
      "        >>> df2 = spark.createDataFrame([(4, -4, -2)], ('C1', 'C2', 'C3'))\n",
      "        >>> df2.select(sequence('C1', 'C2', 'C3').alias('r')).collect()\n",
      "        [Row(r=[4, 2, 0, -2, -4])]\n",
      "    \n",
      "    session_window(timeColumn: 'ColumnOrName', gapDuration: Union[pyspark.sql.column.Column, str]) -> pyspark.sql.column.Column\n",
      "        Generates session window given a timestamp specifying column.\n",
      "        Session window is one of dynamic windows, which means the length of window is varying\n",
      "        according to the given inputs. The length of session window is defined as \"the timestamp\n",
      "        of latest input of the session + gap duration\", so when the new inputs are bound to the\n",
      "        current session window, the end time of session window can be expanded according to the new\n",
      "        inputs.\n",
      "        Windows can support microsecond precision. Windows in the order of months are not supported.\n",
      "        For a streaming query, you may use the function `current_timestamp` to generate windows on\n",
      "        processing time.\n",
      "        gapDuration is provided as strings, e.g. '1 second', '1 day 12 hours', '2 minutes'. Valid\n",
      "        interval strings are 'week', 'day', 'hour', 'minute', 'second', 'millisecond', 'microsecond'.\n",
      "        It could also be a Column which can be evaluated to gap duration dynamically based on the\n",
      "        input row.\n",
      "        The output column will be a struct called 'session_window' by default with the nested columns\n",
      "        'start' and 'end', where 'start' and 'end' will be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timeColumn : :class:`~pyspark.sql.Column` or str\n",
      "            The column name or column to use as the timestamp for windowing by time.\n",
      "            The time column must be of TimestampType or TimestampNTZType.\n",
      "        gapDuration : :class:`~pyspark.sql.Column` or str\n",
      "            A Python string literal or column specifying the timeout of the session. It could be\n",
      "            static value, e.g. `10 minutes`, `1 second`, or an expression/UDF that specifies gap\n",
      "            duration dynamically based on the input row.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"2016-03-11 09:00:07\", 1)]).toDF(\"date\", \"val\")\n",
      "        >>> w = df.groupBy(session_window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n",
      "        >>> w.select(w.session_window.start.cast(\"string\").alias(\"start\"),\n",
      "        ...          w.session_window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\n",
      "        [Row(start='2016-03-11 09:00:07', end='2016-03-11 09:00:12', sum=1)]\n",
      "        >>> w = df.groupBy(session_window(\"date\", lit(\"5 seconds\"))).agg(sum(\"val\").alias(\"sum\"))\n",
      "        >>> w.select(w.session_window.start.cast(\"string\").alias(\"start\"),\n",
      "        ...          w.session_window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\n",
      "        [Row(start='2016-03-11 09:00:07', end='2016-03-11 09:00:12', sum=1)]\n",
      "    \n",
      "    sha(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sha1 hash value as a hex string of the `col`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.sha(sf.lit(\"Spark\"))).show()\n",
      "        +--------------------+\n",
      "        |          sha(Spark)|\n",
      "        +--------------------+\n",
      "        |85f5955f4b27a9a4c...|\n",
      "        +--------------------+\n",
      "    \n",
      "    sha1(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the hex string result of SHA-1.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(sha1('a').alias('hash')).collect()\n",
      "        [Row(hash='3c01bdbb26f358bab27f267924aa2c9a03fcfdb8')]\n",
      "    \n",
      "    sha2(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384,\n",
      "        and SHA-512). The numBits indicates the desired bit length of the result, which must have a\n",
      "        value of 224, 256, 384, 512, or 0 (which is equivalent to 256).\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        numBits : int\n",
      "            the desired bit length of the result, which must have a\n",
      "            value of 224, 256, 384, 512, or 0 (which is equivalent to 256).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[\"Alice\"], [\"Bob\"]], [\"name\"])\n",
      "        >>> df.withColumn(\"sha2\", sha2(df.name, 256)).show(truncate=False)\n",
      "        +-----+----------------------------------------------------------------+\n",
      "        |name |sha2                                                            |\n",
      "        +-----+----------------------------------------------------------------+\n",
      "        |Alice|3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043|\n",
      "        |Bob  |cd9fb1e148ccd8442e5aa74904cc73bf6fb54d1d54d333bd596aa9bb4bb4e961|\n",
      "        +-----+----------------------------------------------------------------+\n",
      "    \n",
      "    shiftLeft(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        Shift the given value numBits left.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 3.2.0\n",
      "            Use :func:`shiftleft` instead.\n",
      "    \n",
      "    shiftRight(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        (Signed) shift the given value numBits right.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 3.2.0\n",
      "            Use :func:`shiftright` instead.\n",
      "    \n",
      "    shiftRightUnsigned(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        Unsigned shift the given value numBits right.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 3.2.0\n",
      "            Use :func:`shiftrightunsigned` instead.\n",
      "    \n",
      "    shiftleft(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        Shift the given value numBits left.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to shift.\n",
      "        numBits : int\n",
      "            number of bits to shift.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            shifted value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(21,)], ['a']).select(shiftleft('a', 1).alias('r')).collect()\n",
      "        [Row(r=42)]\n",
      "    \n",
      "    shiftright(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        (Signed) shift the given value numBits right.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to shift.\n",
      "        numBits : int\n",
      "            number of bits to shift.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            shifted values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(42,)], ['a']).select(shiftright('a', 1).alias('r')).collect()\n",
      "        [Row(r=21)]\n",
      "    \n",
      "    shiftrightunsigned(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        Unsigned shift the given value numBits right.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to shift.\n",
      "        numBits : int\n",
      "            number of bits to shift.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            shifted value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(-42,)], ['a'])\n",
      "        >>> df.select(shiftrightunsigned('a', 1).alias('r')).collect()\n",
      "        [Row(r=9223372036854775787)]\n",
      "    \n",
      "    shuffle(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: Generates a random permutation of the given array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of elements in random order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 20, 3, 5],), ([1, 20, None, 3],)], ['data'])\n",
      "        >>> df.select(shuffle(df.data).alias('s')).collect()  # doctest: +SKIP\n",
      "        [Row(s=[3, 1, 5, 20]), Row(s=[20, None, 3, 1])]\n",
      "    \n",
      "    sign(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the signum of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(\n",
      "        ...     sf.sign(sf.lit(-5)),\n",
      "        ...     sf.sign(sf.lit(6))\n",
      "        ... ).show()\n",
      "        +--------+-------+\n",
      "        |sign(-5)|sign(6)|\n",
      "        +--------+-------+\n",
      "        |    -1.0|    1.0|\n",
      "        +--------+-------+\n",
      "    \n",
      "    signum(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the signum of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(\n",
      "        ...     sf.signum(sf.lit(-5)),\n",
      "        ...     sf.signum(sf.lit(6))\n",
      "        ... ).show()\n",
      "        +----------+---------+\n",
      "        |SIGNUM(-5)|SIGNUM(6)|\n",
      "        +----------+---------+\n",
      "        |      -1.0|      1.0|\n",
      "        +----------+---------+\n",
      "    \n",
      "    sin(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes sine of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            sine of the angle, as if computed by `java.lang.Math.sin()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(sin(lit(math.radians(90)))).first()\n",
      "        Row(SIN(1.57079...)=1.0)\n",
      "    \n",
      "    sinh(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes hyperbolic sine of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            hyperbolic angle.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hyperbolic sine of the given value,\n",
      "            as if computed by `java.lang.Math.sinh()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(sinh(lit(1.1))).first()\n",
      "        Row(SINH(1.1)=1.33564...)\n",
      "    \n",
      "    size(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns the length of the array or map stored in the column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            length of the array/map.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(size(df.data)).collect()\n",
      "        [Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]\n",
      "    \n",
      "    skewness(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the skewness of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            skewness of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
      "        >>> df.select(skewness(df.c)).first()\n",
      "        Row(skewness(c)=0.70710...)\n",
      "    \n",
      "    slice(x: 'ColumnOrName', start: Union[ForwardRef('ColumnOrName'), int], length: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Collection function: returns an array containing all the elements in `x` from index `start`\n",
      "        (array indices start at 1, or from the end if `start` is negative) with the specified `length`.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            column name or column containing the array to be sliced\n",
      "        start : :class:`~pyspark.sql.Column` or str or int\n",
      "            column name, column, or int containing the starting index\n",
      "        length : :class:`~pyspark.sql.Column` or str or int\n",
      "            column name, column, or int containing the length of the slice\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of array type. Subset of array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], ['x'])\n",
      "        >>> df.select(slice(df.x, 2, 2).alias(\"sliced\")).collect()\n",
      "        [Row(sliced=[2, 3]), Row(sliced=[5])]\n",
      "    \n",
      "    some(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns true if at least one value of `col` is true.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to check if at least one value is true.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            true if at least one value of `col` is true, false otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [[True], [True], [True]], [\"flag\"]\n",
      "        ... ).select(sf.some(\"flag\")).show()\n",
      "        +----------+\n",
      "        |some(flag)|\n",
      "        +----------+\n",
      "        |      true|\n",
      "        +----------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [[True], [False], [True]], [\"flag\"]\n",
      "        ... ).select(sf.some(\"flag\")).show()\n",
      "        +----------+\n",
      "        |some(flag)|\n",
      "        +----------+\n",
      "        |      true|\n",
      "        +----------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [[False], [False], [False]], [\"flag\"]\n",
      "        ... ).select(sf.some(\"flag\")).show()\n",
      "        +----------+\n",
      "        |some(flag)|\n",
      "        +----------+\n",
      "        |     false|\n",
      "        +----------+\n",
      "    \n",
      "    sort_array(col: 'ColumnOrName', asc: bool = True) -> pyspark.sql.column.Column\n",
      "        Collection function: sorts the input array in ascending or descending order according\n",
      "        to the natural ordering of the array elements. Null elements will be placed at the beginning\n",
      "        of the returned array in ascending order or at the end of the returned array in descending\n",
      "        order.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        asc : bool, optional\n",
      "            whether to sort in ascending or descending order. If `asc` is True (default)\n",
      "            then ascending and if False then descending.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            sorted array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(sort_array(df.data).alias('r')).collect()\n",
      "        [Row(r=[None, 1, 2, 3]), Row(r=[1]), Row(r=[])]\n",
      "        >>> df.select(sort_array(df.data, asc=False).alias('r')).collect()\n",
      "        [Row(r=[3, 2, 1, None]), Row(r=[1]), Row(r=[])]\n",
      "    \n",
      "    soundex(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the SoundEx encoding for a string\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            SoundEx encoded string.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Peters\",),(\"Uhrbach\",)], ['name'])\n",
      "        >>> df.select(soundex(df.name).alias(\"soundex\")).collect()\n",
      "        [Row(soundex='P362'), Row(soundex='U612')]\n",
      "    \n",
      "    spark_partition_id() -> pyspark.sql.column.Column\n",
      "        A column for partition ID.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is non deterministic because it depends on data partitioning and task scheduling.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            partition id the record belongs to.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(2)\n",
      "        >>> df.repartition(1).select(spark_partition_id().alias(\"pid\")).collect()\n",
      "        [Row(pid=0), Row(pid=0)]\n",
      "    \n",
      "    split(str: 'ColumnOrName', pattern: str, limit: int = -1) -> pyspark.sql.column.Column\n",
      "        Splits str around matches of the given pattern.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            a string expression to split\n",
      "        pattern : str\n",
      "            a string representing a regular expression. The regex string should be\n",
      "            a Java regular expression.\n",
      "        limit : int, optional\n",
      "            an integer which controls the number of times `pattern` is applied.\n",
      "        \n",
      "            * ``limit > 0``: The resulting array's length will not be more than `limit`, and the\n",
      "                             resulting array's last entry will contain all input beyond the last\n",
      "                             matched pattern.\n",
      "            * ``limit <= 0``: `pattern` will be applied as many times as possible, and the resulting\n",
      "                              array can be of any size.\n",
      "        \n",
      "            .. versionchanged:: 3.0\n",
      "               `split` now takes an optional `limit` field. If not provided, default limit value is -1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            array of separated strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])\n",
      "        >>> df.select(split(df.s, '[ABC]', 2).alias('s')).collect()\n",
      "        [Row(s=['one', 'twoBthreeC'])]\n",
      "        >>> df.select(split(df.s, '[ABC]', -1).alias('s')).collect()\n",
      "        [Row(s=['one', 'two', 'three', ''])]\n",
      "    \n",
      "    split_part(src: 'ColumnOrName', delimiter: 'ColumnOrName', partNum: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Splits `str` by delimiter and return requested part of the split (1-based).\n",
      "        If any input is null, returns null. if `partNum` is out of range of split parts,\n",
      "        returns empty string. If `partNum` is 0, throws an error. If `partNum` is negative,\n",
      "        the parts are counted backward from the end of the string.\n",
      "        If the `delimiter` is an empty string, the `str` is not split.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        src : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string to be splited.\n",
      "        delimiter : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string, the delimiter used for split.\n",
      "        partNum : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string, requested part of the split (1-based).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"11.12.13\", \".\", 3,)], [\"a\", \"b\", \"c\"])\n",
      "        >>> df.select(split_part(df.a, df.b, df.c).alias('r')).collect()\n",
      "        [Row(r='13')]\n",
      "    \n",
      "    sqrt(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the square root of the specified float value.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(sqrt(lit(4))).show()\n",
      "        +-------+\n",
      "        |SQRT(4)|\n",
      "        +-------+\n",
      "        |    2.0|\n",
      "        +-------+\n",
      "    \n",
      "    stack(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Separates `col1`, ..., `colk` into `n` rows. Uses column names col0, col1, etc. by default\n",
      "        unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            the first element should be a literal int for the number of rows to be separated,\n",
      "            and the remaining are input elements to be separated.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, 2, 3)], [\"a\", \"b\", \"c\"])\n",
      "        >>> df.select(stack(lit(2), df.a, df.b, df.c)).show(truncate=False)\n",
      "        +----+----+\n",
      "        |col0|col1|\n",
      "        +----+----+\n",
      "        |1   |2   |\n",
      "        |3   |NULL|\n",
      "        +----+----+\n",
      "    \n",
      "    startswith(str: 'ColumnOrName', prefix: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a boolean. The value is True if str starts with prefix.\n",
      "        Returns NULL if either input expression is NULL. Otherwise, returns False.\n",
      "        Both str or prefix must be of STRING or BINARY type.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string.\n",
      "        prefix : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string, the prefix.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Spark SQL\", \"Spark\",)], [\"a\", \"b\"])\n",
      "        >>> df.select(startswith(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=True)]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"414243\", \"4142\",)], [\"e\", \"f\"])\n",
      "        >>> df = df.select(to_binary(\"e\").alias(\"e\"), to_binary(\"f\").alias(\"f\"))\n",
      "        >>> df.printSchema()\n",
      "        root\n",
      "         |-- e: binary (nullable = true)\n",
      "         |-- f: binary (nullable = true)\n",
      "        >>> df.select(startswith(\"e\", \"f\"), startswith(\"f\", \"e\")).show()\n",
      "        +----------------+----------------+\n",
      "        |startswith(e, f)|startswith(f, e)|\n",
      "        +----------------+----------------+\n",
      "        |            true|           false|\n",
      "        +----------------+----------------+\n",
      "    \n",
      "    std(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: alias for stddev_samp.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            standard deviation of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(6).select(sf.std(\"id\")).show()\n",
      "        +------------------+\n",
      "        |           std(id)|\n",
      "        +------------------+\n",
      "        |1.8708286933869...|\n",
      "        +------------------+\n",
      "    \n",
      "    stddev(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: alias for stddev_samp.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            standard deviation of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(6).select(sf.stddev(\"id\")).show()\n",
      "        +------------------+\n",
      "        |        stddev(id)|\n",
      "        +------------------+\n",
      "        |1.8708286933869...|\n",
      "        +------------------+\n",
      "    \n",
      "    stddev_pop(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns population standard deviation of\n",
      "        the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            standard deviation of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(6).select(sf.stddev_pop(\"id\")).show()\n",
      "        +-----------------+\n",
      "        |   stddev_pop(id)|\n",
      "        +-----------------+\n",
      "        |1.707825127659...|\n",
      "        +-----------------+\n",
      "    \n",
      "    stddev_samp(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the unbiased sample standard deviation of\n",
      "        the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            standard deviation of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(6).select(sf.stddev_samp(\"id\")).show()\n",
      "        +------------------+\n",
      "        |   stddev_samp(id)|\n",
      "        +------------------+\n",
      "        |1.8708286933869...|\n",
      "        +------------------+\n",
      "    \n",
      "    str_to_map(text: 'ColumnOrName', pairDelim: Optional[ForwardRef('ColumnOrName')] = None, keyValueDelim: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Creates a map after splitting the text into key/value pairs using delimiters.\n",
      "        Both `pairDelim` and `keyValueDelim` are treated as regular expressions.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        text : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        pairDelim : :class:`~pyspark.sql.Column` or str, optional\n",
      "            delimiter to use to split pair.\n",
      "        keyValueDelim : :class:`~pyspark.sql.Column` or str, optional\n",
      "            delimiter to use to split key/value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"a:1,b:2,c:3\",)], [\"e\"])\n",
      "        >>> df.select(str_to_map(df.e, lit(\",\"), lit(\":\")).alias('r')).collect()\n",
      "        [Row(r={'a': '1', 'b': '2', 'c': '3'})]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"a:1,b:2,c:3\",)], [\"e\"])\n",
      "        >>> df.select(str_to_map(df.e, lit(\",\")).alias('r')).collect()\n",
      "        [Row(r={'a': '1', 'b': '2', 'c': '3'})]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"a:1,b:2,c:3\",)], [\"e\"])\n",
      "        >>> df.select(str_to_map(df.e).alias('r')).collect()\n",
      "        [Row(r={'a': '1', 'b': '2', 'c': '3'})]\n",
      "    \n",
      "    struct(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')], Tuple[ForwardRef('ColumnOrName_'), ...]]) -> pyspark.sql.column.Column\n",
      "        Creates a new struct column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : list, set, str or :class:`~pyspark.sql.Column`\n",
      "            column names or :class:`~pyspark.sql.Column`\\s to contain in the output struct.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a struct type column of given columns.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n",
      "        >>> df.select(struct('age', 'name').alias(\"struct\")).collect()\n",
      "        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n",
      "        >>> df.select(struct([df.age, df.name]).alias(\"struct\")).collect()\n",
      "        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n",
      "    \n",
      "    substr(str: 'ColumnOrName', pos: 'ColumnOrName', len: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Returns the substring of `str` that starts at `pos` and is of length `len`,\n",
      "        or the slice of byte array that starts at `pos` and is of length `len`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        src : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string.\n",
      "        pos : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string, the substring of `str` that starts at `pos`.\n",
      "        len : :class:`~pyspark.sql.Column` or str, optional\n",
      "            A column of string, the substring of `str` is of length `len`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"Spark SQL\", 5, 1,)], [\"a\", \"b\", \"c\"]\n",
      "        ... ).select(sf.substr(\"a\", \"b\", \"c\")).show()\n",
      "        +---------------+\n",
      "        |substr(a, b, c)|\n",
      "        +---------------+\n",
      "        |              k|\n",
      "        +---------------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"Spark SQL\", 5, 1,)], [\"a\", \"b\", \"c\"]\n",
      "        ... ).select(sf.substr(\"a\", \"b\")).show()\n",
      "        +------------------------+\n",
      "        |substr(a, b, 2147483647)|\n",
      "        +------------------------+\n",
      "        |                   k SQL|\n",
      "        +------------------------+\n",
      "    \n",
      "    substring(str: 'ColumnOrName', pos: int, len: int) -> pyspark.sql.column.Column\n",
      "        Substring starts at `pos` and is of length `len` when str is String type or\n",
      "        returns the slice of byte array that starts at `pos` in byte and is of length `len`\n",
      "        when str is Binary type.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        pos : int\n",
      "            starting position in str.\n",
      "        len : int\n",
      "            length of chars.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            substring of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n",
      "        [Row(s='ab')]\n",
      "    \n",
      "    substring_index(str: 'ColumnOrName', delim: str, count: int) -> pyspark.sql.column.Column\n",
      "        Returns the substring from string str before count occurrences of the delimiter delim.\n",
      "        If count is positive, everything the left of the final delimiter (counting from left) is\n",
      "        returned. If count is negative, every to the right of the final delimiter (counting from the\n",
      "        right) is returned. substring_index performs a case-sensitive match when searching for delim.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        delim : str\n",
      "            delimiter of values.\n",
      "        count : int\n",
      "            number of occurrences.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            substring of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n",
      "        >>> df.select(substring_index(df.s, '.', 2).alias('s')).collect()\n",
      "        [Row(s='a.b')]\n",
      "        >>> df.select(substring_index(df.s, '.', -3).alias('s')).collect()\n",
      "        [Row(s='b.c.d')]\n",
      "    \n",
      "    sum(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the sum of all values in the expression.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(10)\n",
      "        >>> df.select(sum(df[\"id\"])).show()\n",
      "        +-------+\n",
      "        |sum(id)|\n",
      "        +-------+\n",
      "        |     45|\n",
      "        +-------+\n",
      "    \n",
      "    sumDistinct(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the sum of distinct values in the expression.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 3.2.0\n",
      "            Use :func:`sum_distinct` instead.\n",
      "    \n",
      "    sum_distinct(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the sum of distinct values in the expression.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None,), (1,), (1,), (2,)], schema=[\"numbers\"])\n",
      "        >>> df.select(sum_distinct(col(\"numbers\"))).show()\n",
      "        +---------------------+\n",
      "        |sum(DISTINCT numbers)|\n",
      "        +---------------------+\n",
      "        |                    3|\n",
      "        +---------------------+\n",
      "    \n",
      "    tan(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes tangent of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            tangent of the given value, as if computed by `java.lang.Math.tan()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(tan(lit(math.radians(45)))).first()\n",
      "        Row(TAN(0.78539...)=0.99999...)\n",
      "    \n",
      "    tanh(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes hyperbolic tangent of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            hyperbolic angle\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hyperbolic tangent of the given value\n",
      "            as if computed by `java.lang.Math.tanh()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(tanh(lit(math.radians(90)))).first()\n",
      "        Row(TANH(1.57079...)=0.91715...)\n",
      "    \n",
      "    timestamp_micros(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Creates timestamp from the number of microseconds since UTC epoch.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            unix time values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            converted timestamp value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
      "        >>> time_df = spark.createDataFrame([(1230219000,)], ['unix_time'])\n",
      "        >>> time_df.select(timestamp_micros(time_df.unix_time).alias('ts')).show()\n",
      "        +--------------------+\n",
      "        |                  ts|\n",
      "        +--------------------+\n",
      "        |1970-01-01 00:20:...|\n",
      "        +--------------------+\n",
      "        >>> time_df.select(timestamp_micros('unix_time').alias('ts')).printSchema()\n",
      "        root\n",
      "         |-- ts: timestamp (nullable = true)\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    timestamp_millis(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Creates timestamp from the number of milliseconds since UTC epoch.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            unix time values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            converted timestamp value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
      "        >>> time_df = spark.createDataFrame([(1230219000,)], ['unix_time'])\n",
      "        >>> time_df.select(timestamp_millis(time_df.unix_time).alias('ts')).show()\n",
      "        +-------------------+\n",
      "        |                 ts|\n",
      "        +-------------------+\n",
      "        |1970-01-15 05:43:39|\n",
      "        +-------------------+\n",
      "        >>> time_df.select(timestamp_millis('unix_time').alias('ts')).printSchema()\n",
      "        root\n",
      "         |-- ts: timestamp (nullable = true)\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    timestamp_seconds(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Converts the number of seconds from the Unix epoch (1970-01-01T00:00:00Z)\n",
      "        to a timestamp.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            unix time values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            converted timestamp value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import timestamp_seconds\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
      "        >>> time_df = spark.createDataFrame([(1230219000,)], ['unix_time'])\n",
      "        >>> time_df.select(timestamp_seconds(time_df.unix_time).alias('ts')).show()\n",
      "        +-------------------+\n",
      "        |                 ts|\n",
      "        +-------------------+\n",
      "        |2008-12-25 15:30:00|\n",
      "        +-------------------+\n",
      "        >>> time_df.select(timestamp_seconds('unix_time').alias('ts')).printSchema()\n",
      "        root\n",
      "         |-- ts: timestamp (nullable = true)\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    toDegrees(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 2.1.0\n",
      "            Use :func:`degrees` instead.\n",
      "    \n",
      "    toRadians(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 2.1.0\n",
      "            Use :func:`radians` instead.\n",
      "    \n",
      "    to_binary(col: 'ColumnOrName', format: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Converts the input `col` to a binary value based on the supplied `format`.\n",
      "        The `format` can be a case-insensitive string literal of \"hex\", \"utf-8\", \"utf8\",\n",
      "        or \"base64\". By default, the binary format for conversion is \"hex\" if\n",
      "        `format` is omitted. The function returns NULL if at least one of the\n",
      "        input parameters is NULL.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        format : :class:`~pyspark.sql.Column` or str, optional\n",
      "            format to use to convert binary values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"abc\",)], [\"e\"])\n",
      "        >>> df.select(to_binary(df.e, lit(\"utf-8\")).alias('r')).collect()\n",
      "        [Row(r=bytearray(b'abc'))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"414243\",)], [\"e\"])\n",
      "        >>> df.select(to_binary(df.e).alias('r')).collect()\n",
      "        [Row(r=bytearray(b'ABC'))]\n",
      "    \n",
      "    to_char(col: 'ColumnOrName', format: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Convert `col` to a string based on the `format`.\n",
      "        Throws an exception if the conversion fails. The format can consist of the following\n",
      "        characters, case insensitive:\n",
      "        '0' or '9': Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the\n",
      "        format string matches a sequence of digits in the input value, generating a result\n",
      "        string of the same length as the corresponding sequence in the format string.\n",
      "        The result string is left-padded with zeros if the 0/9 sequence comprises more digits\n",
      "        than the matching part of the decimal value, starts with 0, and is before the decimal\n",
      "        point. Otherwise, it is padded with spaces.\n",
      "        '.' or 'D': Specifies the position of the decimal point (optional, only allowed once).\n",
      "        ',' or 'G': Specifies the position of the grouping (thousands) separator (,).\n",
      "        There must be a 0 or 9 to the left and right of each grouping separator.\n",
      "        '$': Specifies the location of the $ currency sign. This character may only be specified once.\n",
      "        'S' or 'MI': Specifies the position of a '-' or '+' sign (optional, only allowed once at\n",
      "        the beginning or end of the format string). Note that 'S' prints '+' for positive\n",
      "        values but 'MI' prints a space.\n",
      "        'PR': Only allowed at the end of the format string; specifies that the result string\n",
      "        will be wrapped by angle brackets if the input value is negative.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        format : :class:`~pyspark.sql.Column` or str, optional\n",
      "            format to use to convert char values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(78.12,)], [\"e\"])\n",
      "        >>> df.select(to_char(df.e, lit(\"$99.99\")).alias('r')).collect()\n",
      "        [Row(r='$78.12')]\n",
      "    \n",
      "    to_csv(col: 'ColumnOrName', options: Optional[Dict[str, str]] = None) -> pyspark.sql.column.Column\n",
      "        Converts a column containing a :class:`StructType` into a CSV string.\n",
      "        Throws an exception, in the case of an unsupported type.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a struct.\n",
      "        options: dict, optional\n",
      "            options to control converting. accepts the same options as the CSV datasource.\n",
      "            See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "            for the version you use.\n",
      "        \n",
      "            .. # noqa\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a CSV string converted from given :class:`StructType`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> data = [(1, Row(age=2, name='Alice'))]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_csv(df.value).alias(\"csv\")).collect()\n",
      "        [Row(csv='2,Alice')]\n",
      "    \n",
      "    to_date(col: 'ColumnOrName', format: Optional[str] = None) -> pyspark.sql.column.Column\n",
      "        Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.DateType`\n",
      "        using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "        By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n",
      "        is omitted. Equivalent to ``col.cast(\"date\")``.\n",
      "        \n",
      "        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "        \n",
      "        .. versionadded:: 2.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to convert.\n",
      "        format: str, optional\n",
      "            format to use to convert date values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            date value as :class:`pyspark.sql.types.DateType` type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_date(df.t).alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "    \n",
      "    to_json(col: 'ColumnOrName', options: Optional[Dict[str, str]] = None) -> pyspark.sql.column.Column\n",
      "        Converts a column containing a :class:`StructType`, :class:`ArrayType` or a :class:`MapType`\n",
      "        into a JSON string. Throws an exception, in the case of an unsupported type.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a struct, an array or a map.\n",
      "        options : dict, optional\n",
      "            options to control converting. accepts the same options as the JSON datasource.\n",
      "            See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      "            for the version you use.\n",
      "            Additionally the function supports the `pretty` option which enables\n",
      "            pretty JSON generation.\n",
      "        \n",
      "            .. # noqa\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            JSON object as string column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> data = [(1, Row(age=2, name='Alice'))]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='{\"age\":2,\"name\":\"Alice\"}')]\n",
      "        >>> data = [(1, [Row(age=2, name='Alice'), Row(age=3, name='Bob')])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[{\"age\":2,\"name\":\"Alice\"},{\"age\":3,\"name\":\"Bob\"}]')]\n",
      "        >>> data = [(1, {\"name\": \"Alice\"})]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='{\"name\":\"Alice\"}')]\n",
      "        >>> data = [(1, [{\"name\": \"Alice\"}, {\"name\": \"Bob\"}])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[{\"name\":\"Alice\"},{\"name\":\"Bob\"}]')]\n",
      "        >>> data = [(1, [\"Alice\", \"Bob\"])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[\"Alice\",\"Bob\"]')]\n",
      "    \n",
      "    to_number(col: 'ColumnOrName', format: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Convert string 'col' to a number based on the string format 'format'.\n",
      "        Throws an exception if the conversion fails. The format can consist of the following\n",
      "        characters, case insensitive:\n",
      "        '0' or '9': Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the\n",
      "        format string matches a sequence of digits in the input string. If the 0/9\n",
      "        sequence starts with 0 and is before the decimal point, it can only match a digit\n",
      "        sequence of the same size. Otherwise, if the sequence starts with 9 or is after\n",
      "        the decimal point, it can match a digit sequence that has the same or smaller size.\n",
      "        '.' or 'D': Specifies the position of the decimal point (optional, only allowed once).\n",
      "        ',' or 'G': Specifies the position of the grouping (thousands) separator (,).\n",
      "        There must be a 0 or 9 to the left and right of each grouping separator.\n",
      "        'col' must match the grouping separator relevant for the size of the number.\n",
      "        '$': Specifies the location of the $ currency sign. This character may only be\n",
      "        specified once.\n",
      "        'S' or 'MI': Specifies the position of a '-' or '+' sign (optional, only allowed\n",
      "        once at the beginning or end of the format string). Note that 'S' allows '-'\n",
      "        but 'MI' does not.\n",
      "        'PR': Only allowed at the end of the format string; specifies that 'col' indicates a\n",
      "        negative number with wrapping angled brackets.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        format : :class:`~pyspark.sql.Column` or str, optional\n",
      "            format to use to convert number values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"$78.12\",)], [\"e\"])\n",
      "        >>> df.select(to_number(df.e, lit(\"$99.99\")).alias('r')).collect()\n",
      "        [Row(r=Decimal('78.12'))]\n",
      "    \n",
      "    to_timestamp(col: 'ColumnOrName', format: Optional[str] = None) -> pyspark.sql.column.Column\n",
      "        Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.TimestampType`\n",
      "        using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "        By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format\n",
      "        is omitted. Equivalent to ``col.cast(\"timestamp\")``.\n",
      "        \n",
      "        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "        \n",
      "        .. versionadded:: 2.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column values to convert.\n",
      "        format: str, optional\n",
      "            format to use to convert timestamp values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            timestamp value as :class:`pyspark.sql.types.TimestampType` type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_timestamp(df.t).alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "    \n",
      "    to_timestamp_ltz(timestamp: 'ColumnOrName', format: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Parses the `timestamp` with the `format` to a timestamp without time zone.\n",
      "        Returns null with invalid input.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        format : :class:`~pyspark.sql.Column` or str, optional\n",
      "            format to use to convert type `TimestampType` timestamp values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"2016-12-31\",)], [\"e\"])\n",
      "        >>> df.select(to_timestamp_ltz(df.e, lit(\"yyyy-MM-dd\")).alias('r')).collect()\n",
      "        ... # doctest: +SKIP\n",
      "        [Row(r=datetime.datetime(2016, 12, 31, 0, 0))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"2016-12-31\",)], [\"e\"])\n",
      "        >>> df.select(to_timestamp_ltz(df.e).alias('r')).collect()\n",
      "        ... # doctest: +SKIP\n",
      "        [Row(r=datetime.datetime(2016, 12, 31, 0, 0))]\n",
      "    \n",
      "    to_timestamp_ntz(timestamp: 'ColumnOrName', format: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Parses the `timestamp` with the `format` to a timestamp without time zone.\n",
      "        Returns null with invalid input.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        format : :class:`~pyspark.sql.Column` or str, optional\n",
      "            format to use to convert type `TimestampNTZType` timestamp values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"2016-04-08\",)], [\"e\"])\n",
      "        >>> df.select(to_timestamp_ntz(df.e, lit(\"yyyy-MM-dd\")).alias('r')).collect()\n",
      "        ... # doctest: +SKIP\n",
      "        [Row(r=datetime.datetime(2016, 4, 8, 0, 0))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"2016-04-08\",)], [\"e\"])\n",
      "        >>> df.select(to_timestamp_ntz(df.e).alias('r')).collect()\n",
      "        ... # doctest: +SKIP\n",
      "        [Row(r=datetime.datetime(2016, 4, 8, 0, 0))]\n",
      "    \n",
      "    to_unix_timestamp(timestamp: 'ColumnOrName', format: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Returns the UNIX timestamp of the given time.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        format : :class:`~pyspark.sql.Column` or str, optional\n",
      "            format to use to convert UNIX timestamp values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([(\"2016-04-08\",)], [\"e\"])\n",
      "        >>> df.select(to_unix_timestamp(df.e, lit(\"yyyy-MM-dd\")).alias('r')).collect()\n",
      "        [Row(r=1460098800)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "        \n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([(\"2016-04-08\",)], [\"e\"])\n",
      "        >>> df.select(to_unix_timestamp(df.e).alias('r')).collect()\n",
      "        [Row(r=None)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    to_utc_timestamp(timestamp: 'ColumnOrName', tz: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n",
      "        takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in the given\n",
      "        timezone, and renders that timestamp as a timestamp in UTC.\n",
      "        \n",
      "        However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n",
      "        timezone-agnostic. So in Spark this function just shift the timestamp value from the given\n",
      "        timezone to UTC timezone.\n",
      "        \n",
      "        This function may return confusing result if the input is a string with timezone, e.g.\n",
      "        '2018-03-13T06:18:23+00:00'. The reason is that, Spark firstly cast the string to timestamp\n",
      "        according to the timezone in the string, and finally display the result by converting the\n",
      "        timestamp to string according to the session local timezone.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            the column that contains timestamps\n",
      "        tz : :class:`~pyspark.sql.Column` or str\n",
      "            A string detailing the time zone ID that the input should be adjusted to. It should\n",
      "            be in the format of either region-based zone IDs or zone offsets. Region IDs must\n",
      "            have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in\n",
      "            the format '(+|-)HH:mm', for example '-08:00' or '+01:00'. Also 'UTC' and 'Z' are\n",
      "            supported as aliases of '+00:00'. Other short names are not recommended to use\n",
      "            because they can be ambiguous.\n",
      "        \n",
      "            .. versionchanged:: 2.4.0\n",
      "               `tz` can take a :class:`~pyspark.sql.Column` containing timezone ID strings.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            timestamp value represented in UTC timezone.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n",
      "        >>> df.select(to_utc_timestamp(df.ts, \"PST\").alias('utc_time')).collect()\n",
      "        [Row(utc_time=datetime.datetime(1997, 2, 28, 18, 30))]\n",
      "        >>> df.select(to_utc_timestamp(df.ts, df.tz).alias('utc_time')).collect()\n",
      "        [Row(utc_time=datetime.datetime(1997, 2, 28, 1, 30))]\n",
      "    \n",
      "    to_varchar(col: 'ColumnOrName', format: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Convert `col` to a string based on the `format`.\n",
      "        Throws an exception if the conversion fails. The format can consist of the following\n",
      "        characters, case insensitive:\n",
      "        '0' or '9': Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the\n",
      "        format string matches a sequence of digits in the input value, generating a result\n",
      "        string of the same length as the corresponding sequence in the format string.\n",
      "        The result string is left-padded with zeros if the 0/9 sequence comprises more digits\n",
      "        than the matching part of the decimal value, starts with 0, and is before the decimal\n",
      "        point. Otherwise, it is padded with spaces.\n",
      "        '.' or 'D': Specifies the position of the decimal point (optional, only allowed once).\n",
      "        ',' or 'G': Specifies the position of the grouping (thousands) separator (,).\n",
      "        There must be a 0 or 9 to the left and right of each grouping separator.\n",
      "        '$': Specifies the location of the $ currency sign. This character may only be specified once.\n",
      "        'S' or 'MI': Specifies the position of a '-' or '+' sign (optional, only allowed once at\n",
      "        the beginning or end of the format string). Note that 'S' prints '+' for positive\n",
      "        values but 'MI' prints a space.\n",
      "        'PR': Only allowed at the end of the format string; specifies that the result string\n",
      "        will be wrapped by angle brackets if the input value is negative.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        format : :class:`~pyspark.sql.Column` or str, optional\n",
      "            format to use to convert char values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(78.12,)], [\"e\"])\n",
      "        >>> df.select(to_varchar(df.e, lit(\"$99.99\")).alias('r')).collect()\n",
      "        [Row(r='$78.12')]\n",
      "    \n",
      "    transform(col: 'ColumnOrName', f: Union[Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column], Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]]) -> pyspark.sql.column.Column\n",
      "        Returns an array of elements after applying a transformation to each element in the input array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a function that is applied to each element of the input array.\n",
      "            Can take one of the following forms:\n",
      "        \n",
      "            - Unary ``(x: Column) -> Column: ...``\n",
      "            - Binary ``(x: Column, i: Column) -> Column...``, where the second argument is\n",
      "                a 0-based index of the element.\n",
      "        \n",
      "            and can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a new array of transformed elements.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [1, 2, 3, 4])], (\"key\", \"values\"))\n",
      "        >>> df.select(transform(\"values\", lambda x: x * 2).alias(\"doubled\")).show()\n",
      "        +------------+\n",
      "        |     doubled|\n",
      "        +------------+\n",
      "        |[2, 4, 6, 8]|\n",
      "        +------------+\n",
      "        \n",
      "        >>> def alternate(x, i):\n",
      "        ...     return when(i % 2 == 0, x).otherwise(-x)\n",
      "        ...\n",
      "        >>> df.select(transform(\"values\", alternate).alias(\"alternated\")).show()\n",
      "        +--------------+\n",
      "        |    alternated|\n",
      "        +--------------+\n",
      "        |[1, -2, 3, -4]|\n",
      "        +--------------+\n",
      "    \n",
      "    transform_keys(col: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Applies a function to every key-value pair in a map and returns\n",
      "        a map with the results of those applications as the new keys for the pairs.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a binary function ``(k: Column, v: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a new map of enties where new keys were calculated by applying given function to\n",
      "            each key value argument.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, {\"foo\": -2.0, \"bar\": 2.0})], (\"id\", \"data\"))\n",
      "        >>> row = df.select(transform_keys(\n",
      "        ...     \"data\", lambda k, _: upper(k)).alias(\"data_upper\")\n",
      "        ... ).head()\n",
      "        >>> sorted(row[\"data_upper\"].items())\n",
      "        [('BAR', 2.0), ('FOO', -2.0)]\n",
      "    \n",
      "    transform_values(col: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Applies a function to every key-value pair in a map and returns\n",
      "        a map with the results of those applications as the new values for the pairs.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a binary function ``(k: Column, v: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a new map of enties where new values were calculated by applying given function to\n",
      "            each key value argument.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, {\"IT\": 10.0, \"SALES\": 2.0, \"OPS\": 24.0})], (\"id\", \"data\"))\n",
      "        >>> row = df.select(transform_values(\n",
      "        ...     \"data\", lambda k, v: when(k.isin(\"IT\", \"OPS\"), v + 10.0).otherwise(v)\n",
      "        ... ).alias(\"new_data\")).head()\n",
      "        >>> sorted(row[\"new_data\"].items())\n",
      "        [('IT', 20.0), ('OPS', 34.0), ('SALES', 2.0)]\n",
      "    \n",
      "    translate(srcCol: 'ColumnOrName', matching: str, replace: str) -> pyspark.sql.column.Column\n",
      "        A function translate any character in the `srcCol` by a character in `matching`.\n",
      "        The characters in `replace` is corresponding to the characters in `matching`.\n",
      "        Translation will happen whenever any character in the string is matching with the character\n",
      "        in the `matching`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        srcCol : :class:`~pyspark.sql.Column` or str\n",
      "            Source column or strings\n",
      "        matching : str\n",
      "            matching characters.\n",
      "        replace : str\n",
      "            characters for replacement. If this is shorter than `matching` string then\n",
      "            those chars that don't have replacement will be dropped.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            replaced value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('translate',)], ['a']).select(translate('a', \"rnlt\", \"123\") \\\n",
      "        ...     .alias('r')).collect()\n",
      "        [Row(r='1a2s3ae')]\n",
      "    \n",
      "    trim(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Trim the spaces from both ends for the specified string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            trimmed values from both sides.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n",
      "        >>> df.select(trim(\"value\").alias(\"r\")).withColumn(\"length\", length(\"r\")).show()\n",
      "        +-----+------+\n",
      "        |    r|length|\n",
      "        +-----+------+\n",
      "        |Spark|     5|\n",
      "        |Spark|     5|\n",
      "        |Spark|     5|\n",
      "        +-----+------+\n",
      "    \n",
      "    trunc(date: 'ColumnOrName', format: str) -> pyspark.sql.column.Column\n",
      "        Returns date truncated to the unit specified by the format.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        date : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to truncate.\n",
      "        format : str\n",
      "            'year', 'yyyy', 'yy' to truncate by year,\n",
      "            or 'month', 'mon', 'mm' to truncate by month\n",
      "            Other options are: 'week', 'quarter'\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            truncated date.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28',)], ['d'])\n",
      "        >>> df.select(trunc(df.d, 'year').alias('year')).collect()\n",
      "        [Row(year=datetime.date(1997, 1, 1))]\n",
      "        >>> df.select(trunc(df.d, 'mon').alias('month')).collect()\n",
      "        [Row(month=datetime.date(1997, 2, 1))]\n",
      "    \n",
      "    try_add(left: 'ColumnOrName', right: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the sum of `left`and `right` and the result is null on overflow.\n",
      "        The acceptable input types are the same with the `+` operator.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : :class:`~pyspark.sql.Column` or str\n",
      "        right : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1982, 15), (1990, 2)], [\"birth\", \"age\"])\n",
      "        >>> df.select(try_add(df.birth, df.age).alias('r')).collect()\n",
      "        [Row(r=1997), Row(r=1992)]\n",
      "        \n",
      "        >>> from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
      "        >>> schema = StructType([\n",
      "        ...     StructField(\"i\", IntegerType(), True),\n",
      "        ...     StructField(\"d\", StringType(), True),\n",
      "        ... ])\n",
      "        >>> df = spark.createDataFrame([(1, '2015-09-30')], schema)\n",
      "        >>> df = df.select(df.i, to_date(df.d).alias('d'))\n",
      "        >>> df.select(try_add(df.d, df.i).alias('r')).collect()\n",
      "        [Row(r=datetime.date(2015, 10, 1))]\n",
      "        \n",
      "        >>> df.select(try_add(df.d, make_interval(df.i)).alias('r')).collect()\n",
      "        [Row(r=datetime.date(2016, 9, 30))]\n",
      "        \n",
      "        >>> df.select(\n",
      "        ...     try_add(df.d, make_interval(lit(0), lit(0), lit(0), df.i)).alias('r')\n",
      "        ... ).collect()\n",
      "        [Row(r=datetime.date(2015, 10, 1))]\n",
      "        \n",
      "        >>> df.select(\n",
      "        ...     try_add(make_interval(df.i), make_interval(df.i)).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +-------+\n",
      "        |r      |\n",
      "        +-------+\n",
      "        |2 years|\n",
      "        +-------+\n",
      "    \n",
      "    try_aes_decrypt(input: 'ColumnOrName', key: 'ColumnOrName', mode: Optional[ForwardRef('ColumnOrName')] = None, padding: Optional[ForwardRef('ColumnOrName')] = None, aad: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        This is a special version of `aes_decrypt` that performs the same operation,\n",
      "        but returns a NULL value instead of raising an error if the decryption cannot be performed.\n",
      "        Returns a decrypted value of `input` using AES in `mode` with `padding`. Key lengths of 16,\n",
      "        24 and 32 bits are supported. Supported combinations of (`mode`, `padding`) are ('ECB',\n",
      "        'PKCS'), ('GCM', 'NONE') and ('CBC', 'PKCS'). Optional additional authenticated data (AAD) is\n",
      "        only supported for GCM. If provided for encryption, the identical AAD value must be provided\n",
      "        for decryption. The default mode is GCM.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        input : :class:`~pyspark.sql.Column` or str\n",
      "            The binary value to decrypt.\n",
      "        key : :class:`~pyspark.sql.Column` or str\n",
      "            The passphrase to use to decrypt the data.\n",
      "        mode : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Specifies which block cipher mode should be used to decrypt messages. Valid modes: ECB,\n",
      "            GCM, CBC.\n",
      "        padding : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Specifies how to pad messages whose length is not a multiple of the block size. Valid\n",
      "            values: PKCS, NONE, DEFAULT. The DEFAULT padding means PKCS for ECB, NONE for GCM and PKCS\n",
      "            for CBC.\n",
      "        aad : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Optional additional authenticated data. Only supported for GCM mode. This can be any\n",
      "            free-form input and must be provided for both encryption and decryption.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4\",\n",
      "        ...     \"abcdefghijklmnop12345678ABCDEFGH\", \"GCM\", \"DEFAULT\",\n",
      "        ...     \"This is an AAD mixed into the input\",)],\n",
      "        ...     [\"input\", \"key\", \"mode\", \"padding\", \"aad\"]\n",
      "        ... )\n",
      "        >>> df.select(try_aes_decrypt(\n",
      "        ...     unbase64(df.input), df.key, df.mode, df.padding, df.aad).alias('r')\n",
      "        ... ).collect()\n",
      "        [Row(r=bytearray(b'Spark'))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"AAAAAAAAAAAAAAAAAAAAAPSd4mWyMZ5mhvjiAPQJnfg=\",\n",
      "        ...     \"abcdefghijklmnop12345678ABCDEFGH\", \"CBC\", \"DEFAULT\",)],\n",
      "        ...     [\"input\", \"key\", \"mode\", \"padding\"]\n",
      "        ... )\n",
      "        >>> df.select(try_aes_decrypt(\n",
      "        ...     unbase64(df.input), df.key, df.mode, df.padding).alias('r')\n",
      "        ... ).collect()\n",
      "        [Row(r=bytearray(b'Spark'))]\n",
      "        \n",
      "        >>> df.select(try_aes_decrypt(unbase64(df.input), df.key, df.mode).alias('r')).collect()\n",
      "        [Row(r=bytearray(b'Spark'))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"83F16B2AA704794132802D248E6BFD4E380078182D1544813898AC97E709B28A94\",\n",
      "        ...     \"0000111122223333\",)],\n",
      "        ...     [\"input\", \"key\"]\n",
      "        ... )\n",
      "        >>> df.select(try_aes_decrypt(unhex(df.input), df.key).alias('r')).collect()\n",
      "        [Row(r=bytearray(b'Spark'))]\n",
      "    \n",
      "    try_avg(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the mean calculated from values of a group and the result is null on overflow.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(1982, 15), (1990, 2)], [\"birth\", \"age\"]\n",
      "        ... ).select(sf.try_avg(\"age\")).show()\n",
      "        +------------+\n",
      "        |try_avg(age)|\n",
      "        +------------+\n",
      "        |         8.5|\n",
      "        +------------+\n",
      "    \n",
      "    try_divide(left: 'ColumnOrName', right: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns `dividend`/`divisor`. It always performs floating point division. Its result is\n",
      "        always null if `divisor` is 0.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : :class:`~pyspark.sql.Column` or str\n",
      "            dividend\n",
      "        right : :class:`~pyspark.sql.Column` or str\n",
      "            divisor\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(6000, 15), (1990, 2)], [\"a\", \"b\"])\n",
      "        >>> df.select(try_divide(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=400.0), Row(r=995.0)]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, 2)], [\"year\", \"month\"])\n",
      "        >>> df.select(\n",
      "        ...     try_divide(make_interval(df.year), df.month).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +--------+\n",
      "        |r       |\n",
      "        +--------+\n",
      "        |6 months|\n",
      "        +--------+\n",
      "        \n",
      "        >>> df.select(\n",
      "        ...     try_divide(make_interval(df.year, df.month), lit(2)).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +--------+\n",
      "        |r       |\n",
      "        +--------+\n",
      "        |7 months|\n",
      "        +--------+\n",
      "        \n",
      "        >>> df.select(\n",
      "        ...     try_divide(make_interval(df.year, df.month), lit(0)).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +----+\n",
      "        |r   |\n",
      "        +----+\n",
      "        |NULL|\n",
      "        +----+\n",
      "    \n",
      "    try_element_at(col: 'ColumnOrName', extraction: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        (array, index) - Returns element of array at given (1-based) index. If Index is 0, Spark will\n",
      "        throw an error. If index < 0, accesses elements from the last to the first. The function\n",
      "        always returns NULL if the index exceeds the length of the array.\n",
      "        \n",
      "        (map, key) - Returns value for given key. The function always returns NULL if the key is not\n",
      "        contained in the map.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array or map\n",
      "        extraction :\n",
      "            index to check for in array or key to check for in map\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], ['data'])\n",
      "        >>> df.select(try_element_at(df.data, lit(1)).alias('r')).collect()\n",
      "        [Row(r='a')]\n",
      "        >>> df.select(try_element_at(df.data, lit(-1)).alias('r')).collect()\n",
      "        [Row(r='c')]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},)], ['data'])\n",
      "        >>> df.select(try_element_at(df.data, lit(\"a\")).alias('r')).collect()\n",
      "        [Row(r=1.0)]\n",
      "    \n",
      "    try_multiply(left: 'ColumnOrName', right: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns `left`*`right` and the result is null on overflow. The acceptable input types are the\n",
      "        same with the `*` operator.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : :class:`~pyspark.sql.Column` or str\n",
      "            multiplicand\n",
      "        right : :class:`~pyspark.sql.Column` or str\n",
      "            multiplier\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(6000, 15), (1990, 2)], [\"a\", \"b\"])\n",
      "        >>> df.select(try_multiply(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=90000), Row(r=3980)]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(2, 3),], [\"a\", \"b\"])\n",
      "        >>> df.select(try_multiply(make_interval(df.a), df.b).alias('r')).show(truncate=False)\n",
      "        +-------+\n",
      "        |r      |\n",
      "        +-------+\n",
      "        |6 years|\n",
      "        +-------+\n",
      "    \n",
      "    try_subtract(left: 'ColumnOrName', right: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns `left`-`right` and the result is null on overflow. The acceptable input types are the\n",
      "        same with the `-` operator.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : :class:`~pyspark.sql.Column` or str\n",
      "        right : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(6000, 15), (1990, 2)], [\"a\", \"b\"])\n",
      "        >>> df.select(try_subtract(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=5985), Row(r=1988)]\n",
      "        \n",
      "        >>> from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
      "        >>> schema = StructType([\n",
      "        ...     StructField(\"i\", IntegerType(), True),\n",
      "        ...     StructField(\"d\", StringType(), True),\n",
      "        ... ])\n",
      "        >>> df = spark.createDataFrame([(1, '2015-09-30')], schema)\n",
      "        >>> df = df.select(df.i, to_date(df.d).alias('d'))\n",
      "        >>> df.select(try_subtract(df.d, df.i).alias('r')).collect()\n",
      "        [Row(r=datetime.date(2015, 9, 29))]\n",
      "        \n",
      "        >>> df.select(try_subtract(df.d, make_interval(df.i)).alias('r')).collect()\n",
      "        [Row(r=datetime.date(2014, 9, 30))]\n",
      "        \n",
      "        >>> df.select(\n",
      "        ...     try_subtract(df.d, make_interval(lit(0), lit(0), lit(0), df.i)).alias('r')\n",
      "        ... ).collect()\n",
      "        [Row(r=datetime.date(2015, 9, 29))]\n",
      "        \n",
      "        >>> df.select(\n",
      "        ...     try_subtract(make_interval(df.i), make_interval(df.i)).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +---------+\n",
      "        |r        |\n",
      "        +---------+\n",
      "        |0 seconds|\n",
      "        +---------+\n",
      "    \n",
      "    try_sum(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the sum calculated from values of a group and the result is null on overflow.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(10).select(sf.try_sum(\"id\")).show()\n",
      "        +-----------+\n",
      "        |try_sum(id)|\n",
      "        +-----------+\n",
      "        |         45|\n",
      "        +-----------+\n",
      "    \n",
      "    try_to_binary(col: 'ColumnOrName', format: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        This is a special version of `to_binary` that performs the same operation, but returns a NULL\n",
      "        value instead of raising an error if the conversion cannot be performed.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        format : :class:`~pyspark.sql.Column` or str, optional\n",
      "            format to use to convert binary values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"abc\",)], [\"e\"])\n",
      "        >>> df.select(try_to_binary(df.e, lit(\"utf-8\")).alias('r')).collect()\n",
      "        [Row(r=bytearray(b'abc'))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"414243\",)], [\"e\"])\n",
      "        >>> df.select(try_to_binary(df.e).alias('r')).collect()\n",
      "        [Row(r=bytearray(b'ABC'))]\n",
      "    \n",
      "    try_to_number(col: 'ColumnOrName', format: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Convert string 'col' to a number based on the string format `format`. Returns NULL if the\n",
      "        string 'col' does not match the expected format. The format follows the same semantics as the\n",
      "        to_number function.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        format : :class:`~pyspark.sql.Column` or str, optional\n",
      "            format to use to convert number values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"$78.12\",)], [\"e\"])\n",
      "        >>> df.select(try_to_number(df.e, lit(\"$99.99\")).alias('r')).collect()\n",
      "        [Row(r=Decimal('78.12'))]\n",
      "    \n",
      "    try_to_timestamp(col: 'ColumnOrName', format: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Parses the `col` with the `format` to a timestamp. The function always\n",
      "        returns null on an invalid input with/without ANSI SQL mode enabled. The result data type is\n",
      "        consistent with the value of configuration `spark.sql.timestampType`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column values to convert.\n",
      "        format: str, optional\n",
      "            format to use to convert timestamp values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(try_to_timestamp(df.t).alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "        \n",
      "        >>> df.select(try_to_timestamp(df.t, lit('yyyy-MM-dd HH:mm:ss')).alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "    \n",
      "    typeof(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Return DDL-formatted type string for the data type of the input.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1,)], [\"a\"])\n",
      "        >>> df.select(typeof(df.a).alias('r')).collect()\n",
      "        [Row(r='bigint')]\n",
      "    \n",
      "    ucase(str: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns `str` with all characters changed to uppercase.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.ucase(sf.lit(\"Spark\"))).show()\n",
      "        +------------+\n",
      "        |ucase(Spark)|\n",
      "        +------------+\n",
      "        |       SPARK|\n",
      "        +------------+\n",
      "    \n",
      "    udf(f: Union[Callable[..., Any], ForwardRef('DataTypeOrString'), NoneType] = None, returnType: 'DataTypeOrString' = StringType(), *, useArrow: Optional[bool] = None) -> Union[ForwardRef('UserDefinedFunctionLike'), Callable[[Callable[..., Any]], ForwardRef('UserDefinedFunctionLike')]]\n",
      "        Creates a user defined function (UDF).\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            python function if used as a standalone function\n",
      "        returnType : :class:`pyspark.sql.types.DataType` or str\n",
      "            the return type of the user-defined function. The value can be either a\n",
      "            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "        useArrow : bool or None\n",
      "            whether to use Arrow to optimize the (de)serialization. When it is None, the\n",
      "            Spark config \"spark.sql.execution.pythonUDF.arrow.enabled\" takes effect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> slen = udf(lambda s: len(s), IntegerType())\n",
      "        >>> @udf\n",
      "        ... def to_upper(s):\n",
      "        ...     if s is not None:\n",
      "        ...         return s.upper()\n",
      "        ...\n",
      "        >>> @udf(returnType=IntegerType())\n",
      "        ... def add_one(x):\n",
      "        ...     if x is not None:\n",
      "        ...         return x + 1\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
      "        >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n",
      "        +----------+--------------+------------+\n",
      "        |slen(name)|to_upper(name)|add_one(age)|\n",
      "        +----------+--------------+------------+\n",
      "        |         8|      JOHN DOE|          22|\n",
      "        +----------+--------------+------------+\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The user-defined functions are considered deterministic by default. Due to\n",
      "        optimization, duplicate invocations may be eliminated or the function may even be invoked\n",
      "        more times than it is present in the query. If your function is not deterministic, call\n",
      "        `asNondeterministic` on the user defined function. E.g.:\n",
      "        \n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> import random\n",
      "        >>> random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n",
      "        \n",
      "        The user-defined functions do not support conditional expressions or short circuiting\n",
      "        in boolean expressions and it ends up with being executed all internally. If the functions\n",
      "        can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
      "        \n",
      "        The user-defined functions do not take keyword arguments on the calling side.\n",
      "    \n",
      "    udtf(cls: Optional[Type] = None, *, returnType: Union[pyspark.sql.types.StructType, str], useArrow: Optional[bool] = None) -> Union[ForwardRef('UserDefinedTableFunction'), Callable[[Type], ForwardRef('UserDefinedTableFunction')]]\n",
      "        Creates a user defined table function (UDTF).\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cls : class\n",
      "            the Python user-defined table function handler class.\n",
      "        returnType : :class:`pyspark.sql.types.StructType` or str\n",
      "            the return type of the user-defined table function. The value can be either a\n",
      "            :class:`pyspark.sql.types.StructType` object or a DDL-formatted struct type string.\n",
      "        useArrow : bool or None, optional\n",
      "            whether to use Arrow to optimize the (de)serializations. When it's set to None, the\n",
      "            Spark config \"spark.sql.execution.pythonUDTF.arrow.enabled\" is used.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Implement the UDTF class and create a UDTF:\n",
      "        \n",
      "        >>> class TestUDTF:\n",
      "        ...     def eval(self, *args: Any):\n",
      "        ...         yield \"hello\", \"world\"\n",
      "        ...\n",
      "        >>> from pyspark.sql.functions import udtf\n",
      "        >>> test_udtf = udtf(TestUDTF, returnType=\"c1: string, c2: string\")\n",
      "        >>> test_udtf().show()\n",
      "        +-----+-----+\n",
      "        |   c1|   c2|\n",
      "        +-----+-----+\n",
      "        |hello|world|\n",
      "        +-----+-----+\n",
      "        \n",
      "        UDTF can also be created using the decorator syntax:\n",
      "        \n",
      "        >>> @udtf(returnType=\"c1: int, c2: int\")\n",
      "        ... class PlusOne:\n",
      "        ...     def eval(self, x: int):\n",
      "        ...         yield x, x + 1\n",
      "        ...\n",
      "        >>> from pyspark.sql.functions import lit\n",
      "        >>> PlusOne(lit(1)).show()\n",
      "        +---+---+\n",
      "        | c1| c2|\n",
      "        +---+---+\n",
      "        |  1|  2|\n",
      "        +---+---+\n",
      "        \n",
      "        Arrow optimization can be explicitly enabled when creating UDTFs:\n",
      "        \n",
      "        >>> @udtf(returnType=\"c1: int, c2: int\", useArrow=True)\n",
      "        ... class ArrowPlusOne:\n",
      "        ...     def eval(self, x: int):\n",
      "        ...         yield x, x + 1\n",
      "        ...\n",
      "        >>> ArrowPlusOne(lit(1)).show()\n",
      "        +---+---+\n",
      "        | c1| c2|\n",
      "        +---+---+\n",
      "        |  1|  2|\n",
      "        +---+---+\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        User-defined table functions (UDTFs) are considered non-deterministic by default.\n",
      "        Use `asDeterministic()` to mark a function as deterministic. E.g.:\n",
      "        \n",
      "        >>> class PlusOne:\n",
      "        ...     def eval(self, a: int):\n",
      "        ...         yield a + 1,\n",
      "        >>> plus_one = udtf(PlusOne, returnType=\"r: int\").asDeterministic()\n",
      "        \n",
      "        Use \"yield\" to produce one row for the UDTF result relation as many times\n",
      "        as needed. In the context of a lateral join, each such result row will be\n",
      "        associated with the most recent input row consumed from the \"eval\" method.\n",
      "        \n",
      "        User-defined table functions are considered opaque to the optimizer by default.\n",
      "        As a result, operations like filters from WHERE clauses or limits from\n",
      "        LIMIT/OFFSET clauses that appear after the UDTF call will execute on the\n",
      "        UDTF's result relation. By the same token, any relations forwarded as input\n",
      "        to UDTFs will plan as full table scans in the absence of any explicit such\n",
      "        filtering or other logic explicitly written in a table subquery surrounding the\n",
      "        provided input relation.\n",
      "        \n",
      "        User-defined table functions do not accept keyword arguments on the calling side.\n",
      "    \n",
      "    unbase64(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Decodes a BASE64 encoded string column and returns it as a binary column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            encoded string value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"U3Bhcms=\",\n",
      "        ...                             \"UHlTcGFyaw==\",\n",
      "        ...                             \"UGFuZGFzIEFQSQ==\"], \"STRING\")\n",
      "        >>> df.select(unbase64(\"value\")).show()\n",
      "        +--------------------+\n",
      "        |     unbase64(value)|\n",
      "        +--------------------+\n",
      "        |    [53 70 61 72 6B]|\n",
      "        |[50 79 53 70 61 7...|\n",
      "        |[50 61 6E 64 61 7...|\n",
      "        +--------------------+\n",
      "    \n",
      "    unhex(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Inverse of hex. Interprets each pair of characters as a hexadecimal number\n",
      "        and converts to the byte representation of number.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string representation of given hexadecimal value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('414243',)], ['a']).select(unhex('a')).collect()\n",
      "        [Row(unhex(a)=bytearray(b'ABC'))]\n",
      "    \n",
      "    unix_date(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of days since 1970-01-01.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([('1970-01-02',)], ['t'])\n",
      "        >>> df.select(unix_date(to_date(df.t)).alias('n')).collect()\n",
      "        [Row(n=1)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    unix_micros(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of microseconds since 1970-01-01 00:00:00 UTC.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([('2015-07-22 10:00:00',)], ['t'])\n",
      "        >>> df.select(unix_micros(to_timestamp(df.t)).alias('n')).collect()\n",
      "        [Row(n=1437584400000000)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    unix_millis(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of milliseconds since 1970-01-01 00:00:00 UTC.\n",
      "        Truncates higher levels of precision.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([('2015-07-22 10:00:00',)], ['t'])\n",
      "        >>> df.select(unix_millis(to_timestamp(df.t)).alias('n')).collect()\n",
      "        [Row(n=1437584400000)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    unix_seconds(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of seconds since 1970-01-01 00:00:00 UTC.\n",
      "        Truncates higher levels of precision.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([('2015-07-22 10:00:00',)], ['t'])\n",
      "        >>> df.select(unix_seconds(to_timestamp(df.t)).alias('n')).collect()\n",
      "        [Row(n=1437584400)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    unix_timestamp(timestamp: Optional[ForwardRef('ColumnOrName')] = None, format: str = 'yyyy-MM-dd HH:mm:ss') -> pyspark.sql.column.Column\n",
      "        Convert time string with given pattern ('yyyy-MM-dd HH:mm:ss', by default)\n",
      "        to Unix time stamp (in seconds), using the default timezone and the default\n",
      "        locale, returns null if failed.\n",
      "        \n",
      "        if `timestamp` is None, then it returns current timestamp.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str, optional\n",
      "            timestamps of string values.\n",
      "        format : str, optional\n",
      "            alternative format to use for converting (default: yyyy-MM-dd HH:mm:ss).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            unix time as long integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> time_df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> time_df.select(unix_timestamp('dt', 'yyyy-MM-dd').alias('unix_time')).collect()\n",
      "        [Row(unix_time=1428476400)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    unwrap_udt(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Unwrap UDT data type column into its underlying type.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    upper(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Converts a string expression to upper case.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            upper case values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
      "        >>> df.select(upper(\"value\")).show()\n",
      "        +------------+\n",
      "        |upper(value)|\n",
      "        +------------+\n",
      "        |       SPARK|\n",
      "        |     PYSPARK|\n",
      "        |  PANDAS API|\n",
      "        +------------+\n",
      "    \n",
      "    url_decode(str: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Decodes a `str` in 'application/x-www-form-urlencoded' format\n",
      "        using a specific encoding scheme.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string to decode.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"https%3A%2F%2Fspark.apache.org\",)], [\"a\"])\n",
      "        >>> df.select(url_decode(df.a).alias('r')).collect()\n",
      "        [Row(r='https://spark.apache.org')]\n",
      "    \n",
      "    url_encode(str: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Translates a string into 'application/x-www-form-urlencoded' format\n",
      "        using a specific encoding scheme.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string to encode.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"https://spark.apache.org\",)], [\"a\"])\n",
      "        >>> df.select(url_encode(df.a).alias('r')).collect()\n",
      "        [Row(r='https%3A%2F%2Fspark.apache.org')]\n",
      "    \n",
      "    user() -> pyspark.sql.column.Column\n",
      "        Returns the current database.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.user()).show() # doctest: +SKIP\n",
      "        +--------------+\n",
      "        |current_user()|\n",
      "        +--------------+\n",
      "        | ruifeng.zheng|\n",
      "        +--------------+\n",
      "    \n",
      "    var_pop(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the population variance of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            variance of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(6)\n",
      "        >>> df.select(var_pop(df.id)).first()\n",
      "        Row(var_pop(id)=2.91666...)\n",
      "    \n",
      "    var_samp(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the unbiased sample variance of\n",
      "        the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            variance of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(6)\n",
      "        >>> df.select(var_samp(df.id)).show()\n",
      "        +------------+\n",
      "        |var_samp(id)|\n",
      "        +------------+\n",
      "        |         3.5|\n",
      "        +------------+\n",
      "    \n",
      "    variance(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: alias for var_samp\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            variance of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(6)\n",
      "        >>> df.select(variance(df.id)).show()\n",
      "        +------------+\n",
      "        |var_samp(id)|\n",
      "        +------------+\n",
      "        |         3.5|\n",
      "        +------------+\n",
      "    \n",
      "    version() -> pyspark.sql.column.Column\n",
      "        Returns the Spark version. The string contains 2 fields, the first being a release version\n",
      "        and the second being a git revision.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(version()).show(truncate=False) # doctest: +SKIP\n",
      "        +----------------------------------------------+\n",
      "        |version()                                     |\n",
      "        +----------------------------------------------+\n",
      "        |3.5.0 cafbea5b13623276517a9d716f75745eff91f616|\n",
      "        +----------------------------------------------+\n",
      "    \n",
      "    weekday(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the day of the week for date/timestamp (0 = Monday, 1 = Tuesday, ..., 6 = Sunday).\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the day of the week for date/timestamp (0 = Monday, 1 = Tuesday, ..., 6 = Sunday).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(weekday('dt').alias('day')).show()\n",
      "        +---+\n",
      "        |day|\n",
      "        +---+\n",
      "        |  2|\n",
      "        +---+\n",
      "    \n",
      "    weekofyear(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the week number of a given date as integer.\n",
      "        A week is considered to start on a Monday and week 1 is the first week with more than 3 days,\n",
      "        as defined by ISO 8601\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            `week` of the year for given date as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(weekofyear(df.dt).alias('week')).collect()\n",
      "        [Row(week=15)]\n",
      "    \n",
      "    when(condition: pyspark.sql.column.Column, value: Any) -> pyspark.sql.column.Column\n",
      "        Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "        If :func:`pyspark.sql.Column.otherwise` is not invoked, None is returned for unmatched\n",
      "        conditions.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        condition : :class:`~pyspark.sql.Column`\n",
      "            a boolean :class:`~pyspark.sql.Column` expression.\n",
      "        value :\n",
      "            a literal value, or a :class:`~pyspark.sql.Column` expression.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column representing when expression.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(3)\n",
      "        >>> df.select(when(df['id'] == 2, 3).otherwise(4).alias(\"age\")).show()\n",
      "        +---+\n",
      "        |age|\n",
      "        +---+\n",
      "        |  4|\n",
      "        |  4|\n",
      "        |  3|\n",
      "        +---+\n",
      "        \n",
      "        >>> df.select(when(df.id == 2, df.id + 1).alias(\"age\")).show()\n",
      "        +----+\n",
      "        | age|\n",
      "        +----+\n",
      "        |NULL|\n",
      "        |NULL|\n",
      "        |   3|\n",
      "        +----+\n",
      "    \n",
      "    width_bucket(v: 'ColumnOrName', min: 'ColumnOrName', max: 'ColumnOrName', numBucket: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Returns the bucket number into which the value of this expression would fall\n",
      "        after being evaluated. Note that input arguments must follow conditions listed below;\n",
      "        otherwise, the method will return null.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : str or :class:`~pyspark.sql.Column`\n",
      "            value to compute a bucket number in the histogram\n",
      "        min : str or :class:`~pyspark.sql.Column`\n",
      "            minimum value of the histogram\n",
      "        max : str or :class:`~pyspark.sql.Column`\n",
      "            maximum value of the histogram\n",
      "        numBucket : str, :class:`~pyspark.sql.Column` or int\n",
      "            the number of buckets\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the bucket number into which the value would fall after being evaluated\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (5.3, 0.2, 10.6, 5),\n",
      "        ...     (-2.1, 1.3, 3.4, 3),\n",
      "        ...     (8.1, 0.0, 5.7, 4),\n",
      "        ...     (-0.9, 5.2, 0.5, 2)],\n",
      "        ...     ['v', 'min', 'max', 'n'])\n",
      "        >>> df.select(width_bucket('v', 'min', 'max', 'n')).show()\n",
      "        +----------------------------+\n",
      "        |width_bucket(v, min, max, n)|\n",
      "        +----------------------------+\n",
      "        |                           3|\n",
      "        |                           0|\n",
      "        |                           5|\n",
      "        |                           3|\n",
      "        +----------------------------+\n",
      "    \n",
      "    window(timeColumn: 'ColumnOrName', windowDuration: str, slideDuration: Optional[str] = None, startTime: Optional[str] = None) -> pyspark.sql.column.Column\n",
      "        Bucketize rows into one or more time windows given a timestamp specifying column. Window\n",
      "        starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window\n",
      "        [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in\n",
      "        the order of months are not supported.\n",
      "        \n",
      "        The time column must be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        Durations are provided as strings, e.g. '1 second', '1 day 12 hours', '2 minutes'. Valid\n",
      "        interval strings are 'week', 'day', 'hour', 'minute', 'second', 'millisecond', 'microsecond'.\n",
      "        If the ``slideDuration`` is not provided, the windows will be tumbling windows.\n",
      "        \n",
      "        The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start\n",
      "        window intervals. For example, in order to have hourly tumbling windows that start 15 minutes\n",
      "        past the hour, e.g. 12:15-13:15, 13:15-14:15... provide `startTime` as `15 minutes`.\n",
      "        \n",
      "        The output column will be a struct called 'window' by default with the nested columns 'start'\n",
      "        and 'end', where 'start' and 'end' will be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timeColumn : :class:`~pyspark.sql.Column`\n",
      "            The column or the expression to use as the timestamp for windowing by time.\n",
      "            The time column must be of TimestampType or TimestampNTZType.\n",
      "        windowDuration : str\n",
      "            A string specifying the width of the window, e.g. `10 minutes`,\n",
      "            `1 second`. Check `org.apache.spark.unsafe.types.CalendarInterval` for\n",
      "            valid duration identifiers. Note that the duration is a fixed length of\n",
      "            time, and does not vary over time according to a calendar. For example,\n",
      "            `1 day` always means 86,400,000 milliseconds, not a calendar day.\n",
      "        slideDuration : str, optional\n",
      "            A new window will be generated every `slideDuration`. Must be less than\n",
      "            or equal to the `windowDuration`. Check\n",
      "            `org.apache.spark.unsafe.types.CalendarInterval` for valid duration\n",
      "            identifiers. This duration is likewise absolute, and does not vary\n",
      "            according to a calendar.\n",
      "        startTime : str, optional\n",
      "            The offset with respect to 1970-01-01 00:00:00 UTC with which to start\n",
      "            window intervals. For example, in order to have hourly tumbling windows that\n",
      "            start 15 minutes past the hour, e.g. 12:15-13:15, 13:15-14:15... provide\n",
      "            `startTime` as `15 minutes`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(datetime.datetime(2016, 3, 11, 9, 0, 7), 1)],\n",
      "        ... ).toDF(\"date\", \"val\")\n",
      "        >>> w = df.groupBy(window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n",
      "        >>> w.select(w.window.start.cast(\"string\").alias(\"start\"),\n",
      "        ...          w.window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\n",
      "        [Row(start='2016-03-11 09:00:05', end='2016-03-11 09:00:10', sum=1)]\n",
      "    \n",
      "    window_time(windowColumn: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the event time from a window column. The column window values are produced\n",
      "        by window aggregating operators and are of type `STRUCT<start: TIMESTAMP, end: TIMESTAMP>`\n",
      "        where start is inclusive and end is exclusive. The event time of records produced by window\n",
      "        aggregating operators can be computed as ``window_time(window)`` and are\n",
      "        ``window.end - lit(1).alias(\"microsecond\")`` (as microsecond is the minimal supported event\n",
      "        time precision). The window column must be one produced by a window aggregating operator.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        windowColumn : :class:`~pyspark.sql.Column`\n",
      "            The window column of a window aggregate records.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(datetime.datetime(2016, 3, 11, 9, 0, 7), 1)],\n",
      "        ... ).toDF(\"date\", \"val\")\n",
      "        \n",
      "        Group the data into 5 second time windows and aggregate as sum.\n",
      "        \n",
      "        >>> w = df.groupBy(window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n",
      "        \n",
      "        Extract the window event time using the window_time function.\n",
      "        \n",
      "        >>> w.select(\n",
      "        ...     w.window.end.cast(\"string\").alias(\"end\"),\n",
      "        ...     window_time(w.window).cast(\"string\").alias(\"window_time\"),\n",
      "        ...     \"sum\"\n",
      "        ... ).collect()\n",
      "        [Row(end='2016-03-11 09:00:10', window_time='2016-03-11 09:00:09.999999', sum=1)]\n",
      "    \n",
      "    xpath(xml: 'ColumnOrName', path: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a string array of values within the nodes of xml that match the XPath expression.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [('<a><b>b1</b><b>b2</b><b>b3</b><c>c1</c><c>c2</c></a>',)], ['x'])\n",
      "        >>> df.select(xpath(df.x, lit('a/b/text()')).alias('r')).collect()\n",
      "        [Row(r=['b1', 'b2', 'b3'])]\n",
      "    \n",
      "    xpath_boolean(xml: 'ColumnOrName', path: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns true if the XPath expression evaluates to true, or if a matching node is found.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('<a><b>1</b></a>',)], ['x'])\n",
      "        >>> df.select(xpath_boolean(df.x, lit('a/b')).alias('r')).collect()\n",
      "        [Row(r=True)]\n",
      "    \n",
      "    xpath_double(xml: 'ColumnOrName', path: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a double value, the value zero if no match is found,\n",
      "        or NaN if a match is found but the value is non-numeric.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n",
      "        >>> df.select(xpath_double(df.x, lit('sum(a/b)')).alias('r')).collect()\n",
      "        [Row(r=3.0)]\n",
      "    \n",
      "    xpath_float(xml: 'ColumnOrName', path: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a float value, the value zero if no match is found,\n",
      "        or NaN if a match is found but the value is non-numeric.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n",
      "        >>> df.select(xpath_float(df.x, lit('sum(a/b)')).alias('r')).collect()\n",
      "        [Row(r=3.0)]\n",
      "    \n",
      "    xpath_int(xml: 'ColumnOrName', path: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns an integer value, or the value zero if no match is found,\n",
      "        or a match is found but the value is non-numeric.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n",
      "        >>> df.select(xpath_int(df.x, lit('sum(a/b)')).alias('r')).collect()\n",
      "        [Row(r=3)]\n",
      "    \n",
      "    xpath_long(xml: 'ColumnOrName', path: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a long integer value, or the value zero if no match is found,\n",
      "        or a match is found but the value is non-numeric.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n",
      "        >>> df.select(xpath_long(df.x, lit('sum(a/b)')).alias('r')).collect()\n",
      "        [Row(r=3)]\n",
      "    \n",
      "    xpath_number(xml: 'ColumnOrName', path: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a double value, the value zero if no match is found,\n",
      "        or NaN if a match is found but the value is non-numeric.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [('<a><b>1</b><b>2</b></a>',)], ['x']\n",
      "        ... ).select(sf.xpath_number('x', sf.lit('sum(a/b)'))).show()\n",
      "        +-------------------------+\n",
      "        |xpath_number(x, sum(a/b))|\n",
      "        +-------------------------+\n",
      "        |                      3.0|\n",
      "        +-------------------------+\n",
      "    \n",
      "    xpath_short(xml: 'ColumnOrName', path: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a short integer value, or the value zero if no match is found,\n",
      "        or a match is found but the value is non-numeric.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n",
      "        >>> df.select(xpath_short(df.x, lit('sum(a/b)')).alias('r')).collect()\n",
      "        [Row(r=3)]\n",
      "    \n",
      "    xpath_string(xml: 'ColumnOrName', path: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the text contents of the first xml node that matches the XPath expression.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('<a><b>b</b><c>cc</c></a>',)], ['x'])\n",
      "        >>> df.select(xpath_string(df.x, lit('a/c')).alias('r')).collect()\n",
      "        [Row(r='cc')]\n",
      "    \n",
      "    xxhash64(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calculates the hash code of given columns using the 64-bit variant of the xxHash algorithm,\n",
      "        and returns the result as a long column. The hash computation uses an initial seed of 42.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            one or more columns to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hash value as long column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('ABC', 'DEF')], ['c1', 'c2'])\n",
      "        \n",
      "        Hash for one column\n",
      "        \n",
      "        >>> df.select(xxhash64('c1').alias('hash')).show()\n",
      "        +-------------------+\n",
      "        |               hash|\n",
      "        +-------------------+\n",
      "        |4105715581806190027|\n",
      "        +-------------------+\n",
      "        \n",
      "        Two or more columns\n",
      "        \n",
      "        >>> df.select(xxhash64('c1', 'c2').alias('hash')).show()\n",
      "        +-------------------+\n",
      "        |               hash|\n",
      "        +-------------------+\n",
      "        |3233247871021311208|\n",
      "        +-------------------+\n",
      "    \n",
      "    year(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the year of a given date/timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            year part of the date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(year('dt').alias('year')).collect()\n",
      "        [Row(year=2015)]\n",
      "    \n",
      "    years(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Partition transform function: A transform for timestamps and dates\n",
      "        to partition data into years.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date or timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            data partitioned by years.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(  # doctest: +SKIP\n",
      "        ...     years(\"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    zip_with(left: 'ColumnOrName', right: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Merge two given arrays, element-wise, into a single array using a function.\n",
      "        If one array is shorter, nulls are appended at the end to match the length of the longer\n",
      "        array, before applying the function.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : :class:`~pyspark.sql.Column` or str\n",
      "            name of the first column or expression\n",
      "        right : :class:`~pyspark.sql.Column` or str\n",
      "            name of the second column or expression\n",
      "        f : function\n",
      "            a binary function ``(x1: Column, x2: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            array of calculated values derived by applying given function to each pair of arguments.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [1, 3, 5, 8], [0, 2, 4, 6])], (\"id\", \"xs\", \"ys\"))\n",
      "        >>> df.select(zip_with(\"xs\", \"ys\", lambda x, y: x ** y).alias(\"powers\")).show(truncate=False)\n",
      "        +---------------------------+\n",
      "        |powers                     |\n",
      "        +---------------------------+\n",
      "        |[1.0, 9.0, 625.0, 262144.0]|\n",
      "        +---------------------------+\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, [\"foo\", \"bar\"], [1, 2, 3])], (\"id\", \"xs\", \"ys\"))\n",
      "        >>> df.select(zip_with(\"xs\", \"ys\", lambda x, y: concat_ws(\"_\", x, y)).alias(\"xs_ys\")).show()\n",
      "        +-----------------+\n",
      "        |            xs_ys|\n",
      "        +-----------------+\n",
      "        |[foo_1, bar_2, 3]|\n",
      "        +-----------------+\n",
      "\n",
      "DATA\n",
      "    Callable = typing.Callable\n",
      "        Deprecated alias to collections.abc.Callable.\n",
      "        \n",
      "        Callable[[int], str] signifies a function that takes a single\n",
      "        parameter of type int and returns a str.\n",
      "        \n",
      "        The subscription syntax must always be used with exactly two\n",
      "        values: the argument list and the return type.\n",
      "        The argument list must be a list of types, a ParamSpec,\n",
      "        Concatenate or ellipsis. The return type must be a single type.\n",
      "        \n",
      "        There is no syntax to indicate optional or keyword arguments;\n",
      "        such function types are rarely used as callback types.\n",
      "    \n",
      "    Dict = typing.Dict\n",
      "        A generic version of dict.\n",
      "    \n",
      "    Iterable = typing.Iterable\n",
      "        A generic version of collections.abc.Iterable.\n",
      "    \n",
      "    List = typing.List\n",
      "        A generic version of list.\n",
      "    \n",
      "    Optional = typing.Optional\n",
      "        Optional[X] is equivalent to Union[X, None].\n",
      "    \n",
      "    TYPE_CHECKING = False\n",
      "    Tuple = typing.Tuple\n",
      "        Deprecated alias to builtins.tuple.\n",
      "        \n",
      "        Tuple[X, Y] is the cross-product type of X and Y.\n",
      "        \n",
      "        Example: Tuple[T1, T2] is a tuple of two elements corresponding\n",
      "        to type variables T1 and T2.  Tuple[int, float, str] is a tuple\n",
      "        of an int, a float and a string.\n",
      "        \n",
      "        To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].\n",
      "    \n",
      "    Type = typing.Type\n",
      "        Deprecated alias to builtins.type.\n",
      "        \n",
      "        builtins.type or typing.Type can be used to annotate class objects.\n",
      "        For example, suppose we have the following classes::\n",
      "        \n",
      "            class User: ...  # Abstract base for User classes\n",
      "            class BasicUser(User): ...\n",
      "            class ProUser(User): ...\n",
      "            class TeamUser(User): ...\n",
      "        \n",
      "        And a function that takes a class argument that's a subclass of\n",
      "        User and returns an instance of the corresponding class::\n",
      "        \n",
      "            U = TypeVar('U', bound=User)\n",
      "            def new_user(user_class: Type[U]) -> U:\n",
      "                user = user_class()\n",
      "                # (Here we could write the user object to a database)\n",
      "                return user\n",
      "        \n",
      "            joe = new_user(BasicUser)\n",
      "        \n",
      "        At this point the type checker knows that joe has type BasicUser.\n",
      "    \n",
      "    Union = typing.Union\n",
      "        Union type; Union[X, Y] means either X or Y.\n",
      "        \n",
      "        On Python 3.10 and higher, the | operator\n",
      "        can also be used to denote unions;\n",
      "        X | Y means the same thing to the type checker as Union[X, Y].\n",
      "        \n",
      "        To define a union, use e.g. Union[int, str]. Details:\n",
      "        - The arguments must be types and there must be at least one.\n",
      "        - None as an argument is a special case and is replaced by\n",
      "          type(None).\n",
      "        - Unions of unions are flattened, e.g.::\n",
      "        \n",
      "            assert Union[Union[int, str], float] == Union[int, str, float]\n",
      "        \n",
      "        - Unions of a single argument vanish, e.g.::\n",
      "        \n",
      "            assert Union[int] == int  # The constructor actually returns int\n",
      "        \n",
      "        - Redundant arguments are skipped, e.g.::\n",
      "        \n",
      "            assert Union[int, str, int] == Union[int, str]\n",
      "        \n",
      "        - When comparing unions, the argument order is ignored, e.g.::\n",
      "        \n",
      "            assert Union[int, str] == Union[str, int]\n",
      "        \n",
      "        - You cannot subclass or instantiate a union.\n",
      "        - You can use Optional[X] as a shorthand for Union[X, None].\n",
      "    \n",
      "    ValuesView = typing.ValuesView\n",
      "        A generic version of collections.abc.ValuesView.\n",
      "    \n",
      "    has_numpy = True\n",
      "\n",
      "FILE\n",
      "    /usr/local/spark/python/pyspark/sql/functions.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "help(pyspark.sql.functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ac563-379a-40b8-90c2-1a1a61d76897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c8dd86-01d5-4afa-86c3-6fe983006b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
